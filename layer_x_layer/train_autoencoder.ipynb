{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1 torch\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import gc\n",
    "import importlib\n",
    "import inspect\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import shutil\n",
    "import traceback\n",
    "from collections import OrderedDict, defaultdict\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Mapping, Optional, Union\n",
    "from utils.Dataspec import DatasetSpec\n",
    "\n",
    "\n",
    "# Third-party libraries - NumPy & Scientific\n",
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "\n",
    "# Third-party libraries - PyTorch\n",
    "import torch\n",
    "print (torch.__version__, \"torch\")\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "# from torch.utils.tensorboard.summary import hparams\n",
    "\n",
    "# Third-party libraries - Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "# Third-party libraries - ML Tools\n",
    "from omegaconf import DictConfig, ListConfig, OmegaConf\n",
    "import omegaconf.errors\n",
    "\n",
    "# Local imports\n",
    "# from ext import common\n",
    "import fvdb\n",
    "import fvdb.nn as fvnn\n",
    "from fvdb import JaggedTensor, GridBatch\n",
    "from fvdb.nn import VDBTensor\n",
    "\n",
    "\n",
    "from modules.autoencoding.sunet import StructPredictionNet\n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "batch_size = 4\n",
    "# Tree depth is what builds the different resolutions of the voxel grid.\n",
    "# For example, a tree depth of 3 with a resolution of 512 would give you\n",
    "# a grid of 128x128x128. The depth of the tree determines how many times\n",
    "# the original grid is downsampled. So, a depth of 3 means the original\n",
    "# grid is downsampled three times, resulting in a final resolution of\n",
    "# 128x128x128.\n",
    "tree_depth = 3 # according to 512x512x512 -> 128x128x128\n",
    "voxel_size = 0.0025\n",
    "resolution = 512\n",
    "use_fvdb_loader = True\n",
    "use_hash_tree = True # use hash tree means use early dilation (description in Sec 3.4) \n",
    "\n",
    "\n",
    "with_semantic_branch = False\n",
    "extract_mesh = \"store_true\"\n",
    "\n",
    "solver_order =\"3\"\n",
    "\n",
    "use_dpm = \"store_true\"\n",
    "\n",
    "ddim_step = 50\n",
    "\n",
    "use_ddim = \"store_true\"\n",
    "\n",
    "ema = \"store_true\"\n",
    "\n",
    "batch_len = 64\n",
    "\n",
    "toal_len = 700\n",
    "\n",
    "seed = 0\n",
    "\n",
    "world_size = 1\n",
    "\n",
    "# setup input\n",
    "use_input_normal = True\n",
    "use_input_semantic = False\n",
    "use_input_intensity = False\n",
    "use_input_color = False\n",
    "\n",
    "# setup KL loss\n",
    "cut_ratio = 16 # reduce the dimension of the latent space\n",
    "kl_weight = 1.0 # activate when anneal is off\n",
    "normalize_kld = True\n",
    "enable_anneal = False\n",
    "kl_weight_min = 1e-7\n",
    "kl_weight_max = 1.0\n",
    "anneal_star_iter = 0\n",
    "anneal_end_iter = 70000 # need to adjust for different dataset\n",
    "\n",
    "\n",
    "structure_weight = 20.0\n",
    "normal_weight = 300.0\n",
    "  \n",
    "\n",
    "learning_rate = {\n",
    "  \"init\": 1.0e-4,\n",
    "  \"decay_mult\": 0.7,\n",
    "  \"decay_step\": 50000,\n",
    "  \"clip\": 1.0e-6\n",
    "}\n",
    "weight_decay = 0.0\n",
    "grad_clip = 0.5\n",
    "\n",
    "c_dim = 32\n",
    "  \n",
    "# unet parameters\n",
    "in_channels = 32\n",
    "num_blocks = tree_depth\n",
    "f_maps = 32\n",
    "neck_dense_type = \"UNCHANGED\"\n",
    "neck_bound = [64, 64, 64] # useless but indicate here\n",
    "num_res_blocks = 1\n",
    "use_residual = False\n",
    "order = \"gcr\"\n",
    "is_add_dec = False\n",
    "use_attention = False\n",
    "use_checkpoint = False\n",
    "\n",
    "\n",
    "_custom_name =  \"objaverse\"\n",
    "_objaverse_path = \"/home/benzshawelt/Kedziora/kedziora_research/layer_x_layer/data_gen/voxels/512\"\n",
    "_split_path = \"/home/benzshawelt/Kedziora/kedziora_research/layer_x_layer/data_gen/voxels/\"\n",
    "_text_emb_path = \"\"\n",
    "_null_embed_path = \"./assets/null_text_emb.pkl\"\n",
    "max_text_len= 77\n",
    "text_embed_drop_prob= 0.1\n",
    "\n",
    "train_dataset = \"ObjaverseDataset\"\n",
    "train_val_num_workers= 16\n",
    "train_kwargs = {\n",
    "  \"onet_base_path\": _objaverse_path ,\n",
    "  \"resolution\": resolution,\n",
    "  \"custom_name\": _custom_name,\n",
    "  \"split_base_path\": _split_path,\n",
    "  \"split\": \"train\",\n",
    "  \"text_emb_path\": _text_emb_path,\n",
    "  \"null_embed_path\": _null_embed_path,\n",
    "  \"max_text_len\": max_text_len,\n",
    "  \"text_embed_drop_prob\": text_embed_drop_prob, # ! classifier-free training\n",
    "  \"random_seed\": 0\n",
    "}\n",
    "\n",
    "val_dataset = \"ObjaverseDataset\"\n",
    "val_kwargs = {\n",
    "  \"onet_base_path\": _objaverse_path ,\n",
    "  \"resolution\": resolution,\n",
    "  \"custom_name\": _custom_name,\n",
    "  \"split_base_path\": _split_path,\n",
    "  \"split\": \"test\",\n",
    "  \"text_emb_path\": _text_emb_path,\n",
    "  \"null_embed_path\": _null_embed_path,\n",
    "  \"max_text_len\": max_text_len,\n",
    "  \"random_seed\": \"fixed\"\n",
    "}\n",
    "\n",
    "test_dataset = \"ObjaverseDataset\"\n",
    "test_num_workers =8\n",
    "test_kwargs = {\n",
    "  \"onet_base_path\": _objaverse_path ,\n",
    "  \"resolution\": resolution,\n",
    "  \"custom_name\": _custom_name,\n",
    "  \"split_base_path\": _split_path,\n",
    "  \"split\": \"test\",\n",
    "  \"text_emb_path\": _text_emb_path,\n",
    "  \"null_embed_path\": _null_embed_path,\n",
    "  \"max_text_len\": max_text_len,\n",
    "  \"random_seed\": \"fixed\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparametrize(mu, logvar):\n",
    "    std = logvar.div(2).exp()\n",
    "    eps = Variable(std.data.new(std.size()).normal_())\n",
    "    return mu + std*eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the paths of all of the voxelized shapes, the shapes are stored in .pkl files within 2 folder depths of the base path\n",
    "def get_all_paths(base_path):\n",
    "  import os\n",
    "  # list all of the files in the base path\n",
    "  # print(\"base path\", base_path)\n",
    "  # print(os.listdir(base_path))\n",
    "  all_paths = []\n",
    "  for root, dirs, files in os.walk(base_path):\n",
    "    for file in files:\n",
    "      if file.endswith(\".pkl\"):\n",
    "        all_paths.append(os.path.join(root, file.split(\".\")[0]))\n",
    "        \n",
    "  return all_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, train_split_ratio, test_split_ratio, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    indices = np.arange(len(dataset))\n",
    "    np.random.shuffle(indices)\n",
    "    train_split = int(len(dataset) * train_split_ratio)\n",
    "    # always take .10 of the dataset for test (removed from val)\n",
    "    test_split = int(len(dataset) * test_split_ratio) + train_split\n",
    "\n",
    "    #return the train, val, and test datasets\n",
    "    return torch.utils.data.Subset(dataset, indices[:train_split]), torch.utils.data.Subset(dataset, indices[train_split:test_split]), torch.utils.data.Subset(dataset, indices[test_split:])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_split_paths(base_path, split_ratio, seed=0):\n",
    "    all_paths = get_all_paths(base_path)\n",
    "    train_paths, val_paths, test_split= split_dataset(all_paths, split_ratio, 0.1, seed)\n",
    "\n",
    "    # Save the split paths as a lst file\n",
    "    split_base_path = Path(base_path).parent\n",
    "    train_split_path = split_base_path / \"train.lst\"\n",
    "    val_split_path = split_base_path / \"val.lst\"\n",
    "    test_split_path = split_base_path / \"test.lst\"\n",
    "    \n",
    "\n",
    "    # delete the files if they already exist\n",
    "    if train_split_path.exists():\n",
    "        train_split_path.unlink()\n",
    "    if val_split_path.exists():\n",
    "        val_split_path.unlink()\n",
    "    if test_split_path.exists():\n",
    "        test_split_path.unlink()\n",
    "        \n",
    "    # write the paths to the files\n",
    "\n",
    "    with open(test_split_path, \"w\") as f:\n",
    "        for path in test_split:\n",
    "            f.write(f\"{path}\\n\")\n",
    "    with open(train_split_path, \"w\") as f:\n",
    "        for path in train_paths:\n",
    "            f.write(f\"{path}\\n\")\n",
    "    with open(val_split_path, \"w\") as f:\n",
    "        for path in val_paths:\n",
    "            f.write(f\"{path}\\n\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_split_paths(_objaverse_path , 0.8, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautoencoding\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_encoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Encoder\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mUnetWrapper\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, unet, hparams):\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "from modules.autoencoding.base_encoder import Encoder\n",
    "\n",
    "class UnetWrapper(nn.Module):\n",
    "    def __init__(self, unet, hparams):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(hparams)\n",
    "        self.unet = unet\n",
    "        self.hparams = hparams\n",
    "        # Ensure cut_ratio has a default value if not provided\n",
    "        if \"cut_ratio\" not in self.hparams:\n",
    "            self.hparams[\"cut_ratio\"] = 1.0  # Default value, adjust as needed\n",
    "        \n",
    "\n",
    "    def build_hash_tree_from_grid(self, input_grid):\n",
    "        hash_tree = {}\n",
    "        input_xyz = input_grid.grid_to_world(input_grid.ijk.float())\n",
    "        \n",
    "        for depth in range(self.hparams[\"tree_depth\"]):\n",
    "            if depth != 0 and not self.hparams[\"use_hash_tree\"]:\n",
    "                break            \n",
    "            voxel_size = [sv * 2 ** depth for sv in self.hparams[\"voxel_size\"]]\n",
    "            origins = [sv / 2. for sv in voxel_size]\n",
    "            \n",
    "            if depth == 0:\n",
    "                hash_tree[depth] = input_grid\n",
    "            else:\n",
    "                hash_tree[depth] = fvdb.gridbatch_from_nearest_voxels_to_points(input_xyz, \n",
    "                                                                                  voxel_sizes=voxel_size, \n",
    "                                                                                  origins=origins)\n",
    "        return hash_tree\n",
    "\n",
    "    def forward(self, batch, out: dict):\n",
    "        input_xyz = batch[DatasetSpec.INPUT_PC]\n",
    "        hash_tree = self.build_hash_tree_from_grid(input_xyz)\n",
    "        input_grid = hash_tree[0]\n",
    "        batch.update({'input_grid': input_grid})\n",
    "\n",
    "        if not self.hparams[\"use_hash_tree\"]:\n",
    "            hash_tree = None\n",
    "                \n",
    "        unet_feat = self.encoder(input_grid, batch)\n",
    "        unet_feat = fvnn.VDBTensor(input_grid, input_grid.jagged_like(unet_feat))\n",
    "        unet_res, unet_output, dist_features = self.unet(unet_feat, hash_tree)\n",
    "\n",
    "        out.update({'tree': unet_res.structure_grid})\n",
    "        out.update({\n",
    "            'structure_features': unet_res.structure_features,\n",
    "            'dist_features': dist_features,\n",
    "        })\n",
    "        out.update({'gt_grid': input_grid})\n",
    "        out.update({'gt_tree': hash_tree})\n",
    "        \n",
    "        if self.hparams[\"with_normal_branch\"]:\n",
    "            out.update({\n",
    "                'normal_features': unet_res.normal_features,\n",
    "            })\n",
    "        if self.hparams[\"with_semantic_branch\"]:\n",
    "            out.update({\n",
    "                'semantic_features': unet_res.semantic_features,\n",
    "            })\n",
    "        if self.hparams[\"with_color_branch\"]:\n",
    "            out.update({\n",
    "                'color_features': unet_res.color_features,\n",
    "            })\n",
    "        return out\n",
    "    \n",
    "    def get_dataset_spec(self):\n",
    "        DS = DatasetSpec\n",
    "        all_specs = [DS.SHAPE_NAME, DS.INPUT_PC,\n",
    "                        DS.GT_DENSE_PC, DS.GT_GEOMETRY]\n",
    "        if self.hparams.get(\"use_input_normal\", True):\n",
    "            all_specs.append(DS.TARGET_NORMAL)\n",
    "            all_specs.append(DS.GT_DENSE_NORMAL)\n",
    "        if self.hparams.get(\"use_input_semantic\", False) or self.hparams.get(\"with_semantic_branch\", False):\n",
    "            all_specs.append(DS.GT_SEMANTIC)\n",
    "        if self.hparams.get(\"use_input_intensity\", False):\n",
    "            all_specs.append(DS.INPUT_INTENSITY)\n",
    "        return all_specs\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _encode(self, batch, use_mode=False):\n",
    "        input_xyz = batch[DatasetSpec.INPUT_PC]\n",
    "        hash_tree = self.build_hash_tree_from_grid(input_xyz)\n",
    "        input_grid = hash_tree[0]\n",
    "        batch.update({'input_grid': input_grid})\n",
    "\n",
    "        if not self.hparams[\"use_hash_tree\"]:\n",
    "            hash_tree = None\n",
    "\n",
    "        unet_feat = self.encoder(input_grid, batch)\n",
    "        unet_feat = fvnn.VDBTensor(input_grid, input_grid.jagged_like(unet_feat))\n",
    "        _, x, mu, log_sigma = self.unet.encode(unet_feat, hash_tree=hash_tree)\n",
    "        if use_mode:\n",
    "            sparse_feature = mu\n",
    "        else:\n",
    "            sparse_feature = reparametrize(mu, log_sigma)\n",
    "        \n",
    "        return fvnn.VDBTensor(x.grid, x.grid.jagged_like(sparse_feature))\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def load_from_checkpoint(checkpoint_path):\n",
    "        \"\"\"Load the entire model from a checkpoint without needing separate autoencoder initialization.\"\"\"\n",
    "        # Load the entire checkpoint\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "\n",
    "        u_net = StructPredictionNet(\n",
    "            in_channels=checkpoint.get('in_channels'),    \n",
    "            num_blocks=checkpoint.get('num_blocks'),\n",
    "            f_maps=checkpoint.get('f_maps'),\n",
    "            neck_dense_type=checkpoint.get('neck_dense_type'),\n",
    "            neck_bound=checkpoint.get('neck_bound'),\n",
    "            num_res_blocks=checkpoint.get('num_res_blocks'),\n",
    "            use_residual=checkpoint.get('use_residual'),\n",
    "            order=checkpoint.get('order'),\n",
    "            is_add_dec=checkpoint.get('is_add_dec'),\n",
    "            use_attention=checkpoint.get('use_attention'),\n",
    "            use_checkpoint=checkpoint.get('use_checkpoint'),\n",
    "            c_dim=checkpoint.get('c_dim')\n",
    "        )\n",
    "\n",
    "\n",
    "        unet_wrapper = UnetWrapper(u_net, {\n",
    "            \"tree_depth\": checkpoint.get('tree_depth'),\n",
    "            \"voxel_size\": checkpoint.get('voxel_size'),\n",
    "            \"use_hash_tree\": checkpoint.get('use_hash_tree'),\n",
    "            \"use_input_normal\": checkpoint.get('use_input_normal'),\n",
    "            \"use_input_semantic\": checkpoint.get('use_input_semantic'),\n",
    "            \"use_input_color\": checkpoint.get('use_input_color'),\n",
    "            \"use_input_intensity\": checkpoint.get('use_input_intensity'),\n",
    "            \"c_dim\": checkpoint.get('c_dim'),\n",
    "            \"with_normal_branch\": checkpoint.get('with_normal_branch'),\n",
    "            \"with_semantic_branch\": checkpoint.get('with_semantic_branch'),\n",
    "            \"with_color_branch\": checkpoint.get('with_color_branch'),\n",
    "        })\n",
    "        # Load the state dict\n",
    "        unet_wrapper.load_state_dict(checkpoint.get('model_state_dict', checkpoint))\n",
    "        \n",
    "        unet_wrapper.hparams[\"num_blocks\"] = checkpoint.get('num_blocks')\n",
    "        unet_wrapper.hparams[\"f_maps\"] = checkpoint.get('f_maps')\n",
    "        unet_wrapper.hparams[\"cut_ratio\"] = checkpoint.get('cut_ratio', 1.0)  # Added cut_ratio with default\n",
    "\n",
    "\n",
    "        return unet_wrapper, u_net\n",
    "    \n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"tree_depth\": self.hparams[\"tree_depth\"],\n",
    "            \"voxel_size\": self.hparams[\"voxel_size\"],\n",
    "            \"use_hash_tree\": self.hparams[\"use_hash_tree\"],\n",
    "            \"use_input_normal\": self.hparams[\"use_input_normal\"],\n",
    "            \"use_input_semantic\": self.hparams[\"use_input_semantic\"],\n",
    "            \"use_input_color\": self.hparams[\"use_input_color\"],\n",
    "            \"use_input_intensity\": self.hparams[\"use_input_intensity\"],\n",
    "            \"c_dim\": self.hparams[\"c_dim\"],\n",
    "            \"with_normal_branch\": self.hparams[\"with_normal_branch\"],\n",
    "            \"with_semantic_branch\": self.hparams[\"with_semantic_branch\"],\n",
    "            \"with_color_branch\": self.hparams[\"with_color_branch\"],\n",
    "            \"cut_ratio\": self.hparams.get(\"cut_ratio\", 1.0),  # Added cut_ratio with default\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def load_from_checkpoint(checkpoint_path, industry_mapping):\n",
    "        \"\"\"Load the entire model from a checkpoint without needing separate autoencoder initialization.\"\"\"\n",
    "        # Load the entire checkpoint\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "        u_net = StructPredictionNet(\n",
    "            in_channels=checkpoint.get('in_channels'),    \n",
    "            num_blocks=checkpoint.get('num_blocks'),\n",
    "            f_maps=checkpoint.get('f_maps'),\n",
    "            neck_dense_type=checkpoint.get('neck_dense_type'),\n",
    "            neck_bound=checkpoint.get('neck_bound'),\n",
    "            num_res_blocks=checkpoint.get('num_res_blocks'),\n",
    "            use_residual=checkpoint.get('use_residual'),\n",
    "            order=checkpoint.get('order'),\n",
    "            is_add_dec=checkpoint.get('is_add_dec'),\n",
    "            use_attention=checkpoint.get('use_attention'),\n",
    "            use_checkpoint=checkpoint.get('use_checkpoint'),\n",
    "            c_dim=checkpoint.get('c_dim')\n",
    "        )\n",
    "\n",
    "        unet_wrapper = UnetWrapper(u_net, {\n",
    "            \"tree_depth\": checkpoint.get('tree_depth'),\n",
    "            \"voxel_size\": checkpoint.get('voxel_size'),\n",
    "            \"use_hash_tree\": checkpoint.get('use_hash_tree'),\n",
    "            \"use_input_normal\": checkpoint.get('use_input_normal'),\n",
    "            \"use_input_semantic\": checkpoint.get('use_input_semantic'),\n",
    "            \"use_input_color\": checkpoint.get('use_input_color'),\n",
    "            \"use_input_intensity\": checkpoint.get('use_input_intensity'),\n",
    "            \"c_dim\": checkpoint.get('c_dim'),\n",
    "            \"with_normal_branch\": checkpoint.get('with_normal_branch'),\n",
    "            \"with_semantic_branch\": checkpoint.get('with_semantic_branch'),\n",
    "            \"with_color_branch\": checkpoint.get('with_color_branch'),\n",
    "            \"cut_ratio\": checkpoint.get('cut_ratio', 1.0),  # Added cut_ratio with default\n",
    "        })\n",
    "\n",
    "        # Load the state dict\n",
    "        unet_wrapper.load_state_dict(checkpoint.get('model_state_dict', checkpoint))\n",
    "\n",
    "        return unet_wrapper, u_net\n",
    "\n",
    "    def save_checkpoint(self, filepath):\n",
    "        \"\"\"Save the model weights and configuration to a checkpoint file.\"\"\"\n",
    "        checkpoint = {\n",
    "            # Configuration parameters\n",
    "            **self.get_config(),  # Unpack all config parameters from get_config\n",
    "            \n",
    "            # Add U-Net specific parameters that aren't in get_config\n",
    "            'in_channels': self.unet.in_channels,\n",
    "            'num_blocks': self.unet.num_blocks,\n",
    "            'f_maps': self.unet.f_maps,\n",
    "            'neck_dense_type': self.unet.neck_dense_type,\n",
    "            'neck_bound': self.unet.neck_bound,\n",
    "            'num_res_blocks': self.unet.num_res_blocks,\n",
    "            'use_residual': self.unet.use_residual,\n",
    "            'order': self.unet.order,\n",
    "            'is_add_dec': self.unet.is_add_dec,\n",
    "            'use_attention': self.unet.use_attention,\n",
    "            'use_checkpoint': self.unet.use_checkpoint,\n",
    "            \n",
    "            # Model weights\n",
    "            'model_state_dict': self.state_dict(),\n",
    "        }\n",
    "        \n",
    "        torch.save(checkpoint, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_lr_wrapper(it, lr_config, batch_size, accumulate_grad_batches=1):\n",
    "    return max(\n",
    "        lr_config['decay_mult'] ** (int(it * batch_size * accumulate_grad_batches / lr_config['decay_step'])),\n",
    "        lr_config['clip'] / lr_config['init'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "u_net = StructPredictionNet(\n",
    "  in_channels=in_channels,\n",
    "  num_blocks=num_blocks,\n",
    "  f_maps=f_maps,\n",
    "  neck_dense_type=neck_dense_type,\n",
    "  neck_bound=neck_bound,\n",
    "  num_res_blocks=num_res_blocks,\n",
    "  use_residual=use_residual,\n",
    "  order=order,\n",
    "  is_add_dec=is_add_dec,\n",
    "  use_attention=use_attention,\n",
    "  use_checkpoint=use_checkpoint,\n",
    "  c_dim=c_dim\n",
    ")\n",
    "\n",
    "unet_wrapper = UnetWrapper(u_net, {\n",
    "    \"tree_depth\": tree_depth,\n",
    "    \"voxel_size\": [voxel_size, voxel_size, voxel_size],\n",
    "    \"use_hash_tree\": use_hash_tree,\n",
    "    \"use_input_normal\": use_input_normal,\n",
    "    \"use_input_semantic\": use_input_semantic,\n",
    "    \"use_input_color\": use_input_color,\n",
    "    \"use_input_intensity\": use_input_intensity,\n",
    "    \"c_dim\": c_dim,\n",
    "    \"with_normal_branch\": True,\n",
    "    \"with_semantic_branch\": with_semantic_branch,\n",
    "    \"with_color_branch\": False,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(unet_wrapper.parameters(), lr=learning_rate[\"init\"],\n",
    "                                    weight_decay=weight_decay, amsgrad=True)\n",
    "\n",
    "scheduler = LambdaLR(optimizer,\n",
    "                lr_lambda=functools.partial(\n",
    "                    lambda_lr_wrapper, lr_config=learning_rate, batch_size=batch_size))\n",
    "\n",
    "# exp.global_var_manager.register_variable('skip_backward', False)\n",
    "\n",
    "def list_collate(batch):\n",
    "    \"\"\"\n",
    "    This just do not stack batch dimension.\n",
    "    \"\"\"\n",
    "    \n",
    "    elem = None\n",
    "    for e in batch:\n",
    "        if e is not None:\n",
    "            elem = e\n",
    "            break\n",
    "    elem_type = type(elem)\n",
    "    if isinstance(elem, torch.Tensor):\n",
    "        return batch\n",
    "    elif elem_type.__module__ == 'numpy' and elem_type.__name__ != 'str_' \\\n",
    "            and elem_type.__name__ != 'string_':\n",
    "        if elem_type.__name__ == 'ndarray' or elem_type.__name__ == 'memmap':\n",
    "            return list_collate([torch.as_tensor(b) if b is not None else None for b in batch])\n",
    "        elif elem.shape == ():  # scalars\n",
    "            return torch.as_tensor(batch)\n",
    "    elif isinstance(elem, float):\n",
    "        return torch.tensor(batch, dtype=torch.float64)\n",
    "    elif isinstance(elem, int):\n",
    "        return torch.tensor(batch)\n",
    "    elif isinstance(elem, str):\n",
    "        return batch\n",
    "    elif isinstance(elem, DictConfig) or isinstance(elem, ListConfig):\n",
    "        return batch\n",
    "    elif isinstance(elem, collections.abc.Mapping):\n",
    "        return {key: list_collate([d[key] for d in batch]) for key in elem}\n",
    "    elif isinstance(elem, collections.abc.Sequence):\n",
    "        # check to make sure that the elements in batch have consistent size\n",
    "        it = iter(batch)\n",
    "        elem_size = len(next(it))\n",
    "        if not all(len(elem) == elem_size for elem in it):\n",
    "            raise RuntimeError('each element in list of batch should be of equal size')\n",
    "        transposed = zip(*batch)\n",
    "        return [list_collate(samples) for samples in transposed]\n",
    "    elif isinstance(elem, GridBatch):\n",
    "        return fvdb.jcat(batch)\n",
    "    \n",
    "    return batch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_dataloader():\n",
    "    from data.objaverse import ObjaverseDataset\n",
    "    train_set =  ObjaverseDataset(onet_base_path=train_kwargs[\"onet_base_path\"], \n",
    "                                  spec=unet_wrapper.get_dataset_spec(), \n",
    "                                  split=train_kwargs[\"split\"], \n",
    "                                  resolution=train_kwargs[\"resolution\"], \n",
    "                                  image_base_path=None, \n",
    "                                  random_seed=0, \n",
    "                                  hparams=None, \n",
    "                                  skip_on_error=False, \n",
    "                                  custom_name=\"objaverse\", \n",
    "                                  text_emb_path=\"../data/objaverse/objaverse/text_emb\", \n",
    "                                  null_embed_path=\"./assets/null_text_emb.pkl\", \n",
    "                                  text_embed_drop_prob=0.0, \n",
    "                                  max_text_len=77, \n",
    "                                  duplicate_num=1, \n",
    "                                  split_base_path=_split_path,\n",
    "                                  )\n",
    "        \n",
    "    return DataLoader(train_set, batch_size=batch_size // world_size, shuffle=True,\n",
    "                        num_workers=train_val_num_workers, collate_fn=list_collate)\n",
    "\n",
    "\n",
    "# print(get_dataset_spec())\n",
    "\n",
    "def val_dataloader():\n",
    "    from data.objaverse import ObjaverseDataset\n",
    "    val_set = ObjaverseDataset(onet_base_path=val_kwargs[\"onet_base_path\"],\n",
    "                                spec=unet_wrapper.get_dataset_spec(), \n",
    "                                split=val_kwargs[\"split\"], \n",
    "                                resolution=val_kwargs[\"resolution\"], \n",
    "                                image_base_path=None, \n",
    "                                random_seed=0, \n",
    "                                hparams=None, \n",
    "                                skip_on_error=False, \n",
    "                                custom_name=\"objaverse\", \n",
    "                                text_emb_path=\"../data/objaverse/objaverse/text_emb\", \n",
    "                                null_embed_path=\"./assets/null_text_emb.pkl\", \n",
    "                                text_embed_drop_prob=0.0, \n",
    "                                max_text_len=77, \n",
    "                                duplicate_num=1, \n",
    "                                split_base_path=_split_path,\n",
    "                                )\n",
    "\n",
    "\n",
    "    return DataLoader(val_set, batch_size=batch_size // world_size, shuffle=False,\n",
    "                        num_workers=train_val_num_workers, collate_fn=list_collate)\n",
    "\n",
    "def test_dataloader(resolution=resolution, test_set_shuffle=False):\n",
    "    from data.objaverse import ObjaverseDataset\n",
    "    resolution = resolution # ! use for testing when training on X^3 but testing on Y^3\n",
    "\n",
    "    test_set =  ObjaverseDataset(onet_base_path=test_kwargs[\"onet_base_path\"],\n",
    "                                spec=unet_wrapper.get_dataset_spec(), \n",
    "                                split=test_kwargs[\"split\"], \n",
    "                                resolution=resolution, \n",
    "                                image_base_path=None, \n",
    "                                random_seed=0, \n",
    "                                hparams=None, \n",
    "                                skip_on_error=False, \n",
    "                                custom_name=\"objaverse\", \n",
    "                                text_emb_path=\"../data/objaverse/objaverse/text_emb\", \n",
    "                                null_embed_path=\"./assets/null_text_emb.pkl\", \n",
    "                                text_embed_drop_prob=0.0, \n",
    "                                max_text_len=77, \n",
    "                                duplicate_num=1, \n",
    "                                split_base_path=_split_path,\n",
    "                                )\n",
    "    \n",
    "    if test_set_shuffle:\n",
    "        torch.manual_seed(0)\n",
    "    return DataLoader(test_set, batch_size=1, shuffle=test_set_shuffle, \n",
    "                        num_workers=0, collate_fn=list_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benzshawelt/.conda/envs/xlayer/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "train_dataloader_ = train_dataloader()\n",
    "val_dataloader_ = val_dataloader()\n",
    "test_dataloader_ = test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.exp as exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.color_util import color_from_points, semantic_from_points\n",
    "from utils.loss_util import TorchLossMeter\n",
    "from utils.Dataspec import DatasetSpec as DS\n",
    "    \n",
    "class Loss(nn.Module):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.hparams = hparams\n",
    "        # Set default for use_fvdb_loader if not provided\n",
    "        if \"use_fvdb_loader\" not in self.hparams:\n",
    "            self.hparams[\"use_fvdb_loader\"] = True\n",
    "        # Set default for remain_h if not provided\n",
    "        if \"remain_h\" not in self.hparams:\n",
    "            self.hparams[\"remain_h\"] = False\n",
    "        # Set default kl_weight if not provided\n",
    "        if \"kl_weight\" not in self.hparams:\n",
    "            self.hparams[\"kl_weight\"] = 1.0\n",
    "\n",
    "    def transform_field(self, field: torch.Tensor):\n",
    "        gt_band = 1.0 # not sure if this will be changed\n",
    "        # For scalar voxel_size, use the first element from the list\n",
    "        voxel_size = self.hparams[\"voxel_size\"][0] if isinstance(self.hparams[\"voxel_size\"], list) else self.hparams[\"voxel_size\"]\n",
    "        truncation_size = gt_band * voxel_size\n",
    "        # non-binary supervision (made sure derivative norm at 0 if 1)\n",
    "        field = torch.tanh(field / truncation_size) * truncation_size\n",
    "        return field\n",
    "    \n",
    "    def cross_entropy(self, pd_struct: fvnn.VDBTensor, gt_grid: fvdb.GridBatch, dynamic_grid: fvdb.GridBatch = None):\n",
    "        assert torch.allclose(pd_struct.grid.origins, gt_grid.origins)\n",
    "        assert torch.allclose(pd_struct.grid.voxel_sizes, gt_grid.voxel_sizes)\n",
    "        idx_mask = gt_grid.ijk_to_index(pd_struct.grid.ijk).jdata == -1\n",
    "        idx_mask = idx_mask.long()\n",
    "        if dynamic_grid is not None:\n",
    "            dynamic_mask = dynamic_grid.ijk_to_index(pd_struct.grid.ijk).jdata == -1\n",
    "            loss = F.cross_entropy(pd_struct.jdata, idx_mask, reduction='none') * dynamic_mask.float()\n",
    "            loss = loss.mean()\n",
    "        else:\n",
    "            loss = F.cross_entropy(pd_struct.jdata, idx_mask)\n",
    "        return 0.0 if idx_mask.size(0) == 0 else loss\n",
    "    \n",
    "    def struct_acc(self, pd_struct: fvnn.VDBTensor, gt_grid: fvdb.GridBatch):\n",
    "        assert torch.allclose(pd_struct.grid.origins, gt_grid.origins)\n",
    "        assert torch.allclose(pd_struct.grid.voxel_sizes, gt_grid.voxel_sizes)\n",
    "        idx_mask = gt_grid.ijk_to_index(pd_struct.grid.ijk).jdata == -1\n",
    "        idx_mask = idx_mask.long()\n",
    "        return torch.mean((pd_struct.jdata.argmax(dim=1) == idx_mask).float())\n",
    "    \n",
    "    def grid_iou(self, gt_grid: fvdb.GridBatch, pd_grid: fvdb.GridBatch):\n",
    "        assert gt_grid.grid_count == pd_grid.grid_count\n",
    "        idx = pd_grid.ijk_to_index(gt_grid.ijk)\n",
    "        upi = (pd_grid.num_voxels + gt_grid.num_voxels).cpu().numpy().tolist()\n",
    "        ious = []\n",
    "        for i in range(len(upi)):\n",
    "            inter = torch.sum(idx[i].jdata >= 0).item()\n",
    "            ious.append(inter / (upi[i] - inter + 1.0e-6))\n",
    "        return np.mean(ious)\n",
    "\n",
    "    def normal_loss(self, batch, normal_feats: fvnn.VDBTensor, eps=1e-6):\n",
    "        if self.hparams[\"use_fvdb_loader\"]:\n",
    "            ref_grid = batch['input_grid']\n",
    "            ref_xyz = ref_grid.grid_to_world(ref_grid.ijk.float()) \n",
    "        else:\n",
    "            ref_xyz = fvdb.JaggedTensor(batch[DS.INPUT_PC])\n",
    "        \n",
    "        gt_normal = normal_feats.grid.splat_trilinear(ref_xyz, fvdb.JaggedTensor(batch[DS.TARGET_NORMAL]))\n",
    "        # normalize normal\n",
    "        gt_normal.jdata /= (gt_normal.jdata.norm(dim=1, keepdim=True) + eps)\n",
    "        normal_loss = F.l1_loss(gt_normal.jdata, normal_feats.jdata)\n",
    "        return normal_loss\n",
    "    \n",
    "    def color_loss(self, batch, color_feats: fvnn.VDBTensor):\n",
    "        assert self.hparams[\"use_fvdb_loader\"] is True\n",
    "        # check if color_feats is empty\n",
    "        if color_feats.grid.total_voxels == 0:\n",
    "            return 0.0\n",
    "        ref_grid = batch['input_grid']\n",
    "        ref_xyz = ref_grid.grid_to_world(ref_grid.ijk.float())\n",
    "        ref_color = fvdb.JaggedTensor(batch[DS.INPUT_COLOR])\n",
    "        \n",
    "        target_xyz = color_feats.grid.grid_to_world(color_feats.grid.ijk.float())\n",
    "        target_color = []\n",
    "        slect_color_feats = []\n",
    "        for batch_idx in range(ref_grid.grid_count):\n",
    "            ref_color_i = ref_color[batch_idx].jdata\n",
    "            target_color.append(color_from_points(target_xyz[batch_idx].jdata, ref_xyz[batch_idx].jdata, ref_color_i, k=1))\n",
    "            slect_color_feats.append(color_feats.feature[batch_idx].jdata)\n",
    "            \n",
    "        if len(target_color) == 0 or len(slect_color_feats) == 0: # to avoid JaggedTensor build from empty list\n",
    "            return 0.0  \n",
    "        \n",
    "        target_color = fvdb.JaggedTensor(target_color)\n",
    "        slect_color_feats = fvdb.JaggedTensor(slect_color_feats)\n",
    "        color_loss = F.l1_loss(slect_color_feats.jdata, target_color.jdata)\n",
    "        return color_loss\n",
    "    \n",
    "    def semantic_loss(self, batch, semantic_feats: fvnn.VDBTensor):\n",
    "        assert self.hparams[\"use_fvdb_loader\"] is True\n",
    "        # check if semantic_feats is empty\n",
    "        if semantic_feats.grid.total_voxels == 0:\n",
    "            return 0.0\n",
    "        ref_grid = batch['input_grid']\n",
    "        ref_xyz = ref_grid.grid_to_world(ref_grid.ijk.float())\n",
    "        ref_semantic = fvdb.JaggedTensor(batch[DS.GT_SEMANTIC])\n",
    "        if ref_semantic.jdata.size(0) == 0: # if all samples in this batch is without semantic\n",
    "            return 0.0\n",
    "                \n",
    "        target_xyz = semantic_feats.grid.grid_to_world(semantic_feats.grid.ijk.float())       \n",
    "        target_semantic = []\n",
    "        slect_semantic_feats = []\n",
    "        for batch_idx in range(ref_grid.grid_count):\n",
    "            ref_semantic_i = ref_semantic[batch_idx].jdata\n",
    "            if ref_semantic_i.size(0) == 0:\n",
    "                continue\n",
    "            target_semantic.append(semantic_from_points(target_xyz[batch_idx].jdata, ref_xyz[batch_idx].jdata, ref_semantic_i))\n",
    "            slect_semantic_feats.append(semantic_feats.feature[batch_idx].jdata)\n",
    "                    \n",
    "        if len(target_semantic) == 0 or len(slect_semantic_feats) == 0: # to avoid JaggedTensor build from empty list\n",
    "            return 0.0\n",
    "\n",
    "        target_semantic = fvdb.JaggedTensor(target_semantic)\n",
    "        slect_semantic_feats = fvdb.JaggedTensor(slect_semantic_feats)\n",
    "        \n",
    "        if slect_semantic_feats.jdata.size(0) == 0: # to aviod cross_entropy take empty tensor\n",
    "            return 0.0\n",
    "        \n",
    "        semantic_loss = F.cross_entropy(slect_semantic_feats.jdata, target_semantic.jdata.long())\n",
    "        return semantic_loss\n",
    "    \n",
    "    def get_kl_weight(self, global_step):\n",
    "        # linear annealing the kl weight\n",
    "        if global_step > self.hparams[\"anneal_star_iter\"]:\n",
    "            if global_step < self.hparams[\"anneal_end_iter\"]:\n",
    "                kl_weight = self.hparams[\"kl_weight_min\"] + \\\n",
    "                                         (self.hparams[\"kl_weight_max\"] - self.hparams[\"kl_weight_min\"]) * \\\n",
    "                                         (global_step - self.hparams[\"anneal_star_iter\"]) / \\\n",
    "                                         (self.hparams[\"anneal_end_iter\"] - self.hparams[\"anneal_star_iter\"])\n",
    "            else:\n",
    "                kl_weight = self.hparams[\"kl_weight_max\"]\n",
    "        else:\n",
    "            kl_weight = self.hparams[\"kl_weight_min\"]\n",
    "\n",
    "        return kl_weight\n",
    "\n",
    "    def forward(self, batch, out, compute_metric: bool, global_step, current_epoch, optimizer_idx=0):\n",
    "        loss_dict = TorchLossMeter()\n",
    "        metric_dict = TorchLossMeter()\n",
    "        latent_dict = TorchLossMeter()\n",
    "\n",
    "        dynamic_grid = None\n",
    "\n",
    "        if not self.hparams[\"use_hash_tree\"]:\n",
    "            gt_grid = out['gt_grid']\n",
    "            if self.hparams[\"supervision\"][\"structure_weight\"] > 0.0:\n",
    "                for feat_depth, pd_struct_i in out['structure_features'].items():\n",
    "                    downsample_factor = 2 ** feat_depth\n",
    "                    if self.hparams[\"remain_h\"]:\n",
    "                        pd_voxel_size = pd_struct_i.grid.voxel_sizes[0]\n",
    "                        h_factor = pd_voxel_size[0] // pd_voxel_size[2]\n",
    "                        downsample_factor = [downsample_factor, downsample_factor, downsample_factor // h_factor]\n",
    "                    if downsample_factor != 1:             \n",
    "                        gt_grid_i = gt_grid.coarsened_grid(downsample_factor)\n",
    "                        dyn_grid_i = dynamic_grid.coarsened_grid(downsample_factor) if dynamic_grid is not None else None\n",
    "                    else:\n",
    "                        gt_grid_i = gt_grid\n",
    "                        dyn_grid_i = dynamic_grid\n",
    "                    loss_dict.add_loss(f\"struct-{feat_depth}\", self.cross_entropy(pd_struct_i, gt_grid_i, dyn_grid_i),\n",
    "                                    self.hparams[\"supervision\"][\"structure_weight\"])\n",
    "                    if compute_metric:\n",
    "                        with torch.no_grad():\n",
    "                            metric_dict.add_loss(f\"struct-acc-{feat_depth}\", self.struct_acc(pd_struct_i, gt_grid_i))\n",
    "        else:\n",
    "            if self.hparams[\"supervision\"][\"structure_weight\"] > 0.0:\n",
    "                gt_tree = out['gt_tree']\n",
    "                for feat_depth, pd_struct_i in out['structure_features'].items():\n",
    "                    gt_grid_i = gt_tree[feat_depth]\n",
    "                    # get dynamic grid\n",
    "                    dyn_grid_i = dynamic_grid.coarsened_grid(2 ** feat_depth) if dynamic_grid is not None else None\n",
    "                    loss_dict.add_loss(f\"struct-{feat_depth}\", self.cross_entropy(pd_struct_i, gt_grid_i, dyn_grid_i),\n",
    "                                    self.hparams[\"supervision\"][\"structure_weight\"])\n",
    "                    if compute_metric:\n",
    "                        with torch.no_grad():\n",
    "                            metric_dict.add_loss(f\"struct-acc-{feat_depth}\", self.struct_acc(pd_struct_i, gt_grid_i))\n",
    "        \n",
    "        # compute normal loss\n",
    "        if self.hparams[\"with_normal_branch\"]:\n",
    "            if out['normal_features'] == {}:\n",
    "                normal_loss = 0.0\n",
    "            else:\n",
    "                feat_depth = min(out['normal_features'].keys())\n",
    "                normal_loss = self.normal_loss(batch, out['normal_features'][feat_depth])\n",
    "                    \n",
    "            loss_dict.add_loss(f\"normal\", normal_loss, self.hparams[\"supervision\"][\"normal_weight\"])\n",
    "        \n",
    "        # compute semantic loss\n",
    "        if self.hparams[\"with_semantic_branch\"]:\n",
    "            for feat_depth, pd_semantic_i in out['semantic_features'].items():\n",
    "                semantic_loss = self.semantic_loss(batch, pd_semantic_i)\n",
    "                if semantic_loss == 0.0: # do not take empty into log\n",
    "                    continue\n",
    "                loss_dict.add_loss(f\"semantic_{feat_depth}\", semantic_loss, self.hparams[\"supervision\"][\"semantic_weight\"])\n",
    "                \n",
    "        # compute color loss\n",
    "        if self.hparams[\"with_color_branch\"]:\n",
    "            for feat_depth, pd_color_i in out['color_features'].items():\n",
    "                color_loss = self.color_loss(batch, pd_color_i)\n",
    "                if color_loss == 0.0:\n",
    "                    continue\n",
    "                loss_dict.add_loss(f\"color_{feat_depth}\", color_loss, self.hparams[\"supervision\"][\"color_weight\"])\n",
    "\n",
    "        # compute KL divergence\n",
    "        if \"dist_features\" in out:\n",
    "            dist_features = out['dist_features']\n",
    "            kld = 0.0\n",
    "            for latent_id, (mu, logvar) in enumerate(dist_features):\n",
    "                num_voxel = mu.size(0)\n",
    "                kld_temp = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "                kld_total = kld_temp.item()\n",
    "                if self.hparams[\"normalize_kld\"]:\n",
    "                    kld_temp /= num_voxel\n",
    "\n",
    "                kld += kld_temp\n",
    "                latent_dict.add_loss(f\"mu-{latent_id}\", mu.mean())\n",
    "                latent_dict.add_loss(f\"logvar-{latent_id}\", logvar.mean())\n",
    "                latent_dict.add_loss(f\"kld-true-{latent_id}\", kld_temp.item())\n",
    "                latent_dict.add_loss(f\"kld-total-{latent_id}\", kld_total)\n",
    "\n",
    "            if self.hparams[\"enable_anneal\"]:\n",
    "                loss_dict.add_loss(\"kld\", kld, self.get_kl_weight(global_step))\n",
    "            else:\n",
    "                loss_dict.add_loss(\"kld\", kld, self.hparams[\"kl_weight\"])\n",
    "            \n",
    "        return loss_dict, metric_dict, latent_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss module\n",
    "loss_module = Loss({\n",
    "    \"tree_depth\": tree_depth,\n",
    "    \"voxel_size\": [voxel_size, voxel_size, voxel_size],\n",
    "    \"use_hash_tree\": use_hash_tree,\n",
    "    \"use_input_normal\": use_input_normal,\n",
    "    \"use_input_semantic\": use_input_semantic,\n",
    "    \"use_input_color\": use_input_color,\n",
    "    \"use_input_intensity\": use_input_intensity,\n",
    "    \"c_dim\": c_dim,\n",
    "    \"with_normal_branch\": True,\n",
    "    \"with_semantic_branch\": with_semantic_branch,\n",
    "    \"with_color_branch\": False,\n",
    "    \"supervision\": {\n",
    "        \"structure_weight\": structure_weight,\n",
    "        \"normal_weight\": normal_weight,\n",
    "        \"semantic_weight\": 0.0,\n",
    "        \"color_weight\": 0.0\n",
    "    },\n",
    "    \"normalize_kld\": normalize_kld,\n",
    "    \"enable_anneal\": enable_anneal,\n",
    "    \"kl_weight_min\": kl_weight_min,\n",
    "    \"kl_weight_max\": kl_weight_max,\n",
    "    \"anneal_star_iter\": anneal_star_iter,\n",
    "    \"anneal_end_iter\": anneal_end_iter,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100], Step [0/2], Loss: 49.02888870239258\n",
      "Epoch [0/100], Step [0/1], Loss: 48.60641098022461\n",
      "Validation loss improved to 48.60641098022461. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _ConnectionBase.__del__ at 0x7f6f93cd4cc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/benzshawelt/.conda/envs/xlayer/lib/python3.11/multiprocessing/connection.py\", line 133, in __del__\n",
      "    self._close()\n",
      "  File \"/home/benzshawelt/.conda/envs/xlayer/lib/python3.11/multiprocessing/connection.py\", line 377, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader_):\n\u001b[1;32m      7\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 9\u001b[0m     out_dict \u001b[38;5;241m=\u001b[39m \u001b[43munet_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     loss_dict, metric_dict, latent_dict \u001b[38;5;241m=\u001b[39m loss_module(batch, out_dict, compute_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, global_step\u001b[38;5;241m=\u001b[39mepoch \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader_) \u001b[38;5;241m+\u001b[39m i, current_epoch\u001b[38;5;241m=\u001b[39mepoch)\n\u001b[1;32m     13\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_dict\u001b[38;5;241m.\u001b[39mget_sum()\n",
      "File \u001b[0;32m~/.conda/envs/xlayer/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/xlayer/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[18], line 34\u001b[0m, in \u001b[0;36mUnetWrapper.forward\u001b[0;34m(self, batch, out)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, out: \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m     33\u001b[0m     input_xyz \u001b[38;5;241m=\u001b[39m batch[DatasetSpec\u001b[38;5;241m.\u001b[39mINPUT_PC]\n\u001b[0;32m---> 34\u001b[0m     hash_tree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_hash_tree_from_grid\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_xyz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     input_grid \u001b[38;5;241m=\u001b[39m hash_tree[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     36\u001b[0m     batch\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_grid\u001b[39m\u001b[38;5;124m'\u001b[39m: input_grid})\n",
      "Cell \u001b[0;32mIn[18], line 27\u001b[0m, in \u001b[0;36mUnetWrapper.build_hash_tree_from_grid\u001b[0;34m(self, input_grid)\u001b[0m\n\u001b[1;32m     25\u001b[0m         hash_tree[depth] \u001b[38;5;241m=\u001b[39m input_grid\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m         hash_tree[depth] \u001b[38;5;241m=\u001b[39m \u001b[43mfvdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgridbatch_from_nearest_voxels_to_points\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_xyz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                                                                          \u001b[49m\u001b[43mvoxel_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvoxel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                                                                          \u001b[49m\u001b[43morigins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morigins\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hash_tree\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    out_dict = {}\n",
    "    unet_wrapper.train()\n",
    "    for i, batch in enumerate(train_dataloader_):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out_dict = unet_wrapper(batch, out_dict)\n",
    "        \n",
    "        loss_dict, metric_dict, latent_dict = loss_module(batch, out_dict, compute_metric=True, global_step=epoch * len(train_dataloader_) + i, current_epoch=epoch)\n",
    "\n",
    "        loss = loss_dict.get_sum()\n",
    "\n",
    "        loss.backward()\n",
    "            \n",
    "        # Gradient clipping\n",
    "\n",
    "        # If detect nan values, then this step is skipped\n",
    "        has_nan_value_cnt = 0\n",
    "        for p in filter(lambda p: p.grad is not None, unet_wrapper.parameters()):\n",
    "            if torch.any(p.grad.data != p.grad.data):\n",
    "                has_nan_value_cnt += 1\n",
    "        if has_nan_value_cnt > 0:\n",
    "            exp.logger.warning(f\"{has_nan_value_cnt} parameters get nan-gradient -- this step will be skipped.\")\n",
    "            for p in filter(lambda p: p.grad is not None, unet_wrapper.parameters()):\n",
    "                p.grad.data.zero_()\n",
    "\n",
    "\n",
    "        torch.nn.utils.clip_grad_value_(unet_wrapper.parameters(), clip_value=grad_clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{epochs}], Step [{i}/{len(train_dataloader_)}], Loss: {loss.item()}\")\n",
    "            \n",
    "    # Validation\n",
    "    unet_wrapper.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_dataloader_):\n",
    "            out_dict = unet_wrapper(batch, out_dict)\n",
    "        \n",
    "            loss_dict, metric_dict, latent_dict = loss_module(batch, out_dict, compute_metric=True, global_step=epoch * len(train_dataloader_) + i, current_epoch=epoch)\n",
    "\n",
    "            loss = loss_dict.get_sum()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Epoch [{epoch}/{epochs}], Step [{i}/{len(val_dataloader_)}], Loss: {loss.item()}\")\n",
    "\n",
    "    # Save the model if validation loss is improved\n",
    "    if loss < best_val_loss:\n",
    "        best_val_loss = loss\n",
    "        print(f\"Validation loss improved to {best_val_loss}. Saving model...\")\n",
    "            \n",
    "        # Save the model\n",
    "        unet_wrapper.save_checkpoint(f\"best_vae_model.pth\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlayer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
