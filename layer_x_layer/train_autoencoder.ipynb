{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1 torch\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import gc\n",
    "import importlib\n",
    "import inspect\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import shutil\n",
    "import traceback\n",
    "from collections import OrderedDict, defaultdict\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Mapping, Optional, Union\n",
    "from utils.Dataspec import DatasetSpec\n",
    "\n",
    "\n",
    "# Third-party libraries - NumPy & Scientific\n",
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "\n",
    "# Third-party libraries - PyTorch\n",
    "import torch\n",
    "print (torch.__version__, \"torch\")\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "# from torch.utils.tensorboard.summary import hparams\n",
    "\n",
    "# Third-party libraries - Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "# Third-party libraries - ML Tools\n",
    "from omegaconf import DictConfig, ListConfig, OmegaConf\n",
    "import omegaconf.errors\n",
    "\n",
    "# Local imports\n",
    "# from ext import common\n",
    "import fvdb\n",
    "import fvdb.nn as fvnn\n",
    "from fvdb import JaggedTensor, GridBatch\n",
    "from fvdb.nn import VDBTensor\n",
    "\n",
    "\n",
    "# Local imports\n",
    "# from modules.autoencoding.hparams import hparams_handler\n",
    "# from utils.loss_util import AverageMeter\n",
    "# from utils.loss_util import TorchLossMeter\n",
    "# from utils import exp \n",
    "\n",
    "# from modules.autoencoding.sunet import StructPredictionNet \n",
    "\n",
    "# from utils.vis_util import vis_pcs\n",
    "\n",
    "\n",
    "# from modules.diffusionmodules.schedulers.scheduling_ddim import DDIMScheduler\n",
    "# from modules.diffusionmodules.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "# from modules.diffusionmodules.schedulers.scheduling_dpmpp_2m import DPMSolverMultistepScheduler\n",
    "\n",
    "\n",
    "# from modules.diffusionmodules.ema import LitEma\n",
    "\n",
    "# Why aren't these used??????\n",
    "from modules.encoders import (SemanticEncoder, ClassEmbedder, PointNetEncoder,\n",
    "                                    StructEncoder, StructEncoder3D, StructEncoder3D_remain_h, StructEncoder3D_v2)\n",
    "\n",
    "from modules.autoencoding.sunet import StructPredictionNet\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "batch_size = 4\n",
    "tree_depth = 3 # according to 512x512x512 -> 128x128x128\n",
    "voxel_size = 0.0025\n",
    "resolution = 512\n",
    "use_fvdb_loader = True\n",
    "use_hash_tree = True # use hash tree means use early dilation (description in Sec 3.4) \n",
    "\n",
    "\n",
    "with_semantic_branch = False\n",
    "extract_mesh = \"store_true\"\n",
    "\n",
    "solver_order =\"3\"\n",
    "\n",
    "use_dpm = \"store_true\"\n",
    "\n",
    "ddim_step = 50\n",
    "\n",
    "use_ddim = \"store_true\"\n",
    "\n",
    "ema = \"store_true\"\n",
    "\n",
    "batch_len = 64\n",
    "\n",
    "toal_len = 700\n",
    "\n",
    "seed = 0\n",
    "\n",
    "world_size = 1\n",
    "\n",
    "# setup input\n",
    "use_input_normal = True\n",
    "use_input_semantic = False\n",
    "use_input_intensity = False\n",
    "\n",
    "# setup KL loss\n",
    "cut_ratio = 16 # reduce the dimension of the latent space\n",
    "kl_weight = 1.0 # activate when anneal is off\n",
    "normalize_kld = True\n",
    "enable_anneal = False\n",
    "kl_weight_min = 1e-7\n",
    "kl_weight_max = 1.0\n",
    "anneal_star_iter = 0\n",
    "anneal_end_iter = 70000 # need to adjust for different dataset\n",
    "\n",
    "\n",
    "structure_weight = 20.0\n",
    "normal_weight = 300.0\n",
    "  \n",
    "\n",
    "learning_rate = {\n",
    "  \"init\": 1.0e-4,\n",
    "  \"decay_mult\": 0.7,\n",
    "  \"decay_step\": 50000,\n",
    "  \"clip\": 1.0e-6\n",
    "}\n",
    "weight_decay = 0.0\n",
    "grad_clip = 0.5\n",
    "\n",
    "c_dim = 32\n",
    "  \n",
    "# unet parameters\n",
    "in_channels = 32\n",
    "num_blocks = tree_depth\n",
    "f_maps = 32\n",
    "neck_dense_type = \"UNCHANGED\"\n",
    "neck_bound = [64, 64, 64] # useless but indicate here\n",
    "num_res_blocks = 1\n",
    "use_residual = False\n",
    "order = \"gcr\"\n",
    "is_add_dec = False\n",
    "use_attention = False\n",
    "use_checkpoint = False\n",
    "\n",
    "\n",
    "_custom_name =  \"objaverse\"\n",
    "_objaverse_path = \"/home/benzshawelt/Kedziora/kedziora_research/layer_x_layer/data_gen/voxels/512\"\n",
    "_split_path = \"/home/benzshawelt/Kedziora/kedziora_research/layer_x_layer/data_gen/voxels/\"\n",
    "_text_emb_path = \"\"\n",
    "_null_embed_path = \"./assets/null_text_emb.pkl\"\n",
    "max_text_len= 77\n",
    "text_embed_drop_prob= 0.1\n",
    "\n",
    "train_dataset = \"ObjaverseDataset\"\n",
    "train_val_num_workers= 16\n",
    "train_kwargs = {\n",
    "  \"onet_base_path\": _objaverse_path ,\n",
    "  \"resolution\": resolution,\n",
    "  \"custom_name\": _custom_name,\n",
    "  \"split_base_path\": _split_path,\n",
    "  \"split\": \"train\",\n",
    "  \"text_emb_path\": _text_emb_path,\n",
    "  \"null_embed_path\": _null_embed_path,\n",
    "  \"max_text_len\": max_text_len,\n",
    "  \"text_embed_drop_prob\": text_embed_drop_prob, # ! classifier-free training\n",
    "  \"random_seed\": 0\n",
    "}\n",
    "\n",
    "val_dataset = \"ObjaverseDataset\"\n",
    "val_kwargs = {\n",
    "  \"onet_base_path\": _objaverse_path ,\n",
    "  \"resolution\": resolution,\n",
    "  \"custom_name\": _custom_name,\n",
    "  \"split_base_path\": _split_path,\n",
    "  \"split\": \"test\",\n",
    "  \"text_emb_path\": _text_emb_path,\n",
    "  \"null_embed_path\": _null_embed_path,\n",
    "  \"max_text_len\": max_text_len,\n",
    "  \"random_seed\": \"fixed\"\n",
    "}\n",
    "\n",
    "test_dataset = \"ObjaverseDataset\"\n",
    "test_num_workers =8\n",
    "test_kwargs = {\n",
    "  \"onet_base_path\": _objaverse_path ,\n",
    "  \"resolution\": resolution,\n",
    "  \"custom_name\": _custom_name,\n",
    "  \"split_base_path\": _split_path,\n",
    "  \"split\": \"test\",\n",
    "  \"text_emb_path\": _text_emb_path,\n",
    "  \"null_embed_path\": _null_embed_path,\n",
    "  \"max_text_len\": max_text_len,\n",
    "  \"random_seed\": \"fixed\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparametrize(mu, logvar):\n",
    "    std = logvar.div(2).exp()\n",
    "    eps = Variable(std.data.new(std.size()).normal_())\n",
    "    return mu + std*eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.autoencoding.base_encoder import Encoder\n",
    "class UnetWrapper(nn.Module):\n",
    "    def __init__(self, unet, encoder_hparams):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.unet = unet\n",
    "\n",
    "    def build_hash_tree_from_grid(self, input_grid):\n",
    "        hash_tree = {}\n",
    "        input_xyz = input_grid.grid_to_world(input_grid.ijk.float())\n",
    "        \n",
    "        for depth in range(self.hparams.tree_depth):\n",
    "            if depth != 0 and not self.hparams.use_hash_tree:\n",
    "                break            \n",
    "            voxel_size = [sv * 2 ** depth for sv in self.hparams.voxel_size]\n",
    "            origins = [sv / 2. for sv in voxel_size]\n",
    "            \n",
    "            if depth == 0:\n",
    "                hash_tree[depth] = input_grid\n",
    "            else:\n",
    "                hash_tree[depth] = fvdb.sparse_grid_from_nearest_voxels_to_points(input_xyz, \n",
    "                                                                                  voxel_sizes=voxel_size, \n",
    "                                                                                  origins=origins)\n",
    "        return hash_tree\n",
    "\n",
    "    def forward(self, batch, out: dict):\n",
    "        input_xyz = batch[DatasetSpec.INPUT_PC]\n",
    "        hash_tree = self.build_hash_tree_from_grid(input_xyz)\n",
    "        input_grid = hash_tree[0]\n",
    "        batch.update({'input_grid': input_grid})\n",
    "\n",
    "        if not self.hparams.use_hash_tree:\n",
    "            hash_tree = None\n",
    "                \n",
    "        unet_feat = self.encoder(input_grid, batch)\n",
    "        unet_feat = fvnn.VDBTensor(input_grid, input_grid.jagged_like(unet_feat))\n",
    "        unet_res, unet_output, dist_features = self.unet(unet_feat, hash_tree)\n",
    "\n",
    "        out.update({'tree': unet_res.structure_grid})\n",
    "        out.update({\n",
    "            'structure_features': unet_res.structure_features,\n",
    "            'dist_features': dist_features,\n",
    "        })\n",
    "        out.update({'gt_grid': input_grid})\n",
    "        out.update({'gt_tree': hash_tree})\n",
    "        \n",
    "        if self.hparams.with_normal_branch:\n",
    "            out.update({\n",
    "                'normal_features': unet_res.normal_features,\n",
    "            })\n",
    "        if self.hparams.with_semantic_branch:\n",
    "            out.update({\n",
    "                'semantic_features': unet_res.semantic_features,\n",
    "            })\n",
    "        if self.hparams.with_color_branch:\n",
    "            out.update({\n",
    "                'color_features': unet_res.color_features,\n",
    "            })\n",
    "        return out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _encode(self, batch, use_mode=False):\n",
    "        input_xyz = batch[DatasetSpec.INPUT_PC]\n",
    "        hash_tree = self.build_hash_tree_from_grid(input_xyz)\n",
    "        input_grid = hash_tree[0]\n",
    "        batch.update({'input_grid': input_grid})\n",
    "\n",
    "        if not self.hparams.use_hash_tree:\n",
    "            hash_tree = None\n",
    "\n",
    "        unet_feat = self.encoder(input_grid, batch)\n",
    "        unet_feat = fvnn.VDBTensor(input_grid, input_grid.jagged_like(unet_feat))\n",
    "        _, x, mu, log_sigma = self.unet.encode(unet_feat, hash_tree=hash_tree)\n",
    "        if use_mode:\n",
    "            sparse_feature = mu\n",
    "        else:\n",
    "            sparse_feature = reparametrize(mu, log_sigma)\n",
    "        \n",
    "        return fvnn.VDBTensor(x.grid, x.grid.jagged_like(sparse_feature))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the paths of all of the voxelized shapes, the shapes are stored in .pkl files within 2 folder depths of the base path\n",
    "def get_all_paths(base_path):\n",
    "  import os\n",
    "  # list all of the files in the base path\n",
    "  # print(\"base path\", base_path)\n",
    "  # print(os.listdir(base_path))\n",
    "  all_paths = []\n",
    "  for root, dirs, files in os.walk(base_path):\n",
    "    for file in files:\n",
    "      if file.endswith(\".pkl\"):\n",
    "        all_paths.append(os.path.join(root, file.split(\".\")[0]))\n",
    "        \n",
    "  return all_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, train_split_ratio, test_split_ratio, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    indices = np.arange(len(dataset))\n",
    "    np.random.shuffle(indices)\n",
    "    train_split = int(len(dataset) * train_split_ratio)\n",
    "    # always take .10 of the dataset for test (removed from val)\n",
    "    test_split = int(len(dataset) * test_split_ratio) + train_split\n",
    "\n",
    "    #return the train, val, and test datasets\n",
    "    return torch.utils.data.Subset(dataset, indices[:train_split]), torch.utils.data.Subset(dataset, indices[train_split:test_split]), torch.utils.data.Subset(dataset, indices[test_split:])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_split_paths(base_path, split_ratio, seed=0):\n",
    "    all_paths = get_all_paths(base_path)\n",
    "    train_paths, val_paths, test_split= split_dataset(all_paths, split_ratio, 0.1, seed)\n",
    "\n",
    "    # Save the split paths as a lst file\n",
    "    split_base_path = Path(base_path).parent\n",
    "    train_split_path = split_base_path / \"train.lst\"\n",
    "    val_split_path = split_base_path / \"val.lst\"\n",
    "    test_split_path = split_base_path / \"test.lst\"\n",
    "    \n",
    "\n",
    "    # delete the files if they already exist\n",
    "    if train_split_path.exists():\n",
    "        train_split_path.unlink()\n",
    "    if val_split_path.exists():\n",
    "        val_split_path.unlink()\n",
    "    if test_split_path.exists():\n",
    "        test_split_path.unlink()\n",
    "        \n",
    "    # write the paths to the files\n",
    "\n",
    "    with open(test_split_path, \"w\") as f:\n",
    "        for path in test_split:\n",
    "            f.write(f\"{path}\\n\")\n",
    "    with open(train_split_path, \"w\") as f:\n",
    "        for path in train_paths:\n",
    "            f.write(f\"{path}\\n\")\n",
    "    with open(val_split_path, \"w\") as f:\n",
    "        for path in val_paths:\n",
    "            f.write(f\"{path}\\n\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_split_paths(_objaverse_path , 0.8, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_lr_wrapper(it, lr_config, batch_size, accumulate_grad_batches=1):\n",
    "    return max(\n",
    "        lr_config['decay_mult'] ** (int(it * batch_size * accumulate_grad_batches / lr_config['decay_step'])),\n",
    "        lr_config['clip'] / lr_config['init'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "u_net = StructPredictionNet(\n",
    "  in_channels=in_channels,\n",
    "  num_blocks=num_blocks,\n",
    "  f_maps=f_maps,\n",
    "  neck_dense_type=neck_dense_type,\n",
    "  neck_bound=neck_bound,\n",
    "  num_res_blocks=num_res_blocks,\n",
    "  use_residual=use_residual,\n",
    "  order=order,\n",
    "  is_add_dec=is_add_dec,\n",
    "  use_attention=use_attention,\n",
    "  use_checkpoint=use_checkpoint,\n",
    "  c_dim=c_dim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(u_net.parameters(), lr=learning_rate[\"init\"],\n",
    "                                    weight_decay=weight_decay, amsgrad=True)\n",
    "\n",
    "scheduler = LambdaLR(optimizer,\n",
    "                lr_lambda=functools.partial(\n",
    "                    lambda_lr_wrapper, lr_config=learning_rate, batch_size=batch_size))\n",
    "\n",
    "# exp.global_var_manager.register_variable('skip_backward', False)\n",
    "\n",
    "def list_collate(batch):\n",
    "    \"\"\"\n",
    "    This just do not stack batch dimension.\n",
    "    \"\"\"\n",
    "    \n",
    "    elem = None\n",
    "    for e in batch:\n",
    "        if e is not None:\n",
    "            elem = e\n",
    "            break\n",
    "    elem_type = type(elem)\n",
    "    if isinstance(elem, torch.Tensor):\n",
    "        return batch\n",
    "    elif elem_type.__module__ == 'numpy' and elem_type.__name__ != 'str_' \\\n",
    "            and elem_type.__name__ != 'string_':\n",
    "        if elem_type.__name__ == 'ndarray' or elem_type.__name__ == 'memmap':\n",
    "            return list_collate([torch.as_tensor(b) if b is not None else None for b in batch])\n",
    "        elif elem.shape == ():  # scalars\n",
    "            return torch.as_tensor(batch)\n",
    "    elif isinstance(elem, float):\n",
    "        return torch.tensor(batch, dtype=torch.float64)\n",
    "    elif isinstance(elem, int):\n",
    "        return torch.tensor(batch)\n",
    "    elif isinstance(elem, str):\n",
    "        return batch\n",
    "    elif isinstance(elem, DictConfig) or isinstance(elem, ListConfig):\n",
    "        return batch\n",
    "    elif isinstance(elem, collections.abc.Mapping):\n",
    "        return {key: list_collate([d[key] for d in batch]) for key in elem}\n",
    "    elif isinstance(elem, collections.abc.Sequence):\n",
    "        # check to make sure that the elements in batch have consistent size\n",
    "        it = iter(batch)\n",
    "        elem_size = len(next(it))\n",
    "        if not all(len(elem) == elem_size for elem in it):\n",
    "            raise RuntimeError('each element in list of batch should be of equal size')\n",
    "        transposed = zip(*batch)\n",
    "        return [list_collate(samples) for samples in transposed]\n",
    "    elif isinstance(elem, GridBatch):\n",
    "        return fvdb.jcat(batch)\n",
    "    \n",
    "    return batch\n",
    "\n",
    "def get_dataset_spec():\n",
    "    DS = DatasetSpec\n",
    "    all_specs = [DS.SHAPE_NAME, DS.INPUT_PC,\n",
    "                    DS.GT_DENSE_PC, DS.GT_GEOMETRY]\n",
    "    if use_input_normal:\n",
    "        all_specs.append(DS.TARGET_NORMAL)\n",
    "        all_specs.append(DS.GT_DENSE_NORMAL)\n",
    "    if use_input_semantic or with_semantic_branch:\n",
    "        all_specs.append(DS.GT_SEMANTIC)\n",
    "    if use_input_intensity:\n",
    "        all_specs.append(DS.INPUT_INTENSITY)\n",
    "    return all_specs\n",
    "\n",
    "\n",
    "\n",
    "def train_dataloader():\n",
    "    from data.objaverse import ObjaverseDataset\n",
    "    train_set =  ObjaverseDataset(onet_base_path=train_kwargs[\"onet_base_path\"], \n",
    "                                  spec=get_dataset_spec(), \n",
    "                                  split=train_kwargs[\"split\"], \n",
    "                                  resolution=train_kwargs[\"resolution\"], \n",
    "                                  image_base_path=None, \n",
    "                                  random_seed=0, \n",
    "                                  hparams=None, \n",
    "                                  skip_on_error=False, \n",
    "                                  custom_name=\"objaverse\", \n",
    "                                  text_emb_path=\"../data/objaverse/objaverse/text_emb\", \n",
    "                                  null_embed_path=\"./assets/null_text_emb.pkl\", \n",
    "                                  text_embed_drop_prob=0.0, \n",
    "                                  max_text_len=77, \n",
    "                                  duplicate_num=1, \n",
    "                                  split_base_path=_split_path,\n",
    "                                  )\n",
    "        \n",
    "    return DataLoader(train_set, batch_size=batch_size // world_size, shuffle=True,\n",
    "                        num_workers=train_val_num_workers, collate_fn=list_collate)\n",
    "\n",
    "\n",
    "# print(get_dataset_spec())\n",
    "\n",
    "def val_dataloader():\n",
    "    from data.objaverse import ObjaverseDataset\n",
    "    val_set = ObjaverseDataset(onet_base_path=val_kwargs[\"onet_base_path\"],\n",
    "                                spec=get_dataset_spec(), \n",
    "                                split=val_kwargs[\"split\"], \n",
    "                                resolution=val_kwargs[\"resolution\"], \n",
    "                                image_base_path=None, \n",
    "                                random_seed=0, \n",
    "                                hparams=None, \n",
    "                                skip_on_error=False, \n",
    "                                custom_name=\"objaverse\", \n",
    "                                text_emb_path=\"../data/objaverse/objaverse/text_emb\", \n",
    "                                null_embed_path=\"./assets/null_text_emb.pkl\", \n",
    "                                text_embed_drop_prob=0.0, \n",
    "                                max_text_len=77, \n",
    "                                duplicate_num=1, \n",
    "                                split_base_path=_split_path,\n",
    "                                )\n",
    "\n",
    "\n",
    "    return DataLoader(val_set, batch_size=batch_size // world_size, shuffle=False,\n",
    "                        num_workers=train_val_num_workers, collate_fn=list_collate)\n",
    "\n",
    "def test_dataloader(resolution=resolution, test_set_shuffle=False):\n",
    "    from data.objaverse import ObjaverseDataset\n",
    "    resolution = resolution # ! use for testing when training on X^3 but testing on Y^3\n",
    "\n",
    "    test_set =  ObjaverseDataset(onet_base_path=test_kwargs[\"onet_base_path\"],\n",
    "                                spec=get_dataset_spec(), \n",
    "                                split=test_kwargs[\"split\"], \n",
    "                                resolution=resolution, \n",
    "                                image_base_path=None, \n",
    "                                random_seed=0, \n",
    "                                hparams=None, \n",
    "                                skip_on_error=False, \n",
    "                                custom_name=\"objaverse\", \n",
    "                                text_emb_path=\"../data/objaverse/objaverse/text_emb\", \n",
    "                                null_embed_path=\"./assets/null_text_emb.pkl\", \n",
    "                                text_embed_drop_prob=0.0, \n",
    "                                max_text_len=77, \n",
    "                                duplicate_num=1, \n",
    "                                split_base_path=_split_path,\n",
    "                                )\n",
    "    \n",
    "    if test_set_shuffle:\n",
    "        torch.manual_seed(0)\n",
    "    return DataLoader(test_set, batch_size=1, shuffle=test_set_shuffle, \n",
    "                        num_workers=0, collate_fn=list_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benzshawelt/.conda/envs/xlayer/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "train_dataloader_ = train_dataloader()\n",
    "val_dataloader_ = val_dataloader()\n",
    "test_dataloader_ = test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.exp as exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benzshawelt/.conda/envs/xlayer/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/home/benzshawelt/Kedziora/kedziora_research/layer_x_layer/data/objaverse.py:199: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  input_data = torch.load(os.path.join(self.onet_base_path, category, model) + \".pkl\")\n",
      "/home/benzshawelt/Kedziora/kedziora_research/layer_x_layer/data/objaverse.py:199: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  input_data = torch.load(os.path.join(self.onet_base_path, category, model) + \".pkl\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<DatasetSpec.SHAPE_NAME: 100>: '000-000/001559b408504642831e6c604eaf583f', <DatasetSpec.TARGET_NORMAL: 300>: tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]]), <DatasetSpec.INPUT_PC: 200>: <fvdb._Cpp.GridBatch object at 0x7fd101237130>, <DatasetSpec.GT_DENSE_PC: 400>: <fvdb._Cpp.GridBatch object at 0x7fd101237130>, <DatasetSpec.GT_DENSE_NORMAL: 500>: tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])}{<DatasetSpec.SHAPE_NAME: 100>: '000-000/000a82b4e6bf4e909fbe5a3b0e6d67dc', <DatasetSpec.TARGET_NORMAL: 300>: tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]]), <DatasetSpec.INPUT_PC: 200>: <fvdb._Cpp.GridBatch object at 0x7fcdc8e0e630>, <DatasetSpec.GT_DENSE_PC: 400>: <fvdb._Cpp.GridBatch object at 0x7fcdc8e0e630>, <DatasetSpec.GT_DENSE_NORMAL: 500>: tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])}\n",
      "\n",
      "{<DatasetSpec.SHAPE_NAME: 100>: '000-000/000a00944e294f7a94f95d420fdd45eb', <DatasetSpec.TARGET_NORMAL: 300>: tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]]), <DatasetSpec.INPUT_PC: 200>: <fvdb._Cpp.GridBatch object at 0x7fcdc8de7470>, <DatasetSpec.GT_DENSE_PC: 400>: <fvdb._Cpp.GridBatch object at 0x7fcdc8de7470>, <DatasetSpec.GT_DENSE_NORMAL: 500>: tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])}{<DatasetSpec.SHAPE_NAME: 100>: '000-000/00124bcf3ca3463fbe05f28218cc0f5c', <DatasetSpec.TARGET_NORMAL: 300>: tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]]), <DatasetSpec.INPUT_PC: 200>: <fvdb._Cpp.GridBatch object at 0x7fd101248ff0>, <DatasetSpec.GT_DENSE_PC: 400>: <fvdb._Cpp.GridBatch object at 0x7fd101248ff0>, <DatasetSpec.GT_DENSE_NORMAL: 500>: tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])}\n",
      "\n",
      "{<DatasetSpec.SHAPE_NAME: 100>: '000-000/0013bdaec08345ec9fd03214030baeb2', <DatasetSpec.TARGET_NORMAL: 300>: tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]]), <DatasetSpec.INPUT_PC: 200>: <fvdb._Cpp.GridBatch object at 0x7fd101236f70>, <DatasetSpec.GT_DENSE_PC: 400>: <fvdb._Cpp.GridBatch object at 0x7fd101236f70>, <DatasetSpec.GT_DENSE_NORMAL: 500>: tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])}\n",
      "{<DatasetSpec.SHAPE_NAME: 100>: '000-000/0028b77436394fd7963c013e04e69e70', <DatasetSpec.TARGET_NORMAL: 300>: tensor([[0., 0., 0.]]), <DatasetSpec.INPUT_PC: 200>: <fvdb._Cpp.GridBatch object at 0x7fd1010873f0>, <DatasetSpec.GT_DENSE_PC: 400>: <fvdb._Cpp.GridBatch object at 0x7fd1010873f0>, <DatasetSpec.GT_DENSE_NORMAL: 500>: tensor([[0., 0., 0.]])}\n",
      "<fvdb._Cpp.GridBatch object at 0x7fcdcab23370>\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "jagged_like(): incompatible function arguments. The following argument types are supported:\n    1. (self: fvdb._Cpp.GridBatch, data: torch.Tensor, ignore_disabled: bool = True) -> fvdb::JaggedTensor\n\nInvoked with: <fvdb._Cpp.GridBatch object at 0x7fd101070df0>, [tensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]]), tensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        ...,\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]]), tensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]]), tensor([[0., 0., 0.]])]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# sinput = VDBTensor(, )\u001b[39;00m\n\u001b[1;32m     11\u001b[0m grids \u001b[38;5;241m=\u001b[39m batch[DatasetSpec\u001b[38;5;241m.\u001b[39mINPUT_PC]\n\u001b[0;32m---> 12\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mgrids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjagged_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mDatasetSpec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTARGET_NORMAL\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m sinput \u001b[38;5;241m=\u001b[39m VDBTensor(batch[DatasetSpec\u001b[38;5;241m.\u001b[39mINPUT_PC],batch[DatasetSpec\u001b[38;5;241m.\u001b[39mTARGET_NORMAL])\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m model(sinput, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: jagged_like(): incompatible function arguments. The following argument types are supported:\n    1. (self: fvdb._Cpp.GridBatch, data: torch.Tensor, ignore_disabled: bool = True) -> fvdb::JaggedTensor\n\nInvoked with: <fvdb._Cpp.GridBatch object at 0x7fd101070df0>, [tensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]]), tensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        ...,\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]]), tensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]]), tensor([[0., 0., 0.]])]"
     ]
    }
   ],
   "source": [
    "# Define the training step\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    u_net.train()\n",
    "    for i, batch in enumerate(train_dataloader_):\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "\n",
    "        \n",
    "        print(batch[DatasetSpec.INPUT_PC][0])\n",
    "        print(batch[DatasetSpec.TARGET_NORMAL][0])\n",
    "        # sinput = VDBTensor(, )\n",
    "        grids = batch[DatasetSpec.INPUT_PC]\n",
    "        features = grids.jagged_like(batch[DatasetSpec.TARGET_NORMAL])\n",
    "        sinput = VDBTensor(batch[DatasetSpec.INPUT_PC],batch[DatasetSpec.TARGET_NORMAL])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        loss = u_net(sinput, None)\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "\n",
    "        # If detect nan values, then this step is skipped\n",
    "        has_nan_value_cnt = 0\n",
    "        for p in filter(lambda p: p.grad is not None, u_net.parameters()):\n",
    "            if torch.any(p.grad.data != p.grad.data):\n",
    "                has_nan_value_cnt += 1\n",
    "        if has_nan_value_cnt > 0:\n",
    "            exp.logger.warning(f\"{has_nan_value_cnt} parameters get nan-gradient -- this step will be skipped.\")\n",
    "            for p in filter(lambda p: p.grad is not None, u_net.parameters()):\n",
    "                p.grad.data.zero_()\n",
    "\n",
    "\n",
    "        torch.nn.utils.clip_grad_value_(u_net.parameters(), clip_value=grad_clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{epochs}], Step [{i}/{len(train_dataloader)}], Loss: {loss.item()}\")\n",
    "            \n",
    "    # Validation\n",
    "    u_net.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_dataloader):\n",
    "            loss = u_net(batch)\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch [{epoch}/{epochs}], Step [{i}/{len(val_dataloader)}], Loss: {loss.item()}\")\n",
    "            \n",
    "    # Save the model\n",
    "    torch.save(u_net.state_dict(), f\"model_{epoch}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlayer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
