{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1 torch\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import gc\n",
    "import importlib\n",
    "import inspect\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import shutil\n",
    "import traceback\n",
    "from collections import OrderedDict, defaultdict\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Mapping, Optional, Union\n",
    "from utils.Dataspec import DatasetSpec\n",
    "\n",
    "\n",
    "# Third-party libraries - NumPy & Scientific\n",
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "\n",
    "# Third-party libraries - PyTorch\n",
    "import torch\n",
    "print (torch.__version__, \"torch\")\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "# from torch.utils.tensorboard.summary import hparams\n",
    "\n",
    "# Third-party libraries - Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "# Third-party libraries - ML Tools\n",
    "from omegaconf import DictConfig, ListConfig, OmegaConf\n",
    "import omegaconf.errors\n",
    "\n",
    "# Local imports\n",
    "# from ext import common\n",
    "import fvdb\n",
    "import fvdb.nn as fvnn\n",
    "from fvdb import JaggedTensor, GridBatch\n",
    "from fvdb.nn import VDBTensor\n",
    "\n",
    "\n",
    "from modules.autoencoding.sunet import StructPredictionNet\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "batch_size = 4\n",
    "# Tree depth is what builds the different resolutions of the voxel grid.\n",
    "# For example, a tree depth of 3 with a resolution of 512 would give you\n",
    "# a grid of 128x128x128. The depth of the tree determines how many times\n",
    "# the original grid is downsampled. So, a depth of 3 means the original\n",
    "# grid is downsampled three times, resulting in a final resolution of\n",
    "# 128x128x128.\n",
    "tree_depth = 3 # according to 512x512x512 -> 128x128x128\n",
    "voxel_size = 0.0025\n",
    "resolution = 512\n",
    "use_fvdb_loader = True\n",
    "use_hash_tree = True # use hash tree means use early dilation (description in Sec 3.4) \n",
    "\n",
    "\n",
    "with_semantic_branch = False\n",
    "extract_mesh = \"store_true\"\n",
    "\n",
    "solver_order =\"3\"\n",
    "\n",
    "use_dpm = \"store_true\"\n",
    "\n",
    "ddim_step = 50\n",
    "\n",
    "use_ddim = \"store_true\"\n",
    "\n",
    "ema = \"store_true\"\n",
    "\n",
    "batch_len = 64\n",
    "\n",
    "toal_len = 700\n",
    "\n",
    "seed = 0\n",
    "\n",
    "world_size = 1\n",
    "\n",
    "# setup input\n",
    "use_input_normal = True\n",
    "use_input_semantic = False\n",
    "use_input_intensity = False\n",
    "use_input_color = False\n",
    "\n",
    "# setup KL loss\n",
    "cut_ratio = 16 # reduce the dimension of the latent space\n",
    "kl_weight = 1.0 # activate when anneal is off\n",
    "normalize_kld = True\n",
    "enable_anneal = False\n",
    "kl_weight_min = 1e-7\n",
    "kl_weight_max = 1.0\n",
    "anneal_star_iter = 0\n",
    "anneal_end_iter = 70000 # need to adjust for different dataset\n",
    "\n",
    "\n",
    "structure_weight = 20.0\n",
    "normal_weight = 300.0\n",
    "  \n",
    "\n",
    "learning_rate = {\n",
    "  \"init\": 1.0e-4,\n",
    "  \"decay_mult\": 0.7,\n",
    "  \"decay_step\": 50000,\n",
    "  \"clip\": 1.0e-6\n",
    "}\n",
    "weight_decay = 0.0\n",
    "grad_clip = 0.5\n",
    "\n",
    "c_dim = 32\n",
    "  \n",
    "# unet parameters\n",
    "in_channels = 32\n",
    "num_blocks = tree_depth\n",
    "f_maps = 32\n",
    "neck_dense_type = \"UNCHANGED\"\n",
    "neck_bound = [64, 64, 64] # useless but indicate here\n",
    "num_res_blocks = 1\n",
    "use_residual = False\n",
    "order = \"gcr\"\n",
    "is_add_dec = False\n",
    "use_attention = False\n",
    "use_checkpoint = False\n",
    "\n",
    "\n",
    "_custom_name =  \"objaverse\"\n",
    "_objaverse_path = \"/home/benzshawelt/Kedziora/kedziora_research/layer_x_layer/data_gen/voxels/512\"\n",
    "_split_path = \"/home/benzshawelt/Kedziora/kedziora_research/layer_x_layer/data_gen/voxels/\"\n",
    "_text_emb_path = \"\"\n",
    "_null_embed_path = \"./assets/null_text_emb.pkl\"\n",
    "max_text_len= 77\n",
    "text_embed_drop_prob= 0.1\n",
    "\n",
    "train_dataset = \"ObjaverseDataset\"\n",
    "train_val_num_workers= 16\n",
    "train_kwargs = {\n",
    "  \"onet_base_path\": _objaverse_path ,\n",
    "  \"resolution\": resolution,\n",
    "  \"custom_name\": _custom_name,\n",
    "  \"split_base_path\": _split_path,\n",
    "  \"split\": \"train\",\n",
    "  \"text_emb_path\": _text_emb_path,\n",
    "  \"null_embed_path\": _null_embed_path,\n",
    "  \"max_text_len\": max_text_len,\n",
    "  \"text_embed_drop_prob\": text_embed_drop_prob, # ! classifier-free training\n",
    "  \"random_seed\": 0\n",
    "}\n",
    "\n",
    "val_dataset = \"ObjaverseDataset\"\n",
    "val_kwargs = {\n",
    "  \"onet_base_path\": _objaverse_path ,\n",
    "  \"resolution\": resolution,\n",
    "  \"custom_name\": _custom_name,\n",
    "  \"split_base_path\": _split_path,\n",
    "  \"split\": \"test\",\n",
    "  \"text_emb_path\": _text_emb_path,\n",
    "  \"null_embed_path\": _null_embed_path,\n",
    "  \"max_text_len\": max_text_len,\n",
    "  \"random_seed\": \"fixed\"\n",
    "}\n",
    "\n",
    "test_dataset = \"ObjaverseDataset\"\n",
    "test_num_workers =8\n",
    "test_kwargs = {\n",
    "  \"onet_base_path\": _objaverse_path ,\n",
    "  \"resolution\": resolution,\n",
    "  \"custom_name\": _custom_name,\n",
    "  \"split_base_path\": _split_path,\n",
    "  \"split\": \"test\",\n",
    "  \"text_emb_path\": _text_emb_path,\n",
    "  \"null_embed_path\": _null_embed_path,\n",
    "  \"max_text_len\": max_text_len,\n",
    "  \"random_seed\": \"fixed\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparametrize(mu, logvar):\n",
    "    std = logvar.div(2).exp()\n",
    "    eps = Variable(std.data.new(std.size()).normal_())\n",
    "    return mu + std*eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.autoencoding.base_encoder import Encoder\n",
    "class UnetWrapper(nn.Module):\n",
    "    def __init__(self, unet, hparams):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(hparams)\n",
    "        self.unet = unet\n",
    "        self.hparams = hparams\n",
    "\n",
    "    def build_hash_tree_from_grid(self, input_grid):\n",
    "        hash_tree = {}\n",
    "        input_xyz = input_grid.grid_to_world(input_grid.ijk.float())\n",
    "        \n",
    "        for depth in range(self.hparams[\"tree_depth\"]):\n",
    "            if depth != 0 and not self.hparams[\"use_hash_tree\"]:\n",
    "                break            \n",
    "            voxel_size = [sv * 2 ** depth for sv in self.hparams[\"voxel_size\"]]\n",
    "            origins = [sv / 2. for sv in voxel_size]\n",
    "            \n",
    "            if depth == 0:\n",
    "                hash_tree[depth] = input_grid\n",
    "            else:\n",
    "                hash_tree[depth] = fvdb.gridbatch_from_nearest_voxels_to_points(input_xyz, \n",
    "                                                                                  voxel_sizes=voxel_size, \n",
    "                                                                                  origins=origins)\n",
    "        return hash_tree\n",
    "\n",
    "    def forward(self, batch, out: dict):\n",
    "        input_xyz = batch[DatasetSpec.INPUT_PC]\n",
    "        hash_tree = self.build_hash_tree_from_grid(input_xyz)\n",
    "        input_grid = hash_tree[0]\n",
    "        batch.update({'input_grid': input_grid})\n",
    "\n",
    "        if not self.hparams[\"use_hash_tree\"]:\n",
    "            hash_tree = None\n",
    "                \n",
    "        unet_feat = self.encoder(input_grid, batch)\n",
    "        unet_feat = fvnn.VDBTensor(input_grid, input_grid.jagged_like(unet_feat))\n",
    "        unet_res, unet_output, dist_features = self.unet(unet_feat, hash_tree)\n",
    "\n",
    "        out.update({'tree': unet_res.structure_grid})\n",
    "        out.update({\n",
    "            'structure_features': unet_res.structure_features,\n",
    "            'dist_features': dist_features,\n",
    "        })\n",
    "        out.update({'gt_grid': input_grid})\n",
    "        out.update({'gt_tree': hash_tree})\n",
    "        \n",
    "        if self.hparams[\"with_normal_branch\"]:\n",
    "            out.update({\n",
    "                'normal_features': unet_res.normal_features,\n",
    "            })\n",
    "        if self.hparams[\"with_semantic_branch\"]:\n",
    "            out.update({\n",
    "                'semantic_features': unet_res.semantic_features,\n",
    "            })\n",
    "        if self.hparams[\"with_color_branch\"]:\n",
    "            out.update({\n",
    "                'color_features': unet_res.color_features,\n",
    "            })\n",
    "        return out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _encode(self, batch, use_mode=False):\n",
    "        input_xyz = batch[DatasetSpec.INPUT_PC]\n",
    "        hash_tree = self.build_hash_tree_from_grid(input_xyz)\n",
    "        input_grid = hash_tree[0]\n",
    "        batch.update({'input_grid': input_grid})\n",
    "\n",
    "        if not self.hparams[\"use_hash_tree\"]:\n",
    "            hash_tree = None\n",
    "\n",
    "        unet_feat = self.encoder(input_grid, batch)\n",
    "        unet_feat = fvnn.VDBTensor(input_grid, input_grid.jagged_like(unet_feat))\n",
    "        _, x, mu, log_sigma = self.unet.encode(unet_feat, hash_tree=hash_tree)\n",
    "        if use_mode:\n",
    "            sparse_feature = mu\n",
    "        else:\n",
    "            sparse_feature = reparametrize(mu, log_sigma)\n",
    "        \n",
    "        return fvnn.VDBTensor(x.grid, x.grid.jagged_like(sparse_feature))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the paths of all of the voxelized shapes, the shapes are stored in .pkl files within 2 folder depths of the base path\n",
    "def get_all_paths(base_path):\n",
    "  import os\n",
    "  # list all of the files in the base path\n",
    "  # print(\"base path\", base_path)\n",
    "  # print(os.listdir(base_path))\n",
    "  all_paths = []\n",
    "  for root, dirs, files in os.walk(base_path):\n",
    "    for file in files:\n",
    "      if file.endswith(\".pkl\"):\n",
    "        all_paths.append(os.path.join(root, file.split(\".\")[0]))\n",
    "        \n",
    "  return all_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, train_split_ratio, test_split_ratio, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    indices = np.arange(len(dataset))\n",
    "    np.random.shuffle(indices)\n",
    "    train_split = int(len(dataset) * train_split_ratio)\n",
    "    # always take .10 of the dataset for test (removed from val)\n",
    "    test_split = int(len(dataset) * test_split_ratio) + train_split\n",
    "\n",
    "    #return the train, val, and test datasets\n",
    "    return torch.utils.data.Subset(dataset, indices[:train_split]), torch.utils.data.Subset(dataset, indices[train_split:test_split]), torch.utils.data.Subset(dataset, indices[test_split:])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_split_paths(base_path, split_ratio, seed=0):\n",
    "    all_paths = get_all_paths(base_path)\n",
    "    train_paths, val_paths, test_split= split_dataset(all_paths, split_ratio, 0.1, seed)\n",
    "\n",
    "    # Save the split paths as a lst file\n",
    "    split_base_path = Path(base_path).parent\n",
    "    train_split_path = split_base_path / \"train.lst\"\n",
    "    val_split_path = split_base_path / \"val.lst\"\n",
    "    test_split_path = split_base_path / \"test.lst\"\n",
    "    \n",
    "\n",
    "    # delete the files if they already exist\n",
    "    if train_split_path.exists():\n",
    "        train_split_path.unlink()\n",
    "    if val_split_path.exists():\n",
    "        val_split_path.unlink()\n",
    "    if test_split_path.exists():\n",
    "        test_split_path.unlink()\n",
    "        \n",
    "    # write the paths to the files\n",
    "\n",
    "    with open(test_split_path, \"w\") as f:\n",
    "        for path in test_split:\n",
    "            f.write(f\"{path}\\n\")\n",
    "    with open(train_split_path, \"w\") as f:\n",
    "        for path in train_paths:\n",
    "            f.write(f\"{path}\\n\")\n",
    "    with open(val_split_path, \"w\") as f:\n",
    "        for path in val_paths:\n",
    "            f.write(f\"{path}\\n\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_split_paths(_objaverse_path , 0.8, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_lr_wrapper(it, lr_config, batch_size, accumulate_grad_batches=1):\n",
    "    return max(\n",
    "        lr_config['decay_mult'] ** (int(it * batch_size * accumulate_grad_batches / lr_config['decay_step'])),\n",
    "        lr_config['clip'] / lr_config['init'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "u_net = StructPredictionNet(\n",
    "  in_channels=in_channels,\n",
    "  num_blocks=num_blocks,\n",
    "  f_maps=f_maps,\n",
    "  neck_dense_type=neck_dense_type,\n",
    "  neck_bound=neck_bound,\n",
    "  num_res_blocks=num_res_blocks,\n",
    "  use_residual=use_residual,\n",
    "  order=order,\n",
    "  is_add_dec=is_add_dec,\n",
    "  use_attention=use_attention,\n",
    "  use_checkpoint=use_checkpoint,\n",
    "  c_dim=c_dim\n",
    ")\n",
    "\n",
    "unet_wrapper = UnetWrapper(u_net, {\n",
    "    \"tree_depth\": tree_depth,\n",
    "    \"voxel_size\": [voxel_size, voxel_size, voxel_size],\n",
    "    \"use_hash_tree\": use_hash_tree,\n",
    "    \"use_input_normal\": use_input_normal,\n",
    "    \"use_input_semantic\": use_input_semantic,\n",
    "    \"use_input_color\": use_input_color,\n",
    "    \"use_input_intensity\": use_input_intensity,\n",
    "    \"c_dim\": c_dim,\n",
    "    \"with_normal_branch\": True,\n",
    "    \"with_semantic_branch\": with_semantic_branch,\n",
    "    \"with_color_branch\": False,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(unet_wrapper.parameters(), lr=learning_rate[\"init\"],\n",
    "                                    weight_decay=weight_decay, amsgrad=True)\n",
    "\n",
    "scheduler = LambdaLR(optimizer,\n",
    "                lr_lambda=functools.partial(\n",
    "                    lambda_lr_wrapper, lr_config=learning_rate, batch_size=batch_size))\n",
    "\n",
    "# exp.global_var_manager.register_variable('skip_backward', False)\n",
    "\n",
    "def list_collate(batch):\n",
    "    \"\"\"\n",
    "    This just do not stack batch dimension.\n",
    "    \"\"\"\n",
    "    \n",
    "    elem = None\n",
    "    for e in batch:\n",
    "        if e is not None:\n",
    "            elem = e\n",
    "            break\n",
    "    elem_type = type(elem)\n",
    "    if isinstance(elem, torch.Tensor):\n",
    "        return batch\n",
    "    elif elem_type.__module__ == 'numpy' and elem_type.__name__ != 'str_' \\\n",
    "            and elem_type.__name__ != 'string_':\n",
    "        if elem_type.__name__ == 'ndarray' or elem_type.__name__ == 'memmap':\n",
    "            return list_collate([torch.as_tensor(b) if b is not None else None for b in batch])\n",
    "        elif elem.shape == ():  # scalars\n",
    "            return torch.as_tensor(batch)\n",
    "    elif isinstance(elem, float):\n",
    "        return torch.tensor(batch, dtype=torch.float64)\n",
    "    elif isinstance(elem, int):\n",
    "        return torch.tensor(batch)\n",
    "    elif isinstance(elem, str):\n",
    "        return batch\n",
    "    elif isinstance(elem, DictConfig) or isinstance(elem, ListConfig):\n",
    "        return batch\n",
    "    elif isinstance(elem, collections.abc.Mapping):\n",
    "        return {key: list_collate([d[key] for d in batch]) for key in elem}\n",
    "    elif isinstance(elem, collections.abc.Sequence):\n",
    "        # check to make sure that the elements in batch have consistent size\n",
    "        it = iter(batch)\n",
    "        elem_size = len(next(it))\n",
    "        if not all(len(elem) == elem_size for elem in it):\n",
    "            raise RuntimeError('each element in list of batch should be of equal size')\n",
    "        transposed = zip(*batch)\n",
    "        return [list_collate(samples) for samples in transposed]\n",
    "    elif isinstance(elem, GridBatch):\n",
    "        return fvdb.jcat(batch)\n",
    "    \n",
    "    return batch\n",
    "\n",
    "def get_dataset_spec():\n",
    "    DS = DatasetSpec\n",
    "    all_specs = [DS.SHAPE_NAME, DS.INPUT_PC,\n",
    "                    DS.GT_DENSE_PC, DS.GT_GEOMETRY]\n",
    "    if use_input_normal:\n",
    "        all_specs.append(DS.TARGET_NORMAL)\n",
    "        all_specs.append(DS.GT_DENSE_NORMAL)\n",
    "    if use_input_semantic or with_semantic_branch:\n",
    "        all_specs.append(DS.GT_SEMANTIC)\n",
    "    if use_input_intensity:\n",
    "        all_specs.append(DS.INPUT_INTENSITY)\n",
    "    return all_specs\n",
    "\n",
    "\n",
    "\n",
    "def train_dataloader():\n",
    "    from data.objaverse import ObjaverseDataset\n",
    "    train_set =  ObjaverseDataset(onet_base_path=train_kwargs[\"onet_base_path\"], \n",
    "                                  spec=get_dataset_spec(), \n",
    "                                  split=train_kwargs[\"split\"], \n",
    "                                  resolution=train_kwargs[\"resolution\"], \n",
    "                                  image_base_path=None, \n",
    "                                  random_seed=0, \n",
    "                                  hparams=None, \n",
    "                                  skip_on_error=False, \n",
    "                                  custom_name=\"objaverse\", \n",
    "                                  text_emb_path=\"../data/objaverse/objaverse/text_emb\", \n",
    "                                  null_embed_path=\"./assets/null_text_emb.pkl\", \n",
    "                                  text_embed_drop_prob=0.0, \n",
    "                                  max_text_len=77, \n",
    "                                  duplicate_num=1, \n",
    "                                  split_base_path=_split_path,\n",
    "                                  )\n",
    "        \n",
    "    return DataLoader(train_set, batch_size=batch_size // world_size, shuffle=True,\n",
    "                        num_workers=train_val_num_workers, collate_fn=list_collate)\n",
    "\n",
    "\n",
    "# print(get_dataset_spec())\n",
    "\n",
    "def val_dataloader():\n",
    "    from data.objaverse import ObjaverseDataset\n",
    "    val_set = ObjaverseDataset(onet_base_path=val_kwargs[\"onet_base_path\"],\n",
    "                                spec=get_dataset_spec(), \n",
    "                                split=val_kwargs[\"split\"], \n",
    "                                resolution=val_kwargs[\"resolution\"], \n",
    "                                image_base_path=None, \n",
    "                                random_seed=0, \n",
    "                                hparams=None, \n",
    "                                skip_on_error=False, \n",
    "                                custom_name=\"objaverse\", \n",
    "                                text_emb_path=\"../data/objaverse/objaverse/text_emb\", \n",
    "                                null_embed_path=\"./assets/null_text_emb.pkl\", \n",
    "                                text_embed_drop_prob=0.0, \n",
    "                                max_text_len=77, \n",
    "                                duplicate_num=1, \n",
    "                                split_base_path=_split_path,\n",
    "                                )\n",
    "\n",
    "\n",
    "    return DataLoader(val_set, batch_size=batch_size // world_size, shuffle=False,\n",
    "                        num_workers=train_val_num_workers, collate_fn=list_collate)\n",
    "\n",
    "def test_dataloader(resolution=resolution, test_set_shuffle=False):\n",
    "    from data.objaverse import ObjaverseDataset\n",
    "    resolution = resolution # ! use for testing when training on X^3 but testing on Y^3\n",
    "\n",
    "    test_set =  ObjaverseDataset(onet_base_path=test_kwargs[\"onet_base_path\"],\n",
    "                                spec=get_dataset_spec(), \n",
    "                                split=test_kwargs[\"split\"], \n",
    "                                resolution=resolution, \n",
    "                                image_base_path=None, \n",
    "                                random_seed=0, \n",
    "                                hparams=None, \n",
    "                                skip_on_error=False, \n",
    "                                custom_name=\"objaverse\", \n",
    "                                text_emb_path=\"../data/objaverse/objaverse/text_emb\", \n",
    "                                null_embed_path=\"./assets/null_text_emb.pkl\", \n",
    "                                text_embed_drop_prob=0.0, \n",
    "                                max_text_len=77, \n",
    "                                duplicate_num=1, \n",
    "                                split_base_path=_split_path,\n",
    "                                )\n",
    "    \n",
    "    if test_set_shuffle:\n",
    "        torch.manual_seed(0)\n",
    "    return DataLoader(test_set, batch_size=1, shuffle=test_set_shuffle, \n",
    "                        num_workers=0, collate_fn=list_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benzshawelt/.conda/envs/xlayer/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "train_dataloader_ = train_dataloader()\n",
    "val_dataloader_ = val_dataloader()\n",
    "test_dataloader_ = test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.exp as exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.color_util import color_from_points, semantic_from_points\n",
    "from utils.loss_util import TorchLossMeter\n",
    "from utils.Dataspec import DatasetSpec as DS\n",
    "    \n",
    "class Loss(nn.Module):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.hparams = hparams\n",
    "        # Set default for use_fvdb_loader if not provided\n",
    "        if \"use_fvdb_loader\" not in self.hparams:\n",
    "            self.hparams[\"use_fvdb_loader\"] = True\n",
    "        # Set default for remain_h if not provided\n",
    "        if \"remain_h\" not in self.hparams:\n",
    "            self.hparams[\"remain_h\"] = False\n",
    "        # Set default kl_weight if not provided\n",
    "        if \"kl_weight\" not in self.hparams:\n",
    "            self.hparams[\"kl_weight\"] = 1.0\n",
    "\n",
    "    def transform_field(self, field: torch.Tensor):\n",
    "        gt_band = 1.0 # not sure if this will be changed\n",
    "        # For scalar voxel_size, use the first element from the list\n",
    "        voxel_size = self.hparams[\"voxel_size\"][0] if isinstance(self.hparams[\"voxel_size\"], list) else self.hparams[\"voxel_size\"]\n",
    "        truncation_size = gt_band * voxel_size\n",
    "        # non-binary supervision (made sure derivative norm at 0 if 1)\n",
    "        field = torch.tanh(field / truncation_size) * truncation_size\n",
    "        return field\n",
    "    \n",
    "    def cross_entropy(self, pd_struct: fvnn.VDBTensor, gt_grid: fvdb.GridBatch, dynamic_grid: fvdb.GridBatch = None):\n",
    "        assert torch.allclose(pd_struct.grid.origins, gt_grid.origins)\n",
    "        assert torch.allclose(pd_struct.grid.voxel_sizes, gt_grid.voxel_sizes)\n",
    "        idx_mask = gt_grid.ijk_to_index(pd_struct.grid.ijk).jdata == -1\n",
    "        idx_mask = idx_mask.long()\n",
    "        if dynamic_grid is not None:\n",
    "            dynamic_mask = dynamic_grid.ijk_to_index(pd_struct.grid.ijk).jdata == -1\n",
    "            loss = F.cross_entropy(pd_struct.jdata, idx_mask, reduction='none') * dynamic_mask.float()\n",
    "            loss = loss.mean()\n",
    "        else:\n",
    "            loss = F.cross_entropy(pd_struct.jdata, idx_mask)\n",
    "        return 0.0 if idx_mask.size(0) == 0 else loss\n",
    "    \n",
    "    def struct_acc(self, pd_struct: fvnn.VDBTensor, gt_grid: fvdb.GridBatch):\n",
    "        assert torch.allclose(pd_struct.grid.origins, gt_grid.origins)\n",
    "        assert torch.allclose(pd_struct.grid.voxel_sizes, gt_grid.voxel_sizes)\n",
    "        idx_mask = gt_grid.ijk_to_index(pd_struct.grid.ijk).jdata == -1\n",
    "        idx_mask = idx_mask.long()\n",
    "        return torch.mean((pd_struct.jdata.argmax(dim=1) == idx_mask).float())\n",
    "    \n",
    "    def grid_iou(self, gt_grid: fvdb.GridBatch, pd_grid: fvdb.GridBatch):\n",
    "        assert gt_grid.grid_count == pd_grid.grid_count\n",
    "        idx = pd_grid.ijk_to_index(gt_grid.ijk)\n",
    "        upi = (pd_grid.num_voxels + gt_grid.num_voxels).cpu().numpy().tolist()\n",
    "        ious = []\n",
    "        for i in range(len(upi)):\n",
    "            inter = torch.sum(idx[i].jdata >= 0).item()\n",
    "            ious.append(inter / (upi[i] - inter + 1.0e-6))\n",
    "        return np.mean(ious)\n",
    "\n",
    "    def normal_loss(self, batch, normal_feats: fvnn.VDBTensor, eps=1e-6):\n",
    "        if self.hparams[\"use_fvdb_loader\"]:\n",
    "            ref_grid = batch['input_grid']\n",
    "            ref_xyz = ref_grid.grid_to_world(ref_grid.ijk.float()) \n",
    "        else:\n",
    "            ref_xyz = fvdb.JaggedTensor(batch[DS.INPUT_PC])\n",
    "        \n",
    "        gt_normal = normal_feats.grid.splat_trilinear(ref_xyz, fvdb.JaggedTensor(batch[DS.TARGET_NORMAL]))\n",
    "        # normalize normal\n",
    "        gt_normal.jdata /= (gt_normal.jdata.norm(dim=1, keepdim=True) + eps)\n",
    "        normal_loss = F.l1_loss(gt_normal.jdata, normal_feats.jdata)\n",
    "        return normal_loss\n",
    "    \n",
    "    def color_loss(self, batch, color_feats: fvnn.VDBTensor):\n",
    "        assert self.hparams[\"use_fvdb_loader\"] is True\n",
    "        # check if color_feats is empty\n",
    "        if color_feats.grid.total_voxels == 0:\n",
    "            return 0.0\n",
    "        ref_grid = batch['input_grid']\n",
    "        ref_xyz = ref_grid.grid_to_world(ref_grid.ijk.float())\n",
    "        ref_color = fvdb.JaggedTensor(batch[DS.INPUT_COLOR])\n",
    "        \n",
    "        target_xyz = color_feats.grid.grid_to_world(color_feats.grid.ijk.float())\n",
    "        target_color = []\n",
    "        slect_color_feats = []\n",
    "        for batch_idx in range(ref_grid.grid_count):\n",
    "            ref_color_i = ref_color[batch_idx].jdata\n",
    "            target_color.append(color_from_points(target_xyz[batch_idx].jdata, ref_xyz[batch_idx].jdata, ref_color_i, k=1))\n",
    "            slect_color_feats.append(color_feats.feature[batch_idx].jdata)\n",
    "            \n",
    "        if len(target_color) == 0 or len(slect_color_feats) == 0: # to avoid JaggedTensor build from empty list\n",
    "            return 0.0  \n",
    "        \n",
    "        target_color = fvdb.JaggedTensor(target_color)\n",
    "        slect_color_feats = fvdb.JaggedTensor(slect_color_feats)\n",
    "        color_loss = F.l1_loss(slect_color_feats.jdata, target_color.jdata)\n",
    "        return color_loss\n",
    "    \n",
    "    def semantic_loss(self, batch, semantic_feats: fvnn.VDBTensor):\n",
    "        assert self.hparams[\"use_fvdb_loader\"] is True\n",
    "        # check if semantic_feats is empty\n",
    "        if semantic_feats.grid.total_voxels == 0:\n",
    "            return 0.0\n",
    "        ref_grid = batch['input_grid']\n",
    "        ref_xyz = ref_grid.grid_to_world(ref_grid.ijk.float())\n",
    "        ref_semantic = fvdb.JaggedTensor(batch[DS.GT_SEMANTIC])\n",
    "        if ref_semantic.jdata.size(0) == 0: # if all samples in this batch is without semantic\n",
    "            return 0.0\n",
    "                \n",
    "        target_xyz = semantic_feats.grid.grid_to_world(semantic_feats.grid.ijk.float())       \n",
    "        target_semantic = []\n",
    "        slect_semantic_feats = []\n",
    "        for batch_idx in range(ref_grid.grid_count):\n",
    "            ref_semantic_i = ref_semantic[batch_idx].jdata\n",
    "            if ref_semantic_i.size(0) == 0:\n",
    "                continue\n",
    "            target_semantic.append(semantic_from_points(target_xyz[batch_idx].jdata, ref_xyz[batch_idx].jdata, ref_semantic_i))\n",
    "            slect_semantic_feats.append(semantic_feats.feature[batch_idx].jdata)\n",
    "                    \n",
    "        if len(target_semantic) == 0 or len(slect_semantic_feats) == 0: # to avoid JaggedTensor build from empty list\n",
    "            return 0.0\n",
    "\n",
    "        target_semantic = fvdb.JaggedTensor(target_semantic)\n",
    "        slect_semantic_feats = fvdb.JaggedTensor(slect_semantic_feats)\n",
    "        \n",
    "        if slect_semantic_feats.jdata.size(0) == 0: # to aviod cross_entropy take empty tensor\n",
    "            return 0.0\n",
    "        \n",
    "        semantic_loss = F.cross_entropy(slect_semantic_feats.jdata, target_semantic.jdata.long())\n",
    "        return semantic_loss\n",
    "    \n",
    "    def get_kl_weight(self, global_step):\n",
    "        # linear annealing the kl weight\n",
    "        if global_step > self.hparams[\"anneal_star_iter\"]:\n",
    "            if global_step < self.hparams[\"anneal_end_iter\"]:\n",
    "                kl_weight = self.hparams[\"kl_weight_min\"] + \\\n",
    "                                         (self.hparams[\"kl_weight_max\"] - self.hparams[\"kl_weight_min\"]) * \\\n",
    "                                         (global_step - self.hparams[\"anneal_star_iter\"]) / \\\n",
    "                                         (self.hparams[\"anneal_end_iter\"] - self.hparams[\"anneal_star_iter\"])\n",
    "            else:\n",
    "                kl_weight = self.hparams[\"kl_weight_max\"]\n",
    "        else:\n",
    "            kl_weight = self.hparams[\"kl_weight_min\"]\n",
    "\n",
    "        return kl_weight\n",
    "\n",
    "    def forward(self, batch, out, compute_metric: bool, global_step, current_epoch, optimizer_idx=0):\n",
    "        loss_dict = TorchLossMeter()\n",
    "        metric_dict = TorchLossMeter()\n",
    "        latent_dict = TorchLossMeter()\n",
    "\n",
    "        dynamic_grid = None\n",
    "\n",
    "        if not self.hparams[\"use_hash_tree\"]:\n",
    "            gt_grid = out['gt_grid']\n",
    "            if self.hparams[\"supervision\"][\"structure_weight\"] > 0.0:\n",
    "                for feat_depth, pd_struct_i in out['structure_features'].items():\n",
    "                    downsample_factor = 2 ** feat_depth\n",
    "                    if self.hparams[\"remain_h\"]:\n",
    "                        pd_voxel_size = pd_struct_i.grid.voxel_sizes[0]\n",
    "                        h_factor = pd_voxel_size[0] // pd_voxel_size[2]\n",
    "                        downsample_factor = [downsample_factor, downsample_factor, downsample_factor // h_factor]\n",
    "                    if downsample_factor != 1:             \n",
    "                        gt_grid_i = gt_grid.coarsened_grid(downsample_factor)\n",
    "                        dyn_grid_i = dynamic_grid.coarsened_grid(downsample_factor) if dynamic_grid is not None else None\n",
    "                    else:\n",
    "                        gt_grid_i = gt_grid\n",
    "                        dyn_grid_i = dynamic_grid\n",
    "                    loss_dict.add_loss(f\"struct-{feat_depth}\", self.cross_entropy(pd_struct_i, gt_grid_i, dyn_grid_i),\n",
    "                                    self.hparams[\"supervision\"][\"structure_weight\"])\n",
    "                    if compute_metric:\n",
    "                        with torch.no_grad():\n",
    "                            metric_dict.add_loss(f\"struct-acc-{feat_depth}\", self.struct_acc(pd_struct_i, gt_grid_i))\n",
    "        else:\n",
    "            if self.hparams[\"supervision\"][\"structure_weight\"] > 0.0:\n",
    "                gt_tree = out['gt_tree']\n",
    "                for feat_depth, pd_struct_i in out['structure_features'].items():\n",
    "                    gt_grid_i = gt_tree[feat_depth]\n",
    "                    # get dynamic grid\n",
    "                    dyn_grid_i = dynamic_grid.coarsened_grid(2 ** feat_depth) if dynamic_grid is not None else None\n",
    "                    loss_dict.add_loss(f\"struct-{feat_depth}\", self.cross_entropy(pd_struct_i, gt_grid_i, dyn_grid_i),\n",
    "                                    self.hparams[\"supervision\"][\"structure_weight\"])\n",
    "                    if compute_metric:\n",
    "                        with torch.no_grad():\n",
    "                            metric_dict.add_loss(f\"struct-acc-{feat_depth}\", self.struct_acc(pd_struct_i, gt_grid_i))\n",
    "        \n",
    "        # compute normal loss\n",
    "        if self.hparams[\"with_normal_branch\"]:\n",
    "            if out['normal_features'] == {}:\n",
    "                normal_loss = 0.0\n",
    "            else:\n",
    "                feat_depth = min(out['normal_features'].keys())\n",
    "                normal_loss = self.normal_loss(batch, out['normal_features'][feat_depth])\n",
    "                    \n",
    "            loss_dict.add_loss(f\"normal\", normal_loss, self.hparams[\"supervision\"][\"normal_weight\"])\n",
    "        \n",
    "        # compute semantic loss\n",
    "        if self.hparams[\"with_semantic_branch\"]:\n",
    "            for feat_depth, pd_semantic_i in out['semantic_features'].items():\n",
    "                semantic_loss = self.semantic_loss(batch, pd_semantic_i)\n",
    "                if semantic_loss == 0.0: # do not take empty into log\n",
    "                    continue\n",
    "                loss_dict.add_loss(f\"semantic_{feat_depth}\", semantic_loss, self.hparams[\"supervision\"][\"semantic_weight\"])\n",
    "                \n",
    "        # compute color loss\n",
    "        if self.hparams[\"with_color_branch\"]:\n",
    "            for feat_depth, pd_color_i in out['color_features'].items():\n",
    "                color_loss = self.color_loss(batch, pd_color_i)\n",
    "                if color_loss == 0.0:\n",
    "                    continue\n",
    "                loss_dict.add_loss(f\"color_{feat_depth}\", color_loss, self.hparams[\"supervision\"][\"color_weight\"])\n",
    "\n",
    "        # compute KL divergence\n",
    "        if \"dist_features\" in out:\n",
    "            dist_features = out['dist_features']\n",
    "            kld = 0.0\n",
    "            for latent_id, (mu, logvar) in enumerate(dist_features):\n",
    "                num_voxel = mu.size(0)\n",
    "                kld_temp = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "                kld_total = kld_temp.item()\n",
    "                if self.hparams[\"normalize_kld\"]:\n",
    "                    kld_temp /= num_voxel\n",
    "\n",
    "                kld += kld_temp\n",
    "                latent_dict.add_loss(f\"mu-{latent_id}\", mu.mean())\n",
    "                latent_dict.add_loss(f\"logvar-{latent_id}\", logvar.mean())\n",
    "                latent_dict.add_loss(f\"kld-true-{latent_id}\", kld_temp.item())\n",
    "                latent_dict.add_loss(f\"kld-total-{latent_id}\", kld_total)\n",
    "\n",
    "            if self.hparams[\"enable_anneal\"]:\n",
    "                loss_dict.add_loss(\"kld\", kld, self.get_kl_weight(global_step))\n",
    "            else:\n",
    "                loss_dict.add_loss(\"kld\", kld, self.hparams[\"kl_weight\"])\n",
    "            \n",
    "        return loss_dict, metric_dict, latent_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss module\n",
    "loss_module = Loss({\n",
    "    \"tree_depth\": tree_depth,\n",
    "    \"voxel_size\": [voxel_size, voxel_size, voxel_size],\n",
    "    \"use_hash_tree\": use_hash_tree,\n",
    "    \"use_input_normal\": use_input_normal,\n",
    "    \"use_input_semantic\": use_input_semantic,\n",
    "    \"use_input_color\": use_input_color,\n",
    "    \"use_input_intensity\": use_input_intensity,\n",
    "    \"c_dim\": c_dim,\n",
    "    \"with_normal_branch\": True,\n",
    "    \"with_semantic_branch\": with_semantic_branch,\n",
    "    \"with_color_branch\": False,\n",
    "    \"supervision\": {\n",
    "        \"structure_weight\": structure_weight,\n",
    "        \"normal_weight\": normal_weight,\n",
    "        \"semantic_weight\": 0.0,\n",
    "        \"color_weight\": 0.0\n",
    "    },\n",
    "    \"normalize_kld\": normalize_kld,\n",
    "    \"enable_anneal\": enable_anneal,\n",
    "    \"kl_weight_min\": kl_weight_min,\n",
    "    \"kl_weight_max\": kl_weight_max,\n",
    "    \"anneal_star_iter\": anneal_star_iter,\n",
    "    \"anneal_end_iter\": anneal_end_iter,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100], Step [0/2], Loss: 52.5380859375\n",
      "Epoch [0/100], Step [0/1], Loss: 50.51181411743164\n",
      "Epoch [1/100], Step [0/2], Loss: 44.66753387451172\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m unet_wrapper\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(val_dataloader_):\n\u001b[1;32m     42\u001b[0m         out_dict \u001b[38;5;241m=\u001b[39m unet_wrapper(batch, out_dict)\n\u001b[1;32m     44\u001b[0m         loss_dict, metric_dict, latent_dict \u001b[38;5;241m=\u001b[39m loss_module(batch, out_dict, compute_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, global_step\u001b[38;5;241m=\u001b[39mepoch \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader_) \u001b[38;5;241m+\u001b[39m i, current_epoch\u001b[38;5;241m=\u001b[39mepoch)\n",
      "File \u001b[0;32m~/.conda/envs/xlayer/lib/python3.11/site-packages/torch/utils/data/dataloader.py:440\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/xlayer/lib/python3.11/site-packages/torch/utils/data/dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/xlayer/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1083\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1081\u001b[0m _utils\u001b[38;5;241m.\u001b[39msignal_handling\u001b[38;5;241m.\u001b[39m_set_SIGCHLD_handler()\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_pids_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1083\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/xlayer/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1116\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._reset\u001b[0;34m(self, loader, first_iter)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;66;03m# prime the prefetch loop\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prefetch_factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_workers):\n\u001b[0;32m-> 1116\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_put_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/xlayer/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1361\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_put_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1358\u001b[0m     \u001b[38;5;66;03m# not found (i.e., didn't break)\u001b[39;00m\n\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m-> 1361\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_index_queues\u001b[49m\u001b[43m[\u001b[49m\u001b[43mworker_queue_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[possibly-undefined]\u001b[39;00m\n\u001b[1;32m   1362\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_idx] \u001b[38;5;241m=\u001b[39m (worker_queue_idx,)\n\u001b[1;32m   1363\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/xlayer/lib/python3.11/multiprocessing/queues.py:94\u001b[0m, in \u001b[0;36mQueue.put\u001b[0;34m(self, obj, block, timeout)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_notempty:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_thread \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_start_thread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer\u001b[38;5;241m.\u001b[39mappend(obj)\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_notempty\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m~/.conda/envs/xlayer/lib/python3.11/multiprocessing/queues.py:177\u001b[0m, in \u001b[0;36mQueue._start_thread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_thread\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    176\u001b[0m debug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoing self._thread.start()\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 177\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m debug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m... done self._thread.start()\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_joincancelled:\n",
      "File \u001b[0;32m~/.conda/envs/xlayer/lib/python3.11/threading.py:969\u001b[0m, in \u001b[0;36mThread.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    967\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m _limbo[\u001b[38;5;28mself\u001b[39m]\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m--> 969\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_started\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/xlayer/lib/python3.11/threading.py:629\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    627\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 629\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/.conda/envs/xlayer/lib/python3.11/threading.py:327\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the training step\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    out_dict = {}\n",
    "    unet_wrapper.train()\n",
    "    for i, batch in enumerate(train_dataloader_):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out_dict = unet_wrapper(batch, out_dict)\n",
    "        \n",
    "        loss_dict, metric_dict, latent_dict = loss_module(batch, out_dict, compute_metric=True, global_step=epoch * len(train_dataloader_) + i, current_epoch=epoch)\n",
    "\n",
    "        loss = loss_dict.get_sum()\n",
    "\n",
    "        loss.backward()\n",
    "            \n",
    "        # Gradient clipping\n",
    "\n",
    "        # If detect nan values, then this step is skipped\n",
    "        has_nan_value_cnt = 0\n",
    "        for p in filter(lambda p: p.grad is not None, unet_wrapper.parameters()):\n",
    "            if torch.any(p.grad.data != p.grad.data):\n",
    "                has_nan_value_cnt += 1\n",
    "        if has_nan_value_cnt > 0:\n",
    "            exp.logger.warning(f\"{has_nan_value_cnt} parameters get nan-gradient -- this step will be skipped.\")\n",
    "            for p in filter(lambda p: p.grad is not None, unet_wrapper.parameters()):\n",
    "                p.grad.data.zero_()\n",
    "\n",
    "\n",
    "        torch.nn.utils.clip_grad_value_(unet_wrapper.parameters(), clip_value=grad_clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{epochs}], Step [{i}/{len(train_dataloader_)}], Loss: {loss.item()}\")\n",
    "            \n",
    "    # Validation\n",
    "    unet_wrapper.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_dataloader_):\n",
    "            out_dict = unet_wrapper(batch, out_dict)\n",
    "        \n",
    "            loss_dict, metric_dict, latent_dict = loss_module(batch, out_dict, compute_metric=True, global_step=epoch * len(train_dataloader_) + i, current_epoch=epoch)\n",
    "\n",
    "            loss = loss_dict.get_sum()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Epoch [{epoch}/{epochs}], Step [{i}/{len(val_dataloader_)}], Loss: {loss.item()}\")\n",
    "            \n",
    "    # Save the model\n",
    "    torch.save(unet_wrapper.state_dict(), f\"model_{epoch}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlayer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
