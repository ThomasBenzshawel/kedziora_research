{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1 torch\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import gc\n",
    "import importlib\n",
    "import inspect\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import shutil\n",
    "import traceback\n",
    "from collections import OrderedDict, defaultdict\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Mapping, Optional, Union\n",
    "\n",
    "# Third-party libraries - NumPy & Scientific\n",
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "\n",
    "# Third-party libraries - PyTorch\n",
    "import torch\n",
    "print (torch.__version__, \"torch\")\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "# from torch.utils.tensorboard.summary import hparams\n",
    "\n",
    "# Third-party libraries - Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "# Third-party libraries - ML Tools\n",
    "from omegaconf import DictConfig, ListConfig, OmegaConf\n",
    "import omegaconf.errors\n",
    "\n",
    "# Local imports\n",
    "# from ext import common\n",
    "import fvdb\n",
    "import fvdb.nn as fvnn\n",
    "from fvdb import JaggedTensor, GridBatch\n",
    "\n",
    "# Local imports\n",
    "# from modules.autoencoding.hparams import hparams_handler\n",
    "# from utils.loss_util import AverageMeter\n",
    "# from utils.loss_util import TorchLossMeter\n",
    "# from utils import exp \n",
    "\n",
    "# from modules.autoencoding.sunet import StructPredictionNet \n",
    "\n",
    "# from utils.vis_util import vis_pcs\n",
    "\n",
    "\n",
    "# from modules.diffusionmodules.schedulers.scheduling_ddim import DDIMScheduler\n",
    "# from modules.diffusionmodules.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "# from modules.diffusionmodules.schedulers.scheduling_dpmpp_2m import DPMSolverMultistepScheduler\n",
    "\n",
    "\n",
    "# from modules.diffusionmodules.ema import LitEma\n",
    "\n",
    "# Why aren't these used??????\n",
    "from modules.encoders import (SemanticEncoder, ClassEmbedder, PointNetEncoder,\n",
    "                                    StructEncoder, StructEncoder3D, StructEncoder3D_remain_h, StructEncoder3D_v2)\n",
    "\n",
    "from modules.autoencoding.sunet import StructPredictionNet\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "batch_size = 4\n",
    "tree_depth = 3 # according to 512x512x512 -> 128x128x128\n",
    "voxel_size = 0.0025\n",
    "resolution = 512\n",
    "use_fvdb_loader = True\n",
    "use_hash_tree = True # use hash tree means use early dilation (description in Sec 3.4) \n",
    "\n",
    "\n",
    "with_semantic_branch = False\n",
    "extract_mesh = \"store_true\"\n",
    "\n",
    "solver_order =\"3\"\n",
    "\n",
    "use_dpm = \"store_true\"\n",
    "\n",
    "ddim_step = 50\n",
    "\n",
    "use_ddim = \"store_true\"\n",
    "\n",
    "ema = \"store_true\"\n",
    "\n",
    "batch_len = 64\n",
    "\n",
    "toal_len = 700\n",
    "\n",
    "seed = 0\n",
    "\n",
    "world_size = 1\n",
    "\n",
    "# setup input\n",
    "use_input_normal = True\n",
    "use_input_semantic = False\n",
    "use_input_intensity = False\n",
    "\n",
    "# setup KL loss\n",
    "cut_ratio = 16 # reduce the dimension of the latent space\n",
    "kl_weight = 1.0 # activate when anneal is off\n",
    "normalize_kld = True\n",
    "enable_anneal = False\n",
    "kl_weight_min = 1e-7\n",
    "kl_weight_max = 1.0\n",
    "anneal_star_iter = 0\n",
    "anneal_end_iter = 70000 # need to adjust for different dataset\n",
    "\n",
    "\n",
    "structure_weight = 20.0\n",
    "normal_weight = 300.0\n",
    "  \n",
    "\n",
    "learning_rate = {\n",
    "  \"init\": 1.0e-4,\n",
    "  \"decay_mult\": 0.7,\n",
    "  \"decay_step\": 50000,\n",
    "  \"clip\": 1.0e-6\n",
    "}\n",
    "weight_decay = 0.0\n",
    "grad_clip = 0.5\n",
    "\n",
    "c_dim = 32\n",
    "  \n",
    "# unet parameters\n",
    "in_channels = 32\n",
    "num_blocks = tree_depth\n",
    "f_maps = 32\n",
    "neck_dense_type = \"UNCHANGED\"\n",
    "neck_bound = [64, 64, 64] # useless but indicate here\n",
    "num_res_blocks = 1\n",
    "use_residual = False\n",
    "order = \"gcr\"\n",
    "is_add_dec = False\n",
    "use_attention = False\n",
    "use_checkpoint = False\n",
    "\n",
    "\n",
    "_custom_name =  \"objaverse\"\n",
    "_objaverse_path = \"/home/benzshawelt/Kedziora/kedziora_research/layer_x_layer/data_gen/voxels/512\"\n",
    "_split_path = \"/home/benzshawelt/Kedziora/kedziora_research/layer_x_layer/data_gen/voxels/\"\n",
    "_text_emb_path = \"\"\n",
    "_null_embed_path = \"./assets/null_text_emb.pkl\"\n",
    "max_text_len= 77\n",
    "text_embed_drop_prob= 0.1\n",
    "\n",
    "train_dataset = \"ObjaverseDataset\"\n",
    "train_val_num_workers= 16\n",
    "train_kwargs = {\n",
    "  \"onet_base_path\": _objaverse_path ,\n",
    "  \"resolution\": resolution,\n",
    "  \"custom_name\": _custom_name,\n",
    "  \"split_base_path\": _split_path,\n",
    "  \"split\": \"train\",\n",
    "  \"text_emb_path\": _text_emb_path,\n",
    "  \"null_embed_path\": _null_embed_path,\n",
    "  \"max_text_len\": max_text_len,\n",
    "  \"text_embed_drop_prob\": text_embed_drop_prob, # ! classifier-free training\n",
    "  \"random_seed\": 0\n",
    "}\n",
    "\n",
    "val_dataset = \"ObjaverseDataset\"\n",
    "val_kwargs = {\n",
    "  \"onet_base_path\": _objaverse_path ,\n",
    "  \"resolution\": resolution,\n",
    "  \"custom_name\": _custom_name,\n",
    "  \"split_base_path\": _split_path,\n",
    "  \"split\": \"test\",\n",
    "  \"text_emb_path\": _text_emb_path,\n",
    "  \"null_embed_path\": _null_embed_path,\n",
    "  \"max_text_len\": max_text_len,\n",
    "  \"random_seed\": \"fixed\"\n",
    "}\n",
    "\n",
    "test_dataset = \"ObjaverseDataset\"\n",
    "test_num_workers =8\n",
    "test_kwargs = {\n",
    "  \"onet_base_path\": _objaverse_path ,\n",
    "  \"resolution\": resolution,\n",
    "  \"custom_name\": _custom_name,\n",
    "  \"split_base_path\": _split_path,\n",
    "  \"split\": \"test\",\n",
    "  \"text_emb_path\": _text_emb_path,\n",
    "  \"null_embed_path\": _null_embed_path,\n",
    "  \"max_text_len\": max_text_len,\n",
    "  \"random_seed\": \"fixed\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the paths of all of the voxelized shapes, the shapes are stored in .pkl files within 2 folder depths of the base path\n",
    "def get_all_paths(base_path):\n",
    "  import os\n",
    "  # list all of the files in the base path\n",
    "  # print(\"base path\", base_path)\n",
    "  # print(os.listdir(base_path))\n",
    "  all_paths = []\n",
    "  for root, dirs, files in os.walk(base_path):\n",
    "    for file in files:\n",
    "      if file.endswith(\".pkl\"):\n",
    "        all_paths.append(os.path.join(root, file.split(\".\")[0]))\n",
    "        \n",
    "  return all_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, train_split_ratio, test_split_ratio, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    indices = np.arange(len(dataset))\n",
    "    np.random.shuffle(indices)\n",
    "    train_split = int(len(dataset) * train_split_ratio)\n",
    "    # always take .10 of the dataset for test (removed from val)\n",
    "    test_split = int(len(dataset) * test_split_ratio) + train_split\n",
    "\n",
    "    #return the train, val, and test datasets\n",
    "    return torch.utils.data.Subset(dataset, indices[:train_split]), torch.utils.data.Subset(dataset, indices[train_split:test_split]), torch.utils.data.Subset(dataset, indices[test_split:])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_split_paths(base_path, split_ratio, seed=0):\n",
    "    all_paths = get_all_paths(base_path)\n",
    "    train_paths, val_paths, test_split= split_dataset(all_paths, split_ratio, 0.1, seed)\n",
    "\n",
    "    # Save the split paths as a lst file\n",
    "    split_base_path = Path(base_path).parent\n",
    "    train_split_path = split_base_path / \"train.lst\"\n",
    "    val_split_path = split_base_path / \"val.lst\"\n",
    "    test_split_path = split_base_path / \"test.lst\"\n",
    "    \n",
    "\n",
    "    # delete the files if they already exist\n",
    "    if train_split_path.exists():\n",
    "        train_split_path.unlink()\n",
    "    if val_split_path.exists():\n",
    "        val_split_path.unlink()\n",
    "    if test_split_path.exists():\n",
    "        test_split_path.unlink()\n",
    "        \n",
    "    # write the paths to the files\n",
    "\n",
    "    with open(test_split_path, \"w\") as f:\n",
    "        for path in test_split:\n",
    "            f.write(f\"{path}\\n\")\n",
    "    with open(train_split_path, \"w\") as f:\n",
    "        for path in train_paths:\n",
    "            f.write(f\"{path}\\n\")\n",
    "    with open(val_split_path, \"w\") as f:\n",
    "        for path in val_paths:\n",
    "            f.write(f\"{path}\\n\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_split_paths(_objaverse_path , 0.8, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_lr_wrapper(it, lr_config, batch_size, accumulate_grad_batches=1):\n",
    "    return max(\n",
    "        lr_config['decay_mult'] ** (int(it * batch_size * accumulate_grad_batches / lr_config['decay_step'])),\n",
    "        lr_config['clip'] / lr_config['init'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = StructPredictionNet(\n",
    "  in_channels=in_channels,\n",
    "  num_blocks=num_blocks,\n",
    "  f_maps=f_maps,\n",
    "  neck_dense_type=neck_dense_type,\n",
    "  neck_bound=neck_bound,\n",
    "  num_res_blocks=num_res_blocks,\n",
    "  use_residual=use_residual,\n",
    "  order=order,\n",
    "  is_add_dec=is_add_dec,\n",
    "  use_attention=use_attention,\n",
    "  use_checkpoint=use_checkpoint,\n",
    "  c_dim=c_dim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<DatasetSpec.SHAPE_NAME: 100>, <DatasetSpec.INPUT_PC: 200>, <DatasetSpec.GT_DENSE_PC: 400>, <DatasetSpec.GT_GEOMETRY: 800>, <DatasetSpec.TARGET_NORMAL: 300>, <DatasetSpec.GT_DENSE_NORMAL: 500>]\n"
     ]
    }
   ],
   "source": [
    "from utils.Dataspec import DatasetSpec\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate[\"init\"],\n",
    "                                    weight_decay=weight_decay, amsgrad=True)\n",
    "\n",
    "scheduler = LambdaLR(optimizer,\n",
    "                lr_lambda=functools.partial(\n",
    "                    lambda_lr_wrapper, lr_config=learning_rate, batch_size=batch_size))\n",
    "\n",
    "# exp.global_var_manager.register_variable('skip_backward', False)\n",
    "\n",
    "def list_collate(batch):\n",
    "    \"\"\"\n",
    "    This just do not stack batch dimension.\n",
    "    \"\"\"\n",
    "    \n",
    "    elem = None\n",
    "    for e in batch:\n",
    "        if e is not None:\n",
    "            elem = e\n",
    "            break\n",
    "    elem_type = type(elem)\n",
    "    if isinstance(elem, torch.Tensor):\n",
    "        return batch\n",
    "    elif elem_type.__module__ == 'numpy' and elem_type.__name__ != 'str_' \\\n",
    "            and elem_type.__name__ != 'string_':\n",
    "        if elem_type.__name__ == 'ndarray' or elem_type.__name__ == 'memmap':\n",
    "            return list_collate([torch.as_tensor(b) if b is not None else None for b in batch])\n",
    "        elif elem.shape == ():  # scalars\n",
    "            return torch.as_tensor(batch)\n",
    "    elif isinstance(elem, float):\n",
    "        return torch.tensor(batch, dtype=torch.float64)\n",
    "    elif isinstance(elem, int):\n",
    "        return torch.tensor(batch)\n",
    "    elif isinstance(elem, str):\n",
    "        return batch\n",
    "    elif isinstance(elem, DictConfig) or isinstance(elem, ListConfig):\n",
    "        return batch\n",
    "    elif isinstance(elem, collections.abc.Mapping):\n",
    "        return {key: list_collate([d[key] for d in batch]) for key in elem}\n",
    "    elif isinstance(elem, collections.abc.Sequence):\n",
    "        # check to make sure that the elements in batch have consistent size\n",
    "        it = iter(batch)\n",
    "        elem_size = len(next(it))\n",
    "        if not all(len(elem) == elem_size for elem in it):\n",
    "            raise RuntimeError('each element in list of batch should be of equal size')\n",
    "        transposed = zip(*batch)\n",
    "        return [list_collate(samples) for samples in transposed]\n",
    "    elif isinstance(elem, GridBatch):\n",
    "        return fvdb.cat(batch)\n",
    "    \n",
    "    return batch\n",
    "\n",
    "def get_dataset_spec():\n",
    "    DS = DatasetSpec\n",
    "    all_specs = [DS.SHAPE_NAME, DS.INPUT_PC,\n",
    "                    DS.GT_DENSE_PC, DS.GT_GEOMETRY]\n",
    "    if use_input_normal:\n",
    "        all_specs.append(DS.TARGET_NORMAL)\n",
    "        all_specs.append(DS.GT_DENSE_NORMAL)\n",
    "    if use_input_semantic or with_semantic_branch:\n",
    "        all_specs.append(DS.GT_SEMANTIC)\n",
    "    if use_input_intensity:\n",
    "        all_specs.append(DS.INPUT_INTENSITY)\n",
    "    return all_specs\n",
    "\n",
    "\n",
    "\n",
    "def train_dataloader():\n",
    "    from data.objaverse import ObjaverseDataset\n",
    "    train_set =  ObjaverseDataset(onet_base_path=train_kwargs[\"onet_base_path\"], \n",
    "                                  spec=get_dataset_spec(), \n",
    "                                  split=train_kwargs[\"split\"], \n",
    "                                  resolution=train_kwargs[\"resolution\"], \n",
    "                                  image_base_path=None, \n",
    "                                  random_seed=0, \n",
    "                                  hparams=None, \n",
    "                                  skip_on_error=False, \n",
    "                                  custom_name=\"objaverse\", \n",
    "                                  text_emb_path=\"../data/objaverse/objaverse/text_emb\", \n",
    "                                  null_embed_path=\"./assets/null_text_emb.pkl\", \n",
    "                                  text_embed_drop_prob=0.0, \n",
    "                                  max_text_len=77, \n",
    "                                  duplicate_num=1, \n",
    "                                  split_base_path=_split_path,\n",
    "                                  )\n",
    "        \n",
    "    return DataLoader(train_set, batch_size=batch_size // world_size, shuffle=True,\n",
    "                        num_workers=train_val_num_workers, collate_fn=list_collate)\n",
    "\n",
    "\n",
    "print(get_dataset_spec())\n",
    "\n",
    "def val_dataloader():\n",
    "    from data.objaverse import ObjaverseDataset\n",
    "    val_set = ObjaverseDataset(onet_base_path=val_kwargs[\"onet_base_path\"],\n",
    "                                spec=get_dataset_spec(), \n",
    "                                split=val_kwargs[\"split\"], \n",
    "                                resolution=val_kwargs[\"resolution\"], \n",
    "                                image_base_path=None, \n",
    "                                random_seed=0, \n",
    "                                hparams=None, \n",
    "                                skip_on_error=False, \n",
    "                                custom_name=\"objaverse\", \n",
    "                                text_emb_path=\"../data/objaverse/objaverse/text_emb\", \n",
    "                                null_embed_path=\"./assets/null_text_emb.pkl\", \n",
    "                                text_embed_drop_prob=0.0, \n",
    "                                max_text_len=77, \n",
    "                                duplicate_num=1, \n",
    "                                split_base_path=_split_path,\n",
    "                                )\n",
    "\n",
    "\n",
    "    return DataLoader(val_set, batch_size=batch_size // world_size, shuffle=False,\n",
    "                        num_workers=train_val_num_workers, collate_fn=list_collate)\n",
    "\n",
    "def test_dataloader(resolution=resolution, test_set_shuffle=False):\n",
    "    from data.objaverse import ObjaverseDataset\n",
    "    resolution = resolution # ! use for testing when training on X^3 but testing on Y^3\n",
    "\n",
    "    test_set =  ObjaverseDataset(onet_base_path=test_kwargs[\"onet_base_path\"],\n",
    "                                spec=get_dataset_spec(), \n",
    "                                split=test_kwargs[\"split\"], \n",
    "                                resolution=resolution, \n",
    "                                image_base_path=None, \n",
    "                                random_seed=0, \n",
    "                                hparams=None, \n",
    "                                skip_on_error=False, \n",
    "                                custom_name=\"objaverse\", \n",
    "                                text_emb_path=\"../data/objaverse/objaverse/text_emb\", \n",
    "                                null_embed_path=\"./assets/null_text_emb.pkl\", \n",
    "                                text_embed_drop_prob=0.0, \n",
    "                                max_text_len=77, \n",
    "                                duplicate_num=1, \n",
    "                                split_base_path=_split_path,\n",
    "                                )\n",
    "    \n",
    "    if test_set_shuffle:\n",
    "        torch.manual_seed(0)\n",
    "    return DataLoader(test_set, batch_size=1, shuffle=test_set_shuffle, \n",
    "                        num_workers=0, collate_fn=list_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benzshawelt/.conda/envs/xlayer/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "train_dataloader_ = train_dataloader()\n",
    "val_dataloader_ = val_dataloader()\n",
    "test_dataloader_ = test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.exp as exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benzshawelt/Kedziora/kedziora_research/layer_x_layer/data/objaverse.py:199: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  input_data = torch.load(os.path.join(self.onet_base_path, category, model) + \".pkl\")\n",
      "/home/benzshawelt/Kedziora/kedziora_research/layer_x_layer/data/objaverse.py:199: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  input_data = torch.load(os.path.join(self.onet_base_path, category, model) + \".pkl\")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/benzshawelt/.conda/envs/xlayer/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 309, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/benzshawelt/.conda/envs/xlayer/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_1131378/1434144813.py\", line 41, in list_collate\n    return {key: list_collate([d[key] for d in batch]) for key in elem}\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_1131378/1434144813.py\", line 41, in <dictcomp>\n    return {key: list_collate([d[key] for d in batch]) for key in elem}\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_1131378/1434144813.py\", line 51, in list_collate\n    return fvdb.cat(batch)\n           ^^^^^^^^\nAttributeError: module 'fvdb' has no attribute 'cat'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      4\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Forward pass\u001b[39;49;00m\n",
      "File \u001b[0;32m~/.conda/envs/xlayer/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/xlayer/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1344\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1343\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/xlayer/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1370\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1370\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.conda/envs/xlayer/lib/python3.11/site-packages/torch/_utils.py:706\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 706\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mAttributeError\u001b[0m: Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/benzshawelt/.conda/envs/xlayer/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 309, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/benzshawelt/.conda/envs/xlayer/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_1131378/1434144813.py\", line 41, in list_collate\n    return {key: list_collate([d[key] for d in batch]) for key in elem}\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_1131378/1434144813.py\", line 41, in <dictcomp>\n    return {key: list_collate([d[key] for d in batch]) for key in elem}\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_1131378/1434144813.py\", line 51, in list_collate\n    return fvdb.cat(batch)\n           ^^^^^^^^\nAttributeError: module 'fvdb' has no attribute 'cat'\n"
     ]
    }
   ],
   "source": [
    "# Define the training step\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for i, batch in enumerate(train_dataloader_):\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        print(batch)\n",
    "        loss = model(batch, None)\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "\n",
    "        # If detect nan values, then this step is skipped\n",
    "        has_nan_value_cnt = 0\n",
    "        for p in filter(lambda p: p.grad is not None, model.parameters()):\n",
    "            if torch.any(p.grad.data != p.grad.data):\n",
    "                has_nan_value_cnt += 1\n",
    "        if has_nan_value_cnt > 0:\n",
    "            exp.logger.warning(f\"{has_nan_value_cnt} parameters get nan-gradient -- this step will be skipped.\")\n",
    "            for p in filter(lambda p: p.grad is not None, model.parameters()):\n",
    "                p.grad.data.zero_()\n",
    "\n",
    "\n",
    "        torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=grad_clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{epochs}], Step [{i}/{len(train_dataloader)}], Loss: {loss.item()}\")\n",
    "            \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_dataloader):\n",
    "            loss = model(batch)\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch [{epoch}/{epochs}], Step [{i}/{len(val_dataloader)}], Loss: {loss.item()}\")\n",
    "            \n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), f\"model_{epoch}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlayer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
