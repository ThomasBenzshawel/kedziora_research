{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import gc\n",
    "import importlib\n",
    "import inspect\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import shutil\n",
    "import traceback\n",
    "from collections import OrderedDict, defaultdict\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Mapping, Optional, Union\n",
    "import collections\n",
    "\n",
    "# Third-party libraries - NumPy & Scientific\n",
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "\n",
    "# Third-party libraries - PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.tensorboard.summary import hparams\n",
    "\n",
    "# Third-party libraries - Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "from modules.diffusionmodules.ema import LitEma\n",
    "import tqdm\n",
    "\n",
    "# Third-party libraries - ML Tools\n",
    "# import wandb\n",
    "# from loguru import logger\n",
    "from omegaconf import DictConfig, ListConfig, OmegaConf\n",
    "import omegaconf.errors\n",
    "\n",
    "# Local imports\n",
    "# from ext import common\n",
    "import fvdb\n",
    "import fvdb.nn as fvnn\n",
    "from fvdb.nn import VDBTensor\n",
    "from fvdb import JaggedTensor, GridBatch\n",
    "\n",
    "# Local imports\n",
    "from modules.autoencoding.hparams import hparams_handler\n",
    "from utils.loss_util import AverageMeter\n",
    "from utils.loss_util import TorchLossMeter\n",
    "from utils import exp \n",
    "\n",
    "from modules.autoencoding.sunet import StructPredictionNet \n",
    "\n",
    "from modules.diffusionmodules.schedulers.scheduling_ddim import DDIMScheduler\n",
    "from modules.diffusionmodules.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "from modules.diffusionmodules.schedulers.scheduling_dpmpp_2m import DPMSolverMultistepScheduler\n",
    "\n",
    "from utils.Dataspec import DatasetSpec as DS\n",
    "\n",
    "from modules.diffusionmodules.diffusion_cross_attn import UNetModel as Diffusion_Cross_Attn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "\n",
    "from utils.vis_util import vis_pcs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_pos_embed_world = True\n",
    "\n",
    "# voxel_size = 0.001953125\n",
    "resolution = 512\n",
    "\n",
    "# # data info\n",
    "# duplicate_num = 10 # repeat the dataset to save the time of building dataloader\n",
    "# batch_size = 64\n",
    "# accumulate_grad_batches = 4\n",
    "# batch_size_val = 4\n",
    "# train_val_num_workers = 16\n",
    "\n",
    "# # diffusion - inference params\n",
    "# use_ddim = True\n",
    "# num_inference_steps = 100\n",
    "\n",
    "# # diffusion - scheduler-related adjust params\n",
    "# num_train_timesteps = 1000\n",
    "# beta_start = 0.0001\n",
    "# beta_end = 0.02\n",
    "# beta_schedule = \"linear\"\n",
    "# prediction_type = \"v_prediction\"\n",
    "\n",
    "#   # scheduler:\n",
    "#   #   num_train_timesteps: ${num_train_timesteps}\n",
    "#   #   beta_start: ${beta_start}\n",
    "#   #   beta_end: ${beta_end}\n",
    "#   #   beta_schedule: ${beta_schedule} # cosine\n",
    "#   #   variance_type: \"fixed_small\"\n",
    "#   #   clip_sample: False\n",
    "#   #   prediction_type: ${prediction_type} # epsilon\n",
    "\n",
    "# # diffusion - scale by std\n",
    "# scale_by_std = True\n",
    "# scale_factor = 1.0\n",
    "\n",
    "# ema = True\n",
    "# ema_decay = 0.9999\n",
    "\n",
    "\n",
    "# mse_weight = 1.0\n",
    "\n",
    "\n",
    "# weight_decay = 0.0\n",
    "# grad_clip = 0.5\n",
    "\n",
    "# dims_diffuser = 3 # 3D conv\n",
    "# image_size = 128 # use during testing\n",
    "# model_channels = 64\n",
    "# use_middle_attention: True\n",
    "# channel_mult = [1, 2, 2, 4] # 128 -> 16\n",
    "# attention_resolutions = [4, 8] # 32 | 16\n",
    "# num_res_blocks = 2\n",
    "# num_heads = 8\n",
    "# variance_type = \"fixed_small\"\n",
    "# clip_sample = False\n",
    "# context_dim = 1024\n",
    "# use_text_cond = True\n",
    "# use_normal_concat_cond= True\n",
    "# num_classes = 3 # normal dim 3???\n",
    "\n",
    "# use_pos_embed_world= False\n",
    "# use_pos_embed = True\n",
    "\n",
    "\n",
    "learning_rate = {\n",
    "  \"init\": 5.0e-5,\n",
    "  \"decay_mult\": 1.0,\n",
    "  \"decay_step\": 2000000000, # use a constant learning rate\n",
    "  \"clip\": 1.0e-6\n",
    "}\n",
    "\n",
    "# optimizer: \"Adam\"\n",
    "# learning_rate:\n",
    "#   init: 5.0e-5\n",
    "#   decay_mult: 1.0\n",
    "#   decay_step: 2000000000 # use a constant learning rate\n",
    "#   clip: 1.0e-6\n",
    "# weight_decay: 0.0\n",
    "# grad_clip: 0.5\n",
    "\n",
    "\n",
    "# Main hyperparameters for DiffusionModel\n",
    "hparams = {\n",
    "    # Positional embedding options\n",
    "    \"use_pos_embed_world\": False,\n",
    "    \"use_pos_embed\": True,\n",
    "    \"use_pos_embed_high\": False,\n",
    "    \"use_pos_embed_world_high\": False,\n",
    "    \n",
    "    # Diffusion process parameters\n",
    "    \"scale_by_std\": True,\n",
    "    \"scale_factor\": 1.0,\n",
    "    \"ema\": True,\n",
    "    \"ema_decay\": 0.9999,\n",
    "    \"use_ddim\": True,\n",
    "    \"num_inference_steps\": 100,\n",
    "    \n",
    "    # Conditioning options\n",
    "    \"use_text_cond\": True,\n",
    "    \"use_normal_concat_cond\": True,\n",
    "    \"use_semantic_cond\": False,\n",
    "    \"use_mask_cond\": False,\n",
    "    \"use_point_cond\": False,\n",
    "    \"use_class_cond\": False,\n",
    "    \"use_micro_cond\": False,\n",
    "    \"use_single_scan_concat_cond\": False,\n",
    "    \"use_classifier_free\": True,\n",
    "    \"classifier_free_prob\": 0.1,\n",
    "    \n",
    "    # For inference/evaluation\n",
    "    \"diffuser_image_size\": 128,  # image_size from your list\n",
    "    \n",
    "    # Other parameters\n",
    "    \"num_classes\": 3,\n",
    "    \"mse_weight\": 1.0,\n",
    "    \"conditioning_key\": \"c_crossattn\",  # Based on code, this seems most likely given your settings\n",
    "}\n",
    "\n",
    "# Noise scheduler parameters\n",
    "noise_scheduler_params = {\n",
    "    \"num_train_timesteps\": 1000,\n",
    "    \"beta_start\": 0.0001,\n",
    "    \"beta_end\": 0.02,\n",
    "    \"beta_schedule\": \"linear\",\n",
    "    \"prediction_type\": \"v_prediction\",\n",
    "    \"variance_type\": \"fixed_small\",\n",
    "    \"clip_sample\": False\n",
    "}\n",
    "\n",
    "# UNet/Diffuser hyperparameters (passed via diffuser_kwargs)\n",
    "diffuser_kwargs = {\n",
    "    \"dims\": 3,  # 3D conv as specified by dims_diffuser\n",
    "    \"model_channels\": 64,\n",
    "    \"use_middle_attention\": True,\n",
    "    \"channel_mult\": [1, 2, 2, 4],  # 128 -> 16\n",
    "    \"attention_resolutions\": [4, 8],  # 32 | 16\n",
    "    \"num_res_blocks\": 2,\n",
    "    \"num_heads\": 8,\n",
    "    \"context_dim\": 1024\n",
    "}\n",
    "\n",
    "# VAE parameters (retrieved from the vae module)\n",
    "# vae_hparams = {\n",
    "#     \"voxel_size\": 0.001953125,\n",
    "#     \"tree_depth\": None,  # You'll need to provide this\n",
    "#     \"num_blocks\": None,  # You'll need to provide this\n",
    "#     \"f_maps\": None,      # You'll need to provide this\n",
    "#     \"cut_ratio\": None    # You'll need to provide this\n",
    "# }\n",
    "\n",
    "# Training parameters\n",
    "training_params = {\n",
    "    \"batch_size\": 64,\n",
    "    \"accumulate_grad_batches\": 4,\n",
    "    \"batch_size_val\": 4,\n",
    "    \"train_val_num_workers\": 16,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"grad_clip\": 0.5,\n",
    "    \"learning_rate\": {\n",
    "        \"init\": 5.0e-5,\n",
    "        \"decay_mult\": 1.0,\n",
    "        \"decay_step\": 2000000000,  # Constant learning rate\n",
    "        \"clip\": 1.0e-6\n",
    "    }\n",
    "}\n",
    "\n",
    "# Data parameters\n",
    "data_params = {\n",
    "    \"duplicate_num\": 10,  # Repeat dataset to save time building dataloader\n",
    "    \"resolution\": 512\n",
    "}\n",
    "\n",
    "\n",
    "_custom_name =  \"objaverse\"\n",
    "_objaverse_path = \"/home/benzshawelt/Kedziora/kedziora_research/layer_x_layer/data_gen/voxels/512\"\n",
    "_split_path = \"/home/benzshawelt/Kedziora/kedziora_research/layer_x_layer/data_gen/voxels/\"\n",
    "_text_emb_path = \"\"\n",
    "_null_embed_path = \"./assets/null_text_emb.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_sample_pcs(self, ijk: fvdb.JaggedTensor, batch_size=1, M=3, use_center=False):\n",
    "    # M: sample per point\n",
    "    # !: be careful about the batch_size\n",
    "    output_ijk = []\n",
    "    for idx in range(batch_size):\n",
    "        current_ijk = ijk[idx].jdata.float() # N, 3\n",
    "        if use_center:\n",
    "            output_ijk.append(current_ijk)\n",
    "        else:            \n",
    "            N = current_ijk.shape[0]\n",
    "            # create offsets of size M*N x 3 with values in range [-0.5, 0.5]\n",
    "            offsets = torch.FloatTensor(N * M, 3).uniform_(-0.5, 0.5).to(current_ijk.device)\n",
    "            # duplicate your original point cloud M times\n",
    "            expanded_point_cloud = current_ijk.repeat(M, 1)\n",
    "            # add offsets to duplicated points\n",
    "            expanded_point_cloud += offsets\n",
    "            output_ijk.append(expanded_point_cloud)\n",
    "\n",
    "            \n",
    "    return fvdb.JaggedTensor(output_ijk)\n",
    "def decode_to_points(self, latents):\n",
    "    res = self.vae.unet.FeaturesSet()\n",
    "    res, output_x = self.vae.unet.decode(res, latents, is_testing=True)\n",
    "    grid_tree = res.structure_grid\n",
    "    fine_list = []\n",
    "    coarse_list = []\n",
    "    for batch_idx in range(output_x.grid.grid_count):\n",
    "        # fineest level\n",
    "        pd_grid_0 = grid_tree[0]\n",
    "        pd_xyz_0 = pd_grid_0.grid_to_world(self.get_random_sample_pcs(pd_grid_0.ijk[batch_idx], M=8))\n",
    "        fine_list.append(pd_xyz_0.jdata.cpu().numpy())\n",
    "        # coarsest level\n",
    "        pd_grid_1 = grid_tree[len(grid_tree.keys()) - 1]\n",
    "        pd_xyz_1 = pd_grid_1.grid_to_world(self.get_random_sample_pcs(pd_grid_1.ijk[batch_idx], M=3))\n",
    "        coarse_list.append(pd_xyz_1.jdata.cpu().numpy())\n",
    "    # plot the fine and coarse level\n",
    "    viz_fine = vis_pcs(fine_list)\n",
    "    viz_coarse = vis_pcs(coarse_list)\n",
    "    decode_results = np.concatenate([viz_fine, viz_coarse], axis=0)\n",
    "    return decode_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_collate(batch):\n",
    "    \"\"\"\n",
    "    This just do not stack batch dimension.\n",
    "    \"\"\"\n",
    "    elem = None\n",
    "    for e in batch:\n",
    "        if e is not None:\n",
    "            elem = e\n",
    "            break\n",
    "    elem_type = type(elem)\n",
    "    if isinstance(elem, torch.Tensor):\n",
    "        return batch\n",
    "    elif elem_type.__module__ == 'numpy' and elem_type.__name__ != 'str_' \\\n",
    "            and elem_type.__name__ != 'string_':\n",
    "        if elem_type.__name__ == 'ndarray' or elem_type.__name__ == 'memmap':\n",
    "            return list_collate([torch.as_tensor(b) if b is not None else None for b in batch])\n",
    "        elif elem.shape == ():  # scalars\n",
    "            return torch.as_tensor(batch)\n",
    "    elif isinstance(elem, float):\n",
    "        return torch.tensor(batch, dtype=torch.float64)\n",
    "    elif isinstance(elem, int):\n",
    "        return torch.tensor(batch)\n",
    "    elif isinstance(elem, str):\n",
    "        return batch\n",
    "    elif isinstance(elem, DictConfig) or isinstance(elem, ListConfig):\n",
    "        return batch\n",
    "    elif isinstance(elem, collections.abc.Mapping):\n",
    "        return {key: list_collate([d[key] for d in batch]) for key in elem}\n",
    "    elif isinstance(elem, collections.abc.Sequence):\n",
    "        # check to make sure that the elements in batch have consistent size\n",
    "        it = iter(batch)\n",
    "        elem_size = len(next(it))\n",
    "        if not all(len(elem) == elem_size for elem in it):\n",
    "            raise RuntimeError('each element in list of batch should be of equal size')\n",
    "        transposed = zip(*batch)\n",
    "        return [list_collate(samples) for samples in transposed]\n",
    "    elif isinstance(elem, GridBatch):\n",
    "        return fvdb.cat(batch)\n",
    "    \n",
    "    # elif isinstance(elem, pathlib.Path):\n",
    "    #     return batch\n",
    "    # elif elem is None:\n",
    "    #     return batch\n",
    "\n",
    "    # raise NotImplementedError\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, include_input=True, input_dims=3, max_freq_log2=10, num_freqs=10, log_sampling=True, periodic_fns=[torch.sin, torch.cos]):\n",
    "        super().__init__()\n",
    "        embed_fns = []\n",
    "        d = input_dims\n",
    "        out_dim = 0\n",
    "        if include_input:\n",
    "            out_dim += d\n",
    "            \n",
    "        max_freq = max_freq_log2\n",
    "        N_freqs = num_freqs\n",
    "        \n",
    "        if log_sampling:\n",
    "            freq_bands = 2.**torch.linspace(0., max_freq, steps=N_freqs)\n",
    "        else:\n",
    "            freq_bands = torch.linspace(2.**0., 2.**max_freq, steps=N_freqs)\n",
    "        \n",
    "        for freq in freq_bands:\n",
    "            for _ in periodic_fns:\n",
    "                out_dim += d\n",
    "        \n",
    "        self.include_input = include_input\n",
    "        self.freq_bands = freq_bands\n",
    "        self.periodic_fns = periodic_fns\n",
    "        \n",
    "        self.embed_fns = embed_fns\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        output_list = [inputs]\n",
    "        for fn in self.embed_fns:\n",
    "            output_list.append(fn(inputs))\n",
    "            \n",
    "        for freq in self.freq_bands:\n",
    "            for p_fn in self.periodic_fns:\n",
    "                output_list.append(p_fn(inputs * freq))\n",
    "        \n",
    "        return torch.cat(output_list, -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedder(multires, i=0, input_dims=3):\n",
    "    if i == -1:\n",
    "        return nn.Identity(), 3\n",
    "    \n",
    "    embedder_obj = Embedder(max_freq_log2=multires-1, num_freqs=multires, input_dims=input_dims)\n",
    "    return embedder_obj, embedder_obj.out_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionModel(nn.Module):\n",
    "    def __init__(self, hparams, num_classes,use_spatial_transformer,\n",
    "    context_dim, diffuser_kwargs, noise_scheduler_params, trained_autoencoder=None):\n",
    "        super().__init__()\n",
    "        self.hparams = hparams\n",
    "        self.noise_scheduler_hparams = noise_scheduler_params\n",
    "        self.hparams.get(\"ema\", False)\n",
    "        self.hparams.get(\"use_ddim\", False)\n",
    "        self.hparams.get(\"scale_by_std\", True)\n",
    "        self.hparams.get('scale_factor', 1.0)\n",
    "        self.hparams.get('num_inference_steps', 1000)\n",
    "\n",
    "        self.hparams.use_pos_embed = False\n",
    "        self.hparams.use_pos_embed_high = False\n",
    "        self.hparams.get('use_pos_embed_world', False)\n",
    "        self.hparams.use_pos_embed_world_high = False\n",
    "\n",
    "        self.vae = trained_autoencoder\n",
    "        self.vae.requires_grad_(False)\n",
    "\n",
    "\n",
    "        unet_num_blocks = self.vae.hparams[\"num_blocks\"]\n",
    "        num_input_channels = self.vae.hparams[\"f_maps\"] * 2 ** (unet_num_blocks - 1) # Fix by using VAE hparams\n",
    "        num_input_channels = int(num_input_channels / self.vae.hparams[\"cut_ratio\"])\n",
    "\n",
    "        out_channels = num_input_channels\n",
    "        num_classes = None\n",
    "        use_spatial_transformer = False\n",
    "        context_dim=None\n",
    "\n",
    "        if self.hparams.get(\"use_pos_embed\"):\n",
    "            num_input_channels += 3\n",
    "        elif self.hparams.get(\"use_pos_embed_high\"):\n",
    "            embed_fn, input_ch = get_embedder(5)\n",
    "            self.pos_embedder = embed_fn\n",
    "            num_input_channels += input_ch\n",
    "        elif self.hparams.get(\"use_pos_embed_world\"):\n",
    "            num_input_channels += 3\n",
    "        elif self.hparams.get(\"use_pos_embed_world_high\"):\n",
    "            embed_fn, input_ch = get_embedder(5)\n",
    "            self.pos_embedder = embed_fn\n",
    "            num_input_channels += input_ch\n",
    "\n",
    "        # Define the UNet either cross attn or sparse\n",
    "        self.unet = Diffusion_Cross_Attn(num_input_channels=num_input_channels, \n",
    "                                                        out_channels=out_channels, \n",
    "                                                        num_classes=num_classes,\n",
    "                                                        use_spatial_transformer=use_spatial_transformer,\n",
    "                                                        context_dim=context_dim,\n",
    "                                                        **diffuser_kwargs)\n",
    "\n",
    "\n",
    "        # get the schedulers\n",
    "        self.noise_scheduler = DDPMScheduler(noise_scheduler_params)\n",
    "        self.ddim_scheduler = DDIMScheduler(noise_scheduler_params)\n",
    "\n",
    "\n",
    "        self.hparams[\"classifier_free_prob\"] = 0.1 # prob to drop the label\n",
    "            \n",
    "        # finetune config\n",
    "        self.hparams.get('pretrained_model_name_or_path', None)\n",
    "        self.hparams.get('ignore_mismatched_size', False)\n",
    "\n",
    "        if self.hparams[\"ema\"]:\n",
    "            self.unet_ema = LitEma(self.unet, decay=self.hparams[\"ema_decay\"])\n",
    "            \n",
    "        # scale by std\n",
    "        if not self.hparams[\"scale_by_std\"]:\n",
    "            self.scale_factor = self.hparams[\"scale_factor\"]\n",
    "            assert self.scale_factor == 1., 'when not using scale_by_std, scale_factor should be 1.'\n",
    "        else:\n",
    "            self.register_buffer('scale_factor', torch.tensor(self.hparams.scale_factor).float())\n",
    "\n",
    "    # @contextmanager\n",
    "    def ema_scope(self):\n",
    "        if self.hparams[\"ema\"]:\n",
    "            self.unet_ema.store(self.unet.parameters())\n",
    "            self.unet_ema.copy_to(self.unet)\n",
    "        try:\n",
    "            yield None\n",
    "        finally:\n",
    "            if self.hparams[\"ema\"]:\n",
    "                self.unet_ema.restore(self.unet.parameters())\n",
    "                \n",
    "    def get_pos_embed(self, h):\n",
    "        return h[:, :3]\n",
    "    \n",
    "    def get_pos_embed_high(self, h):\n",
    "        xyz = h[:, :3] # N, 3\n",
    "        xyz = self.pos_embedder(xyz) # N, C\n",
    "        return xyz\n",
    "    \n",
    "    def conduct_classifier_free(self, cond, batch_size, device, is_testing=False):\n",
    "        if isinstance(cond, VDBTensor):\n",
    "            cond = cond.feature\n",
    "        assert isinstance(cond, fvdb.JaggedTensor), \"cond should be JaggedTensor\"\n",
    "\n",
    "        mask = torch.rand(batch_size, device=device) < self.hparams[\"classifier_free_prob\"] \n",
    "        new_cond = []\n",
    "        for idx in range(batch_size):\n",
    "            if mask[idx] or is_testing:\n",
    "                # during testing, use this function to zero the condition\n",
    "                new_cond.append(torch.zeros_like(cond[idx].jdata))\n",
    "            else:\n",
    "                new_cond.append(cond[idx].jdata)\n",
    "        new_cond = fvdb.JaggedTensor(new_cond)\n",
    "        return new_cond\n",
    "    \n",
    "    @exp.mem_profile(every=1)\n",
    "    def forward(self, batch, out: dict):\n",
    "        # first get latent from vae, the latent is the input to the diffusion model\n",
    "        # A latent is the encoded feature from the input\n",
    "        with torch.no_grad():\n",
    "            latents = self.vae._encode(batch, use_mode=False)\n",
    "\n",
    "        # To Do: scale the latent\n",
    "        if self.hparams[\"scale_by_std\"]:\n",
    "            latents = latents * self.scale_factor\n",
    "\n",
    "        # then get the noise\n",
    "        latent_data = latents.jdata\n",
    "        noise = torch.randn_like(latent_data) # N, C\n",
    "\n",
    "        # bsz is the batch size ???? TODO\n",
    "        bsz = latents.grid.grid_count\n",
    "        \n",
    "        # Sample a random timestep for each latent\n",
    "        # A timestep is a random point in the training schedule\n",
    "\n",
    "        timesteps = torch.randint(0, self.noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device) # B\n",
    "        timesteps_sparse = timesteps.long()\n",
    "        timesteps_sparse = timesteps_sparse[latents.feature.jidx.long()] # N, 1\n",
    "\n",
    "        # Add noise to the latents according to the noise magnitude at each timestep\n",
    "        # (this is the forward diffusion process)\n",
    "        noisy_latents = self.noise_scheduler.add_noise(latent_data, noise, timesteps_sparse)\n",
    "        \n",
    "        # Predict the target for the noise residual (this is the backward diffusion process for training)\n",
    "        if self.noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "            target = noise\n",
    "        # Currently Used ------------------ Very Important ------------------\n",
    "        elif self.noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "            target = self.noise_scheduler.get_velocity(latent_data, noise, timesteps_sparse)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown prediction type {self.noise_scheduler.config.prediction_type}\")\n",
    "\n",
    "        # Predict the noise residual and compute loss\n",
    "        # forward_cond function use batch-level timesteps\n",
    "        noisy_latents = VDBTensor(grid=latents.grid, feature=latents.grid.jagged_like(noisy_latents))\n",
    "\n",
    "\n",
    "        cond_dict = None\n",
    "        is_testing=False\n",
    "        guidance_scale=1.0\n",
    "   \n",
    "        # Classifier free guidance is when the model is trained with a classifier, \n",
    "        # but at inference time, the classifier is not used\n",
    "        do_classifier_free_guidance = guidance_scale != 1.0\n",
    "\n",
    "        # ! corssattn part\n",
    "        # text condition\n",
    "        if self.hparams[\"use_text_cond\"]:\n",
    "            # traing-time: get text from batch\n",
    "            if batch is not None:\n",
    "                text_emb = torch.stack(batch[DS.TEXT_EMBEDDING]) # B, 77, 1024\n",
    "                mask = torch.stack(batch[DS.TEXT_EMBEDDING_MASK]) # B, 77\n",
    "            else:\n",
    "                text_emb = cond_dict['text_emb']\n",
    "                mask = cond_dict['text_emb_mask']                \n",
    "            context = text_emb\n",
    "            if do_classifier_free_guidance:\n",
    "                context_copy = cond_dict['text_emb_null']\n",
    "                mask_copy = cond_dict['text_emb_mask_null']\n",
    "\n",
    "        \n",
    "        # ! concat part            \n",
    "        concat_list = []        \n",
    "        # semantic condition\n",
    "        # if self.hparams[\"use_semantic_cond\"]:\n",
    "        #     # traing-time: get semantic from batch\n",
    "        #     if batch is not None:\n",
    "        #         input_semantic = fvdb.JaggedTensor(batch[DS.LATENT_SEMANTIC])\n",
    "        #     else:\n",
    "        #         input_semantic = cond_dict['semantics']\n",
    "        #     semantic_cond = self.cond_stage_model(input_semantic.jdata.long())\n",
    "        #     if not is_testing and self.hparams.use_classifier_free: # if VDBtensor, convert to JaggedTensor\n",
    "        #         semantic_cond = self.conduct_classifier_free(semantic_cond, noisy_latents.grid.grid_count, noisy_latents.grid.device)  \n",
    "        #     concat_list.append(semantic_cond) # ! tensor type\n",
    "        \n",
    "        # ! corssattn part\n",
    "        # text condition\n",
    "        if self.hparams[\"use_text_cond\"]:\n",
    "            # traing-time: get text from batch\n",
    "            if batch is not None:\n",
    "                text_emb = torch.stack(batch[DS.TEXT_EMBEDDING]) # B, 77, 1024\n",
    "                mask = torch.stack(batch[DS.TEXT_EMBEDDING_MASK]) # B, 77\n",
    "            else:\n",
    "                text_emb = cond_dict['text_emb']\n",
    "                mask = cond_dict['text_emb_mask']                \n",
    "            context = text_emb\n",
    "            if do_classifier_free_guidance:\n",
    "                context_copy = cond_dict['text_emb_null']\n",
    "                mask_copy = cond_dict['text_emb_mask_null']\n",
    "\n",
    "    \n",
    "        if self.hparams[\"conditioning_key\"] == 'none':\n",
    "            # no condition is used -------------------- VERY IMPORTANT --------------------\n",
    "            model_pred = self.unet(noisy_latents, timesteps)\n",
    "        elif self.hparams[\"conditioning_key\"] == 'c_crossattn':\n",
    "            assert len(concat_list) > 0, \"concat_list should not be empty\"\n",
    "            assert context is not None, \"context should not be None\"\n",
    "            noisy_latents_in = VDBTensor.cat([noisy_latents] + concat_list, dim=1)\n",
    "            model_pred = self.unet(noisy_latents_in, timesteps, context=context, mask=mask)\n",
    "            \n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "\n",
    "        out.update({'pred': model_pred.jdata})\n",
    "        out.update({'target': target})\n",
    "\n",
    "        return out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def extract_latent(self, batch):\n",
    "        return self.vae._encode(batch, use_mode=False)\n",
    "    \n",
    "\n",
    "    # Used for inference / evaluation only, not for training\n",
    "    def evaluation_api(self, batch = None, grids: GridBatch = None, batch_size: int = None, latent_prev: VDBTensor = None, \n",
    "                       use_ddim=False, ddim_step=100, use_ema=True, use_dpm=False, use_karras=False, solver_order=3,\n",
    "                       h_stride=1, guidance_scale: float = 1.0, \n",
    "                       cond_dict=None, res_coarse=None, guided_grid=None):\n",
    "        \"\"\"\n",
    "        * @param grids: GridBatch from previous stage for conditional diffusion\n",
    "        * @param batch_size: batch_size for unconditional diffusion\n",
    "        * @param latent_prev: previous stage latent for conditional diffusion; not implemented yet\n",
    "        * @param use_ddim: use DDIM or not\n",
    "        * @param ddim_step: number of steps for DDIM\n",
    "        * @param use_dpm: use DPM++ solver or not\n",
    "        * @param use_karras: use Karras noise schedule or not \n",
    "        * @param solver_order: order of the solver; 3 for unconditional diffusion, 2 for guided sampling\n",
    "        * @param use_ema: use EMA or not\n",
    "        * @param h_stride: flag for remain_h VAE to create a anisotropic latent grid\n",
    "        * @param cond_dict: conditional dictionary -> only pass if manully effort needed\n",
    "        * @param res_coarse: previous stage result (semantics, normals, etc) for conditional diffusion\n",
    "        \"\"\"\n",
    "        if grids is None: \n",
    "            if batch is not None:\n",
    "                latents = self.extract_latent(batch)\n",
    "                grids = latents.grid\n",
    "            else:\n",
    "                # use dense diffusion\n",
    "                # create a dense grid\n",
    "                assert batch_size is not None, \"batch_size should be provided\"\n",
    "\n",
    "                # Haven't seen this before #TODO\n",
    "                feat_depth = self.vae.hparams[\"tree_depth\"] - 1\n",
    "                gap_stride = 2 ** feat_depth\n",
    "                gap_strides = [gap_stride, gap_stride, gap_stride // h_stride]\n",
    "\n",
    "\n",
    "\n",
    "                if isinstance(self.hparams[\"diffuser_image_size\"], int):\n",
    "                    neck_bound = int(self.hparams[\"diffuser_image_size\"] / 2)\n",
    "                    low_bound = [-neck_bound] * 3\n",
    "                    voxel_bound = [neck_bound * 2] * 3\n",
    "                else:        \n",
    "                    voxel_bound = self.hparams[\"diffuser_image_size\"]\n",
    "                    low_bound = [- int(res / 2) for res in self.hparams[\"diffuser_image_size\"]]\n",
    "                \n",
    "                # sv is the voxel size\n",
    "                voxel_sizes = [sv * gap for sv, gap in zip(self.vae.hparams[\"voxel_size\"], gap_strides)] \n",
    "                origins = [sv / 2. for sv in voxel_sizes]\n",
    "                grids = fvdb.sparse_grid_from_dense(\n",
    "                                batch_size, \n",
    "                                voxel_bound, \n",
    "                                low_bound, \n",
    "                                device=\"cpu\", # hack to fix bugs\n",
    "                                voxel_sizes=voxel_sizes,\n",
    "                                origins=origins).to(self.device)\n",
    "        # parse the cond_dict\n",
    "        if cond_dict is None:\n",
    "            cond_dict = {}\n",
    "\n",
    "        # mask condition\n",
    "        if self.hparams[\"use_semantic_cond\"]:\n",
    "            # check if semantics is in cond_dict\n",
    "            if 'semantics' not in cond_dict:\n",
    "                if batch is not None:\n",
    "                    cond_dict['semantics'] = fvdb.JaggedTensor(batch[DS.LATENT_SEMANTIC])\n",
    "                elif res_coarse is not None:\n",
    "                    cond_semantic = res_coarse.semantic_features[-1].jdata # N, class_num\n",
    "                    cond_semantic = torch.argmax(cond_semantic, dim=1)\n",
    "                    cond_dict['semantics'] = grids.jagged_like(cond_semantic)\n",
    "                else:\n",
    "                    raise NotImplementedError(\"No semantics provided\")\n",
    "                \n",
    "\n",
    "        # single scan concat condition\n",
    "        if self.hparams.use_normal_concat_cond:\n",
    "            # traing-time: get single scan crop from batch\n",
    "            if batch is not None:\n",
    "                ref_grid = fvdb.cat(batch[DS.INPUT_PC])    \n",
    "                ref_xyz = ref_grid.grid_to_world(ref_grid.ijk.float()) \n",
    "                concat_normal = grids.splat_trilinear(ref_xyz, fvdb.JaggedTensor(batch[DS.TARGET_NORMAL]))\n",
    "            elif res_coarse is not None:\n",
    "                concat_normal = res_coarse.normal_features[-1].feature # N, 3\n",
    "                concat_normal.jdata /= (concat_normal.jdata.norm(dim=1, keepdim=True) + 1e-6) # avoid nan\n",
    "            else:\n",
    "                raise NotImplementedError(\"No normal provided\")\n",
    "            cond_dict['normal'] = concat_normal                \n",
    "        \n",
    "        # diffusion process starts here ______________________________________________________________________________________\n",
    "        if use_ema:\n",
    "            with self.ema_scope(\"Evaluation API\"):\n",
    "                latents = self.random_sample_latents(grids, use_ddim=use_ddim, ddim_step=ddim_step, use_dpm=use_dpm, use_karras=use_karras, solver_order=solver_order,\n",
    "                                                     cond_dict=cond_dict, guidance_scale=guidance_scale)\n",
    "        else:\n",
    "            latents = self.random_sample_latents(grids, use_ddim=use_ddim, ddim_step=ddim_step, use_dpm=use_dpm, use_karras=use_karras, solver_order=solver_order,\n",
    "                                                     cond_dict=cond_dict, guidance_scale=guidance_scale)\n",
    "        # decode\n",
    "        res = self.vae.unet.FeaturesSet()\n",
    "        if guided_grid is None:\n",
    "            res, output_x = self.vae.unet.decode(res, latents, is_testing=True)\n",
    "        else:\n",
    "            res, output_x = self.vae.unet.decode(res, latents, guided_grid)\n",
    "        # TODO: add SDF output\n",
    "        return res, output_x\n",
    "    \n",
    "\n",
    "    \n",
    "    def random_sample_latents(self, grids: GridBatch, generator: torch.Generator = None, \n",
    "                              use_ddim=False, ddim_step=None, use_dpm=False, use_karras=False, solver_order=3,\n",
    "                              cond_dict=None, guidance_scale=1.0) -> VDBTensor:\n",
    "        if use_ddim:\n",
    "            if ddim_step is None:\n",
    "                ddim_step = self.hparams[\"num_inference_steps\"]\n",
    "            self.ddim_scheduler.set_timesteps(ddim_step, device=grids.device)\n",
    "            timesteps = self.ddim_scheduler.timesteps\n",
    "            scheduler = self.ddim_scheduler\n",
    "\n",
    "        elif use_dpm:\n",
    "            if ddim_step is None:\n",
    "                ddim_step = self.hparams[\"num_inference_steps\"]\n",
    "            try:\n",
    "                self.dpm_scheduler.set_timesteps(ddim_step, device=grids.device)\n",
    "            except:\n",
    "                # create a new dpm scheduler\n",
    "                self.dpm_scheduler = DPMSolverMultistepScheduler(\n",
    "                    num_train_timesteps=self.noise_scheduler_hparams[\"num_train_timesteps\"],\n",
    "                    beta_start=self.noise_scheduler_hparams[\"beta_start\"],\n",
    "                    beta_end=self.noise_scheduler_hparams[\"beta_end\"],\n",
    "                    beta_schedule=self.noise_scheduler_hparams[\"beta_schedule\"],\n",
    "                    solver_order=solver_order,\n",
    "                    prediction_type=self.noise_scheduler_hparams[\"prediction_type\"],\n",
    "                    algorithm_type=\"dpmsolver++\",\n",
    "                    use_karras_sigmas=use_karras,\n",
    "                )\n",
    "                self.dpm_scheduler.set_timesteps(ddim_step, device=grids.device)\n",
    "            timesteps = self.dpm_scheduler.timesteps\n",
    "            scheduler = self.dpm_scheduler\n",
    "        else:\n",
    "            timesteps = self.noise_scheduler.timesteps\n",
    "            scheduler = self.noise_scheduler\n",
    "        \n",
    "        # prepare the latents\n",
    "        latents = torch.randn(grids.total_voxels, self.unet.out_channels, device=grids.device, generator=generator)\n",
    "        \n",
    "        for i, t in tqdm(enumerate(timesteps)):\n",
    "            latent_model_input = latents\n",
    "            latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "            latent_model_input = VDBTensor(grid=grids, feature=grids.jagged_like(latent_model_input))\n",
    "            # Predict the noise residual\n",
    "            noise_pred = self._forward_cond(latent_model_input, t, cond_dict=cond_dict, is_testing=True, guidance_scale=guidance_scale) # TODO: cond\n",
    "            # compute the previous noisy sample x_t -> x_t-1\n",
    "            latents = scheduler.step(noise_pred.jdata, t, latents).prev_sample # TODO: when there is scale model input, why there is latents\n",
    "            \n",
    "        # scale the latents to the original scale\n",
    "        if self.hparams[\"scale_by_std\"]:\n",
    "            latents = 1. / self.scale_factor * latents\n",
    "        \n",
    "        return VDBTensor(grid=grids, feature=grids.jagged_like(latents))\n",
    "    \n",
    "    def _forward_cond(self, noisy_latents: VDBTensor, timesteps: torch.Tensor, \n",
    "                      batch = None, cond_dict = None, is_testing=False, guidance_scale=1.0) -> VDBTensor:\n",
    "        do_classifier_free_guidance = guidance_scale != 1.0\n",
    "        # ! adm part\n",
    "        # mask condition\n",
    "        if self.hparams[\"use_mask_cond\"]:\n",
    "            coords = noisy_latents.grid.grid_to_world(noisy_latents.grid.ijk.float())\n",
    "            coords = VDBTensor(noisy_latents.grid, coords)\n",
    "            cond = self.cond_stage_model(coords)\n",
    "        # point condition\n",
    "        if self.hparams[\"use_point_cond\"]:\n",
    "            coords = noisy_latents.grid.grid_to_world(noisy_latents.grid.ijk.float()) # JaggedTensor\n",
    "            if self.hparams[\"cond_stage_model_use_normal\"]:\n",
    "                if batch is not None: # training-time: get normal from batch\n",
    "                    ref_xyz = fvdb.JaggedTensor(batch[DS.INPUT_PC])\n",
    "                    # splatting normal\n",
    "                    input_normal = noisy_latents.grid.splat_trilinear(ref_xyz, fvdb.JaggedTensor(batch[DS.TARGET_NORMAL]))\n",
    "                    # normalize normal\n",
    "                    input_normal.jdata /= (input_normal.jdata.norm(dim=1, keepdim=True) + 1e-6) # avoid nan\n",
    "            else:\n",
    "                input_normal = None\n",
    "            cond = self.cond_stage_model(coords, input_normal)\n",
    "        # class condition:\n",
    "        if self.hparams[\"use_class_cond\"]:\n",
    "            if batch is not None:\n",
    "                cond = self.cond_stage_model(batch, key=DS.CLASS)\n",
    "            else:\n",
    "                cond = self.cond_stage_model(cond_dict, key=\"class\") # not checked yet\n",
    "        # micro condition\n",
    "        if self.hparams[\"use_micro_cond\"]:\n",
    "            if batch is not None:\n",
    "                micro = batch[DS.MICRO]\n",
    "                micro = torch.stack(micro).float()\n",
    "            else:\n",
    "                micro = cond_dict['micro']\n",
    "            micro = self.micro_pos_embedder(micro)\n",
    "            cond = self.micro_cond_model(micro)\n",
    "        \n",
    "        # ! concat part            \n",
    "        concat_list = []        \n",
    "        # semantic condition\n",
    "        if self.hparams[\"use_semantic_cond\"]:\n",
    "            # traing-time: get semantic from batch\n",
    "            if batch is not None:\n",
    "                input_semantic = fvdb.JaggedTensor(batch[DS.LATENT_SEMANTIC])\n",
    "            else:\n",
    "                input_semantic = cond_dict['semantics']\n",
    "            semantic_cond = self.cond_stage_model(input_semantic.jdata.long())\n",
    "            if not is_testing and self.hparams[\"use_classifier_free\"]: # if VDBtensor, convert to JaggedTensor\n",
    "                semantic_cond = self.conduct_classifier_free(semantic_cond, noisy_latents.grid.grid_count, noisy_latents.grid.device)  \n",
    "            concat_list.append(semantic_cond) # ! tensor type\n",
    "        # single scan concat condition\n",
    "        if self.hparams[\"use_single_scan_concat_cond\"]:\n",
    "            # traing-time: get single scan crop from batch\n",
    "            if batch is not None:\n",
    "                single_scan = fvdb.JaggedTensor(batch[DS.SINGLE_SCAN_CROP])\n",
    "                single_scan_intensity = fvdb.JaggedTensor(batch[DS.SINGLE_SCAN_INTENSITY_CROP])\n",
    "            else:\n",
    "                single_scan = cond_dict['single_scan']\n",
    "                single_scan_intensity = cond_dict['single_scan_intensity']\n",
    "                \n",
    "            # here use splatting to build the single scan grid tree\n",
    "            single_scan_hash_tree = self.vae.build_normal_hash_tree(single_scan)\n",
    "            single_scan_grid = single_scan_hash_tree[0]            \n",
    "            if self.hparams[\"encode_single_scan_by_points\"]:\n",
    "                single_scan_feat = self.single_scan_pos_embedder(single_scan, single_scan_intensity, single_scan_grid)\n",
    "                single_scan_feat = VDBTensor(single_scan_grid, single_scan_feat)\n",
    "            else:\n",
    "                single_scan_coords = single_scan_grid.grid_to_world(single_scan_grid.ijk.float()).jdata\n",
    "                single_scan_feat = self.single_scan_pos_embedder(single_scan_coords)\n",
    "                single_scan_feat = VDBTensor(single_scan_grid, single_scan_grid.jagged_like(single_scan_feat))\n",
    "            single_scan_cond = self.single_scan_cond_model(single_scan_feat, single_scan_hash_tree)\n",
    "            # align this feature to the latent\n",
    "            single_scan_cond = noisy_latents.grid.fill_to_grid(single_scan_cond.feature, single_scan_cond.grid, 0.0)\n",
    "            if not is_testing and self.hparams[\"use_classifier_free\"]:\n",
    "                single_scan_cond = self.conduct_classifier_free(single_scan_cond, noisy_latents.grid.grid_count, noisy_latents.grid.device)             \n",
    "            concat_list.append(single_scan_cond)\n",
    "        if self.hparams[\"use_normal_concat_cond\"]:\n",
    "            # traing-time: get single scan crop from batch\n",
    "            if batch is not None:\n",
    "                # assert self.hparams.use_fvdb_loader is True, \"use_fvdb_loader should be True for normal concat condition\"\n",
    "                ref_grid = fvdb.cat(batch[DS.INPUT_PC])    \n",
    "                ref_xyz = ref_grid.grid_to_world(ref_grid.ijk.float()) \n",
    "                concat_normal = noisy_latents.grid.splat_trilinear(ref_xyz, fvdb.JaggedTensor(batch[DS.TARGET_NORMAL]))\n",
    "            else:\n",
    "                concat_normal = cond_dict['normal']\n",
    "            concat_normal.jdata /= (concat_normal.jdata.norm(dim=1, keepdim=True) + 1e-6) # avoid nan\n",
    "            if not is_testing and self.hparams[\"use_classifier_free\"]:\n",
    "                concat_normal = self.conduct_classifier_free(concat_normal, noisy_latents.grid.grid_count, noisy_latents.grid.device)            \n",
    "            concat_list.append(concat_normal)\n",
    "\n",
    "        if do_classifier_free_guidance and len(concat_list) > 0: # ! not tested yet\n",
    "            if not self.hparams[\"use_classifier_free\"]:\n",
    "                # logger.info(\"Applying classifier-free guidance without doing it for concat condition\")\n",
    "                concat_list_copy = concat_list\n",
    "            else:\n",
    "                # logger.info(\"Applying classifier-free guidance for concat condition\")    \n",
    "                # assert self.hparams.use_classifier_free, \"do_classifier_free_guidance should be used with use_classifier_free\"\n",
    "                concat_list_copy = []\n",
    "                for cond in concat_list:\n",
    "                    cond = self.conduct_classifier_free(cond, noisy_latents.grid.grid_count, noisy_latents.grid.device, is_testing=True)\n",
    "                    concat_list_copy.append(cond)\n",
    "        \n",
    "        # ! corssattn part\n",
    "        # text condition\n",
    "        if self.hparams[\"use_text_cond\"]:\n",
    "            # traing-time: get text from batch\n",
    "            if batch is not None:\n",
    "                text_emb = torch.stack(batch[DS.TEXT_EMBEDDING]) # B, 77, 1024\n",
    "                mask = torch.stack(batch[DS.TEXT_EMBEDDING_MASK]) # B, 77\n",
    "            else:\n",
    "                text_emb = cond_dict['text_emb']\n",
    "                mask = cond_dict['text_emb_mask']                \n",
    "            context = text_emb\n",
    "            if do_classifier_free_guidance:\n",
    "                context_copy = cond_dict['text_emb_null']\n",
    "                mask_copy = cond_dict['text_emb_mask_null']\n",
    "\n",
    "        # concat pos_emb\n",
    "        if self.hparams[\"use_pos_embed\"]:\n",
    "            pos_embed = noisy_latents.grid.ijk\n",
    "            noisy_latents = VDBTensor.cat([noisy_latents, pos_embed], dim=1)\n",
    "        elif self.hparams[\"use_pos_embed_high\"]:\n",
    "            pos_embed = self.get_pos_embed_high(noisy_latents.grid.ijk.jdata)\n",
    "            noisy_latents = VDBTensor.cat([noisy_latents, pos_embed], dim=1)\n",
    "        elif self.hparams[\"use_pos_embed_world\"]:\n",
    "            pos_embed = noisy_latents.grid.grid_to_world(noisy_latents.grid.ijk.float())\n",
    "            noisy_latents = VDBTensor.cat([noisy_latents, pos_embed], dim=1)\n",
    "        elif self.hparams[\"use_pos_embed_world_high\"]:\n",
    "            pos_embed = noisy_latents.grid.grid_to_world(noisy_latents.grid.ijk.float())\n",
    "            pos_embed = self.get_pos_embed_high(pos_embed.jdata)\n",
    "            noisy_latents = VDBTensor.cat([noisy_latents, pos_embed], dim=1)\n",
    "\n",
    "        if self.hparams[\"conditioning_key\"] == 'none':\n",
    "            model_pred = self.unet(noisy_latents, timesteps)\n",
    "        elif self.hparams[\"conditioning_key\"] == 'concat':\n",
    "            assert len(concat_list) > 0, \"concat_list should not be empty\"\n",
    "            noisy_latents_in = VDBTensor.cat([noisy_latents] + concat_list, dim=1)\n",
    "            model_pred = self.unet(noisy_latents_in, timesteps)\n",
    "            \n",
    "            if do_classifier_free_guidance:\n",
    "                noisy_latents_in_copy = VDBTensor.cat([noisy_latents] + concat_list_copy, dim=1)\n",
    "                model_pred_copy = self.unet(noisy_latents_in_copy, timesteps)\n",
    "                model_pred = VDBTensor(model_pred.grid, model_pred.grid.jagged_like(model_pred.feature.jdata + guidance_scale * (model_pred.feature.jdata - model_pred_copy.feature.jdata)))\n",
    "        elif self.hparams[\"conditioning_key\"] == 'adm':\n",
    "            assert cond is not None, \"cond should not be None\"\n",
    "            model_pred = self.unet(noisy_latents, timesteps, y=cond)\n",
    "        elif self.hparams[\"conditioning_key\"] == 'crossattn': ### !! its always crossattn\n",
    "            assert context is not None, \"context should not be None\"\n",
    "            model_pred = self.unet(noisy_latents, timesteps, context=context, mask=mask)\n",
    "            \n",
    "            if do_classifier_free_guidance:\n",
    "                model_pred_copy = self.unet(noisy_latents, timesteps, context=context_copy, mask=mask_copy)\n",
    "                model_pred = VDBTensor(model_pred.grid, model_pred.grid.jagged_like(model_pred.feature.jdata + guidance_scale * (model_pred.feature.jdata - model_pred_copy.feature.jdata)))\n",
    "        elif self.hparams[\"conditioning_key\"] == 'c_crossattn':\n",
    "            assert len(concat_list) > 0, \"concat_list should not be empty\"\n",
    "            assert context is not None, \"context should not be None\"\n",
    "            noisy_latents_in = VDBTensor.cat([noisy_latents] + concat_list, dim=1)\n",
    "            model_pred = self.unet(noisy_latents_in, timesteps, context=context, mask=mask)\n",
    "            \n",
    "            if do_classifier_free_guidance:\n",
    "                noisy_latents_in_copy = VDBTensor.cat([noisy_latents] + concat_list_copy, dim=1)\n",
    "                model_pred_copy = self.unet(noisy_latents_in_copy, timesteps, context=context_copy, mask=mask_copy)\n",
    "                model_pred = VDBTensor(model_pred.grid, model_pred.grid.jagged_like(model_pred.feature.jdata + guidance_scale * (model_pred.feature.jdata - model_pred_copy.feature.jdata)))\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return model_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = None  # Placeholder for the trained autoencoder // will be  gotten from torch.load_state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'ema'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m diffusion_wrapper \u001b[38;5;241m=\u001b[39m \u001b[43mDiffusionModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhparams_temp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrained_autoencoder\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m, in \u001b[0;36mDiffusionModel.__init__\u001b[0;34m(self, hparams, trained_autoencoder)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhparams \u001b[38;5;241m=\u001b[39m hparams\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhparams, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mema\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mema\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhparams, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_ddim\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhparams\u001b[38;5;241m.\u001b[39muse_ddim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'ema'"
     ]
    }
   ],
   "source": [
    "diffusion_model = DiffusionModel(\n",
    "    hparams=hparams,\n",
    "    num_classes=hparams[\"num_classes\"],\n",
    "    use_spatial_transformer=False,\n",
    "    context_dim=diffuser_kwargs[\"context_dim\"],\n",
    "    diffuser_kwargs=diffuser_kwargs,\n",
    "    noise_scheduler_params=noise_scheduler_params,\n",
    "    trained_autoencoder=vae  #pre-trained VAE model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_lr_wrapper(it, lr_config, batch_size, accumulate_grad_batches=1):\n",
    "    return max(\n",
    "        lr_config['decay_mult'] ** (int(it * batch_size * accumulate_grad_batches / lr_config['decay_step'])),\n",
    "        lr_config['clip'] / lr_config['init'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 0.0\n",
    "batch_size = 64\n",
    "use_input_normal = False\n",
    "use_input_semantic = False\n",
    "with_semantic_branch = False\n",
    "use_input_intensity = False\n",
    "max_text_len = 77\n",
    "text_embed_drop_prob = 0.1\n",
    "\n",
    "\n",
    "train_dataset = \"ObjaverseDataset\"\n",
    "train_val_num_workers= 16\n",
    "train_kwargs = {\n",
    "  \"onet_base_path\": _objaverse_path ,\n",
    "  \"resolution\": resolution,\n",
    "  \"custom_name\": _custom_name,\n",
    "  \"split_base_path\": _split_path,\n",
    "  \"split\": \"train\",\n",
    "  \"text_emb_path\": _text_emb_path,\n",
    "  \"null_embed_path\": _null_embed_path,\n",
    "  \"max_text_len\": max_text_len,\n",
    "  \"text_embed_drop_prob\": text_embed_drop_prob, # ! classifier-free training\n",
    "  \"random_seed\": 0\n",
    "}\n",
    "\n",
    "val_dataset = \"ObjaverseDataset\"\n",
    "val_kwargs = {\n",
    "  \"onet_base_path\": _objaverse_path ,\n",
    "  \"resolution\": resolution,\n",
    "  \"custom_name\": _custom_name,\n",
    "  \"split_base_path\": _split_path,\n",
    "  \"split\": \"test\",\n",
    "  \"text_emb_path\": _text_emb_path,\n",
    "  \"null_embed_path\": _null_embed_path,\n",
    "  \"max_text_len\": max_text_len,\n",
    "  \"random_seed\": \"fixed\"\n",
    "}\n",
    "\n",
    "test_dataset = \"ObjaverseDataset\"\n",
    "test_num_workers =8\n",
    "test_kwargs = {\n",
    "  \"onet_base_path\": _objaverse_path ,\n",
    "  \"resolution\": resolution,\n",
    "  \"custom_name\": _custom_name,\n",
    "  \"split_base_path\": _split_path,\n",
    "  \"split\": \"test\",\n",
    "  \"text_emb_path\": _text_emb_path,\n",
    "  \"null_embed_path\": _null_embed_path,\n",
    "  \"max_text_len\": max_text_len,\n",
    "  \"random_seed\": \"fixed\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(diffusion_model.parameters(), lr=learning_rate[\"init\"],\n",
    "                                    weight_decay=weight_decay, amsgrad=True)\n",
    "\n",
    "scheduler = LambdaLR(optimizer,\n",
    "                lr_lambda=functools.partial(\n",
    "                    lambda_lr_wrapper, lr_config=learning_rate, batch_size=batch_size))\n",
    "\n",
    "# exp.global_var_manager.register_variable('skip_backward', False)\n",
    "\n",
    "def list_collate(batch):\n",
    "    \"\"\"\n",
    "    This just do not stack batch dimension.\n",
    "    \"\"\"\n",
    "    \n",
    "    elem = None\n",
    "    for e in batch:\n",
    "        if e is not None:\n",
    "            elem = e\n",
    "            break\n",
    "    elem_type = type(elem)\n",
    "    if isinstance(elem, torch.Tensor):\n",
    "        return batch\n",
    "    elif elem_type.__module__ == 'numpy' and elem_type.__name__ != 'str_' \\\n",
    "            and elem_type.__name__ != 'string_':\n",
    "        if elem_type.__name__ == 'ndarray' or elem_type.__name__ == 'memmap':\n",
    "            return list_collate([torch.as_tensor(b) if b is not None else None for b in batch])\n",
    "        elif elem.shape == ():  # scalars\n",
    "            return torch.as_tensor(batch)\n",
    "    elif isinstance(elem, float):\n",
    "        return torch.tensor(batch, dtype=torch.float64)\n",
    "    elif isinstance(elem, int):\n",
    "        return torch.tensor(batch)\n",
    "    elif isinstance(elem, str):\n",
    "        return batch\n",
    "    elif isinstance(elem, DictConfig) or isinstance(elem, ListConfig):\n",
    "        return batch\n",
    "    elif isinstance(elem, collections.abc.Mapping):\n",
    "        return {key: list_collate([d[key] for d in batch]) for key in elem}\n",
    "    elif isinstance(elem, collections.abc.Sequence):\n",
    "        # check to make sure that the elements in batch have consistent size\n",
    "        it = iter(batch)\n",
    "        elem_size = len(next(it))\n",
    "        if not all(len(elem) == elem_size for elem in it):\n",
    "            raise RuntimeError('each element in list of batch should be of equal size')\n",
    "        transposed = zip(*batch)\n",
    "        return [list_collate(samples) for samples in transposed]\n",
    "    elif isinstance(elem, GridBatch):\n",
    "        return fvdb.jcat(batch)\n",
    "    \n",
    "    return batch\n",
    "\n",
    "def get_dataset_spec():\n",
    "    all_specs = [DS.SHAPE_NAME, DS.INPUT_PC,\n",
    "                    DS.GT_DENSE_PC, DS.GT_GEOMETRY]\n",
    "    if use_input_normal:\n",
    "        all_specs.append(DS.TARGET_NORMAL)\n",
    "        all_specs.append(DS.GT_DENSE_NORMAL)\n",
    "    if use_input_semantic or with_semantic_branch:\n",
    "        all_specs.append(DS.GT_SEMANTIC)\n",
    "    if use_input_intensity:\n",
    "        all_specs.append(DS.INPUT_INTENSITY)\n",
    "    return all_specs\n",
    "\n",
    "\n",
    "def train_dataloader():\n",
    "    from data.objaverse import ObjaverseDataset\n",
    "    train_set =  ObjaverseDataset(onet_base_path=train_kwargs[\"onet_base_path\"], \n",
    "                                  spec=get_dataset_spec(), \n",
    "                                  split=train_kwargs[\"split\"], \n",
    "                                  resolution=train_kwargs[\"resolution\"], \n",
    "                                  image_base_path=None, \n",
    "                                  random_seed=0, \n",
    "                                  hparams=None, \n",
    "                                  skip_on_error=False, \n",
    "                                  custom_name=\"objaverse\", \n",
    "                                  text_emb_path=\"../data/objaverse/objaverse/text_emb\", \n",
    "                                  null_embed_path=\"./assets/null_text_emb.pkl\", \n",
    "                                  text_embed_drop_prob=0.0, \n",
    "                                  max_text_len=77, \n",
    "                                  duplicate_num=1, \n",
    "                                  split_base_path=_split_path,\n",
    "                                  )\n",
    "        \n",
    "    return DataLoader(train_set, batch_size=batch_size, shuffle=True,\n",
    "                        num_workers=train_val_num_workers, collate_fn=list_collate)\n",
    "\n",
    "\n",
    "# print(get_dataset_spec())\n",
    "\n",
    "def val_dataloader():\n",
    "    from data.objaverse import ObjaverseDataset\n",
    "    val_set = ObjaverseDataset(onet_base_path=val_kwargs[\"onet_base_path\"],\n",
    "                                spec=get_dataset_spec(), \n",
    "                                split=val_kwargs[\"split\"], \n",
    "                                resolution=val_kwargs[\"resolution\"], \n",
    "                                image_base_path=None, \n",
    "                                random_seed=0, \n",
    "                                hparams=None, \n",
    "                                skip_on_error=False, \n",
    "                                custom_name=\"objaverse\", \n",
    "                                text_emb_path=\"../data/objaverse/objaverse/text_emb\", \n",
    "                                null_embed_path=\"./assets/null_text_emb.pkl\", \n",
    "                                text_embed_drop_prob=0.0, \n",
    "                                max_text_len=77, \n",
    "                                duplicate_num=1, \n",
    "                                split_base_path=_split_path,\n",
    "                                )\n",
    "\n",
    "\n",
    "    return DataLoader(val_set, batch_size=batch_size, shuffle=False,\n",
    "                        num_workers=train_val_num_workers, collate_fn=list_collate)\n",
    "\n",
    "def test_dataloader(resolution=resolution, test_set_shuffle=False):\n",
    "    from data.objaverse import ObjaverseDataset\n",
    "    resolution = resolution # ! use for testing when training on X^3 but testing on Y^3\n",
    "\n",
    "    test_set =  ObjaverseDataset(onet_base_path=test_kwargs[\"onet_base_path\"],\n",
    "                                spec=get_dataset_spec(), \n",
    "                                split=test_kwargs[\"split\"], \n",
    "                                resolution=resolution, \n",
    "                                image_base_path=None, \n",
    "                                random_seed=0, \n",
    "                                hparams=None, \n",
    "                                skip_on_error=False, \n",
    "                                custom_name=\"objaverse\", \n",
    "                                text_emb_path=\"../data/objaverse/objaverse/text_emb\", \n",
    "                                null_embed_path=\"./assets/null_text_emb.pkl\", \n",
    "                                text_embed_drop_prob=0.0, \n",
    "                                max_text_len=77, \n",
    "                                duplicate_num=1, \n",
    "                                split_base_path=_split_path,\n",
    "                                )\n",
    "    \n",
    "    if test_set_shuffle:\n",
    "        torch.manual_seed(0)\n",
    "    return DataLoader(test_set, batch_size=1, shuffle=test_set_shuffle, \n",
    "                        num_workers=0, collate_fn=list_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@exp.mem_profile(every=1)\n",
    "def compute_loss(batch, out, compute_metric: bool):\n",
    "    loss_dict = exp.TorchLossMeter()\n",
    "    metric_dict = exp.TorchLossMeter()\n",
    "\n",
    "    # compute the MSE loss\n",
    "    if hparams[\"mse_weight\"] > 0.0:\n",
    "        loss_dict.add_loss(\"mse\", F.mse_loss(out[\"pred\"], out[\"target\"]), hparams[\"mse_weight\"])\n",
    "    if compute_metric: # currently use MSE as metric\n",
    "        metric_dict.add_loss(\"mse\", F.mse_loss(out[\"pred\"], out[\"target\"]))\n",
    "\n",
    "    return loss_dict, metric_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_dict_prefix(\n",
    "    self,\n",
    "    prefix: str,\n",
    "    dictionary: Mapping[str, Any],\n",
    "    prog_bar: bool = False,\n",
    "    logger: bool = True,\n",
    "    on_step: Optional[bool] = None,\n",
    "    on_epoch: Optional[bool] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    This overrides fixes if dict key is not a string...\n",
    "    \"\"\"\n",
    "    dictionary = {\n",
    "        prefix + \"/\" + str(k): v for k, v in dictionary.items()\n",
    "    }\n",
    "    self.log_dict(dictionary=dictionary,\n",
    "                    prog_bar=prog_bar,\n",
    "                    logger=logger, on_step=on_step, on_epoch=on_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_image(self, name: str, img, step: Optional[int] = None):\n",
    "    # if self.trainer.logger is not None:\n",
    "        # if self.logger_type == 'tb':\n",
    "        if img.shape[2] <= 4:\n",
    "            # WHC -> CWH\n",
    "            img = np.transpose(img, (2, 0, 1))\n",
    "        # self.trainer.logger.experiment.add_image(name, img, self.trainer.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = 0\n",
    "\n",
    "for batch_idx, batch in enumerate(train_dataloader()):\n",
    "    batch_loss = 0\n",
    "    \n",
    "    is_val = False\n",
    "    if batch_idx % 10 == 0:\n",
    "        is_val = True\n",
    "\n",
    "    if batch_idx % 100 == 0:\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    out = {'idx': batch_idx}\n",
    "    with exp.pt_profile_named(\"forward\"):\n",
    "        out = diffusion_model(batch, out)\n",
    "\n",
    "    if out is None and not is_val:\n",
    "        # return None\n",
    "        print(\"out is None???\")\n",
    "\n",
    "    # Compute metric in train would be the out['pred'] and out['target']\n",
    "    loss_dict, metric_dict = compute_loss(batch, out, compute_metric=is_val)\n",
    "\n",
    "    if not is_val:\n",
    "        log_dict_prefix('train_loss', loss_dict)\n",
    "    else:\n",
    "        log_dict_prefix('val_metric', metric_dict)\n",
    "        log_dict_prefix('val_loss', loss_dict)\n",
    "\n",
    "        if hparams.log_image:\n",
    "            cond_dict = {}\n",
    "            \n",
    "            if hparams.use_text_cond:\n",
    "                text_emb = torch.stack(batch[DS.TEXT_EMBEDDING]) # B, 77, 1024\n",
    "                mask = torch.stack(batch[DS.TEXT_EMBEDDING_MASK]) # B, 77\n",
    "                cond_dict['text_emb'] = text_emb\n",
    "                cond_dict['text_emb_mask'] = mask\n",
    "            \n",
    "            if batch_idx == 0:\n",
    "                with diffusion_model.ema_scope(\"Plotting\"):\n",
    "                    # first extract latent\n",
    "\n",
    "                    clean_latents = diffusion_model.extract_latent(batch)\n",
    "                    grids = clean_latents.grid\n",
    "\n",
    "                    # sample latents\n",
    "                    sample_latents = diffusion_model.random_sample_latents(grids, use_ddim=hparams.use_ddim, ddim_step=100, cond_dict=cond_dict) # TODO: change this ddim_step to variable\n",
    "                    \n",
    "                    # decode clean latents first\n",
    "                    decode_clean = decode_to_points(clean_latents)\n",
    "                    # Decode sample latents\n",
    "                    decode_sample = decode_to_points(sample_latents)\n",
    "                    sample = np.concatenate([decode_clean, decode_sample], axis=0)\n",
    "                    log_image(\"img/sample\", sample)\n",
    "                    # clean matplotlib opens\n",
    "                    plt.close('all')\n",
    "\n",
    "                    clean_latents = diffusion_model.extract_latent(batch)\n",
    "                    grids = clean_latents.grid\n",
    "                    _ = diffusion_model.random_sample_latents(grids, use_ddim=hparams.use_ddim, ddim_step=100, cond_dict=cond_dict) # TODO: change this ddim_step to variable\n",
    "\n",
    "    loss_sum = loss_dict.get_sum()\n",
    "\n",
    "    batch_loss += loss_sum\n",
    "    if hparams.ema:\n",
    "        diffusion_model.unet_ema(diffusion_model.unet)    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlayer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
