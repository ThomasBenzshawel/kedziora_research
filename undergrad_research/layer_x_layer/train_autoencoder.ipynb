{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import gc\n",
    "import importlib\n",
    "import inspect\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import shutil\n",
    "import traceback\n",
    "from collections import OrderedDict, defaultdict\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Mapping, Optional, Union\n",
    "\n",
    "# Third-party libraries - NumPy & Scientific\n",
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "\n",
    "# Third-party libraries - PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.tensorboard.summary import hparams\n",
    "\n",
    "# Third-party libraries - Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "# Third-party libraries - ML Tools\n",
    "import wandb\n",
    "from loguru import logger\n",
    "from omegaconf import DictConfig, ListConfig, OmegaConf\n",
    "import omegaconf.errors\n",
    "\n",
    "# Local imports\n",
    "from ext import common\n",
    "import fvdb\n",
    "import fvdb.nn as fvnn\n",
    "from fvdb import JaggedTensor, GridBatch\n",
    "\n",
    "# Local imports\n",
    "from xcube_refactored.modules.autoencoding.hparams import hparams_handler\n",
    "from xcube_refactored.utils.loss_util import AverageMeter\n",
    "from xcube_refactored.utils.loss_util import TorchLossMeter\n",
    "from xcube_refactored.utils import exp \n",
    "\n",
    "from xcube_refactored.modules.autoencoding.sunet import StructPredictionNet \n",
    "\n",
    "\n",
    "import gc\n",
    "import importlib\n",
    "from contextlib import contextmanager\n",
    "import os\n",
    "\n",
    "import fvdb\n",
    "from fvdb.nn import VDBTensor\n",
    "from fvdb import GridBatch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from omegaconf import DictConfig, ListConfig, OmegaConf\n",
    "import collections\n",
    "from pathlib import Path\n",
    "from pytorch_lightning.utilities.distributed import rank_zero_only\n",
    "\n",
    "\n",
    "from xcube_refactored.utils import exp\n",
    "from xcube_refactored.utils.vis_util import vis_pcs\n",
    "\n",
    "\n",
    "from xcube_refactored.modules.diffusionmodules.schedulers.scheduling_ddim import DDIMScheduler\n",
    "from xcube_refactored.modules.diffusionmodules.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "from xcube_refactored.modules.diffusionmodules.schedulers.scheduling_dpmpp_2m import DPMSolverMultistepScheduler\n",
    "\n",
    "\n",
    "from xcube_refactored.modules.diffusionmodules.ema import LitEma\n",
    "\n",
    "\n",
    "# Why aren't these used??????\n",
    "from xcube_refactored.modules.diffusionmodules.openaimodel.unet_dense import UNetModel as UNetModel_Dense\n",
    "from xcube_refactored.modules.diffusionmodules.openaimodel.unet_sparse import UNetModel as UNetModel_Sparse\n",
    "from xcube_refactored.modules.diffusionmodules.openaimodel.unet_sparse_crossattn import UNetModel as UNetModel_Sparse_CrossAttn\n",
    "\n",
    "\n",
    "# Why aren't these used??????\n",
    "from xcube_refactored.modules.encoders import (SemanticEncoder, ClassEmbedder, PointNetEncoder,\n",
    "                                    StructEncoder, StructEncoder3D, StructEncoder3D_remain_h, StructEncoder3D_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "batch_size = 4\n",
    "tree_depth = 3 # according to 512x512x512 -> 128x128x128\n",
    "voxel_size = 0.0025\n",
    "resolution = 512\n",
    "use_fvdb_loader = True\n",
    "use_hash_tree = True # use hash tree means use early dilation (description in Sec 3.4) \n",
    "\n",
    "# setup input\n",
    "use_input_normal = True\n",
    "use_input_semantic = False\n",
    "use_input_intensity = False\n",
    "\n",
    "# setup KL loss\n",
    "cut_ratio = 16 # reduce the dimension of the latent space\n",
    "kl_weight = 1.0 # activate when anneal is off\n",
    "normalize_kld = True\n",
    "enable_anneal = False\n",
    "kl_weight_min = 1e-7\n",
    "kl_weight_max = 1.0\n",
    "anneal_star_iter = 0\n",
    "anneal_end_iter = 70000 # need to adjust for different dataset\n",
    "\n",
    "\n",
    "structure_weight = 20.0\n",
    "normal_weight = 300.0\n",
    "  \n",
    "\n",
    "learning_rate = {\n",
    "  \"init\": 1.0e-4,\n",
    "  \"decay_mult\": 0.7,\n",
    "  \"decay_step\": 50000,\n",
    "  \"clip\": 1.0e-6\n",
    "}\n",
    "weight_decay = 0.0\n",
    "grad_clip = 0.5\n",
    "\n",
    "c_dim = 32\n",
    "  \n",
    "# unet parameters\n",
    "in_channels = 32\n",
    "num_blocks = tree_depth\n",
    "f_maps = 32\n",
    "neck_dense_type = \"UNCHANGED\"\n",
    "neck_bound = [64, 64, 64] # useless but indicate here\n",
    "num_res_blocks = 1\n",
    "use_residual = False\n",
    "order = \"gcr\"\n",
    "is_add_dec = False\n",
    "use_attention = False\n",
    "use_checkpoint = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(self.parameters(), lr=lr_config['init'],\n",
    "                                    weight_decay=self.hparams.weight_decay, amsgrad=True)\n",
    "\n",
    "scheduler = LambdaLR(optimizer,\n",
    "                lr_lambda=functools.partial(\n",
    "                    lambda_lr_wrapper, lr_config=lr_config, batch_size=self.hparams.batch_size))\n",
    "\n",
    "exp.global_var_manager.register_variable('skip_backward', False)\n",
    "\n",
    "\n",
    "# Do gradient clipping in the training steps\n",
    "grad_clip_val = self.hparams.get('grad_clip', 1000.)\n",
    "\n",
    "if grad_clip_val == \"inspect\":\n",
    "    from pytorch_lightning.utilities.grads import grad_norm\n",
    "    grad_dict = grad_norm(self, 'inf')      # Get the maximum absolute value.\n",
    "    print(grad_dict)\n",
    "    grad_clip_val = 1000.\n",
    "\n",
    "# torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=grad_clip_val)\n",
    "torch.nn.utils.clip_grad_value_(self.parameters(), clip_value=grad_clip_val)\n",
    "\n",
    "# If detect nan values, then this step is skipped\n",
    "has_nan_value_cnt = 0\n",
    "for p in filter(lambda p: p.grad is not None, self.parameters()):\n",
    "    if torch.any(p.grad.data != p.grad.data):\n",
    "        has_nan_value_cnt += 1\n",
    "if has_nan_value_cnt > 0:\n",
    "    exp.logger.warning(f\"{has_nan_value_cnt} parameters get nan-gradient -- this step will be skipped.\")\n",
    "    for p in filter(lambda p: p.grad is not None, self.parameters()):\n",
    "        p.grad.data.zero_()\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # Note:\n",
    "        import xcube.data as dataset\n",
    "        train_set = dataset.build_dataset(\n",
    "            self.hparams.train_dataset, self.get_dataset_spec(), self.hparams, self.hparams.train_kwargs)\n",
    "        torch.manual_seed(0)\n",
    "        return DataLoader(train_set, batch_size=self.hparams.batch_size // self.trainer.world_size, shuffle=True,\n",
    "                          num_workers=self.hparams.train_val_num_workers, collate_fn=self.get_collate_fn())\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        import xcube.data as dataset\n",
    "        val_set = dataset.build_dataset(\n",
    "            self.hparams.val_dataset, self.get_dataset_spec(), self.hparams, self.hparams.val_kwargs)\n",
    "        return DataLoader(val_set, batch_size=self.hparams.batch_size // self.trainer.world_size, shuffle=False,\n",
    "                          num_workers=self.hparams.train_val_num_workers, collate_fn=self.get_collate_fn())\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        import xcube.data as dataset\n",
    "        self.hparams.test_kwargs.resolution = self.hparams.resolution # ! use for testing when training on X^3 but testing on Y^3\n",
    "\n",
    "        test_set = dataset.build_dataset(\n",
    "            self.hparams.test_dataset, self.get_dataset_spec(), self.hparams, self.hparams.test_kwargs)\n",
    "        if self.hparams.test_set_shuffle:\n",
    "            torch.manual_seed(0)\n",
    "        return DataLoader(test_set, batch_size=1, shuffle=self.hparams.test_set_shuffle, \n",
    "                          num_workers=0, collate_fn=self.get_collate_fn())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
