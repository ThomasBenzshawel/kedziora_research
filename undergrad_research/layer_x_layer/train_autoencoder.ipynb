{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(self.parameters(), lr=lr_config['init'],\n",
    "                                    weight_decay=self.hparams.weight_decay, amsgrad=True)\n",
    "\n",
    "scheduler = LambdaLR(optimizer,\n",
    "                lr_lambda=functools.partial(\n",
    "                    lambda_lr_wrapper, lr_config=lr_config, batch_size=self.hparams.batch_size))\n",
    "\n",
    "exp.global_var_manager.register_variable('skip_backward', False)\n",
    "\n",
    "\n",
    "# Do gradient clipping in the training steps\n",
    "grad_clip_val = self.hparams.get('grad_clip', 1000.)\n",
    "\n",
    "if grad_clip_val == \"inspect\":\n",
    "    from pytorch_lightning.utilities.grads import grad_norm\n",
    "    grad_dict = grad_norm(self, 'inf')      # Get the maximum absolute value.\n",
    "    print(grad_dict)\n",
    "    grad_clip_val = 1000.\n",
    "\n",
    "# torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=grad_clip_val)\n",
    "torch.nn.utils.clip_grad_value_(self.parameters(), clip_value=grad_clip_val)\n",
    "\n",
    "# If detect nan values, then this step is skipped\n",
    "has_nan_value_cnt = 0\n",
    "for p in filter(lambda p: p.grad is not None, self.parameters()):\n",
    "    if torch.any(p.grad.data != p.grad.data):\n",
    "        has_nan_value_cnt += 1\n",
    "if has_nan_value_cnt > 0:\n",
    "    exp.logger.warning(f\"{has_nan_value_cnt} parameters get nan-gradient -- this step will be skipped.\")\n",
    "    for p in filter(lambda p: p.grad is not None, self.parameters()):\n",
    "        p.grad.data.zero_()\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # Note:\n",
    "        import xcube.data as dataset\n",
    "        train_set = dataset.build_dataset(\n",
    "            self.hparams.train_dataset, self.get_dataset_spec(), self.hparams, self.hparams.train_kwargs)\n",
    "        torch.manual_seed(0)\n",
    "        return DataLoader(train_set, batch_size=self.hparams.batch_size // self.trainer.world_size, shuffle=True,\n",
    "                          num_workers=self.hparams.train_val_num_workers, collate_fn=self.get_collate_fn())\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        import xcube.data as dataset\n",
    "        val_set = dataset.build_dataset(\n",
    "            self.hparams.val_dataset, self.get_dataset_spec(), self.hparams, self.hparams.val_kwargs)\n",
    "        return DataLoader(val_set, batch_size=self.hparams.batch_size // self.trainer.world_size, shuffle=False,\n",
    "                          num_workers=self.hparams.train_val_num_workers, collate_fn=self.get_collate_fn())\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        import xcube.data as dataset\n",
    "        self.hparams.test_kwargs.resolution = self.hparams.resolution # ! use for testing when training on X^3 but testing on Y^3\n",
    "\n",
    "        test_set = dataset.build_dataset(\n",
    "            self.hparams.test_dataset, self.get_dataset_spec(), self.hparams, self.hparams.test_kwargs)\n",
    "        if self.hparams.test_set_shuffle:\n",
    "            torch.manual_seed(0)\n",
    "        return DataLoader(test_set, batch_size=1, shuffle=self.hparams.test_set_shuffle, \n",
    "                          num_workers=0, collate_fn=self.get_collate_fn())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
