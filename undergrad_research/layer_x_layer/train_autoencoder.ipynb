{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xcube_refactored'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfvdb\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m JaggedTensor, GridBatch\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Local imports\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxcube_refactored\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautoencoding\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhparams\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hparams_handler\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxcube_refactored\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_util\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AverageMeter\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxcube_refactored\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_util\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TorchLossMeter\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xcube_refactored'"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import gc\n",
    "import importlib\n",
    "import inspect\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import shutil\n",
    "import traceback\n",
    "from collections import OrderedDict, defaultdict\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Mapping, Optional, Union\n",
    "\n",
    "# Third-party libraries - NumPy & Scientific\n",
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "\n",
    "# Third-party libraries - PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "# from torch.utils.tensorboard.summary import hparams\n",
    "\n",
    "# Third-party libraries - Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "# Third-party libraries - ML Tools\n",
    "from omegaconf import DictConfig, ListConfig, OmegaConf\n",
    "import omegaconf.errors\n",
    "\n",
    "# Local imports\n",
    "# from ext import common\n",
    "import fvdb\n",
    "import fvdb.nn as fvnn\n",
    "from fvdb import JaggedTensor, GridBatch\n",
    "\n",
    "# Local imports\n",
    "from xcube_refactored.modules.autoencoding.hparams import hparams_handler\n",
    "from xcube_refactored.utils.loss_util import AverageMeter\n",
    "from xcube_refactored.utils.loss_util import TorchLossMeter\n",
    "from xcube_refactored.utils import exp \n",
    "\n",
    "from xcube_refactored.modules.autoencoding.sunet import StructPredictionNet \n",
    "\n",
    "\n",
    "\n",
    "import gc\n",
    "import importlib\n",
    "from contextlib import contextmanager\n",
    "import os\n",
    "\n",
    "import fvdb\n",
    "from fvdb.nn import VDBTensor\n",
    "from fvdb import GridBatch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from omegaconf import DictConfig, ListConfig, OmegaConf\n",
    "import collections\n",
    "from pathlib import Path\n",
    "\n",
    "from layer_x_layer.utils import exp\n",
    "from layer_x_layer.utils.vis_util import vis_pcs\n",
    "\n",
    "\n",
    "from layer_x_layer.modules.diffusionmodules.schedulers.scheduling_ddim import DDIMScheduler\n",
    "from layer_x_layer.modules.diffusionmodules.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "from layer_x_layer.modules.diffusionmodules.schedulers.scheduling_dpmpp_2m import DPMSolverMultistepScheduler\n",
    "\n",
    "\n",
    "from layer_x_layer.modules.diffusionmodules.ema import LitEma\n",
    "\n",
    "\n",
    "# Why aren't these used??????\n",
    "from layer_x_layer.modules.encoders import (SemanticEncoder, ClassEmbedder, PointNetEncoder,\n",
    "                                    StructEncoder, StructEncoder3D, StructEncoder3D_remain_h, StructEncoder3D_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'max_text_len' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 73\u001b[0m\n\u001b[0;32m     63\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObjaverseDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     64\u001b[0m train_val_num_workers: \u001b[38;5;241m16\u001b[39m\n\u001b[0;32m     65\u001b[0m train_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     66\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monet_base_path\u001b[39m\u001b[38;5;124m\"\u001b[39m: _shapenet_path,\n\u001b[0;32m     67\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresolution\u001b[39m\u001b[38;5;124m\"\u001b[39m: resolution,\n\u001b[0;32m     68\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: _custom_name,\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_base_path\u001b[39m\u001b[38;5;124m\"\u001b[39m: _split_path,\n\u001b[0;32m     70\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     71\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_emb_path\u001b[39m\u001b[38;5;124m\"\u001b[39m: _text_emb_path,\n\u001b[0;32m     72\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnull_embed_path\u001b[39m\u001b[38;5;124m\"\u001b[39m: _null_embed_path,\n\u001b[1;32m---> 73\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_text_len\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_text_len,\n\u001b[0;32m     74\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_embed_drop_prob\u001b[39m\u001b[38;5;124m\"\u001b[39m: text_embed_drop_prob, \u001b[38;5;66;03m# ! classifier-free training\u001b[39;00m\n\u001b[0;32m     75\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom_seed\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     76\u001b[0m }\n\u001b[0;32m     78\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObjaverseDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     79\u001b[0m val_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     80\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monet_base_path\u001b[39m\u001b[38;5;124m\"\u001b[39m: _shapenet_path,\n\u001b[0;32m     81\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresolution\u001b[39m\u001b[38;5;124m\"\u001b[39m: resolution,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     88\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom_seed\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfixed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     89\u001b[0m }\n",
      "\u001b[1;31mNameError\u001b[0m: name 'max_text_len' is not defined"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "batch_size = 4\n",
    "tree_depth = 3 # according to 512x512x512 -> 128x128x128\n",
    "voxel_size = 0.0025\n",
    "resolution = 512\n",
    "use_fvdb_loader = True\n",
    "use_hash_tree = True # use hash tree means use early dilation (description in Sec 3.4) \n",
    "\n",
    "\n",
    "with_semantic_branch = False\n",
    "extract_mesh = \"store_true\"\n",
    "\n",
    "solver_order =\"3\"\n",
    "\n",
    "use_dpm = \"store_true\"\n",
    "\n",
    "ddim_step = 50\n",
    "\n",
    "use_ddim = \"store_true\"\n",
    "\n",
    "ema = \"store_true\"\n",
    "\n",
    "batch_len = 64\n",
    "\n",
    "toal_len = 700\n",
    "\n",
    "seed = 0\n",
    "\n",
    "world_size = 1\n",
    "\n",
    "# setup input\n",
    "use_input_normal = True\n",
    "use_input_semantic = False\n",
    "use_input_intensity = False\n",
    "\n",
    "# setup KL loss\n",
    "cut_ratio = 16 # reduce the dimension of the latent space\n",
    "kl_weight = 1.0 # activate when anneal is off\n",
    "normalize_kld = True\n",
    "enable_anneal = False\n",
    "kl_weight_min = 1e-7\n",
    "kl_weight_max = 1.0\n",
    "anneal_star_iter = 0\n",
    "anneal_end_iter = 70000 # need to adjust for different dataset\n",
    "\n",
    "\n",
    "structure_weight = 20.0\n",
    "normal_weight = 300.0\n",
    "  \n",
    "\n",
    "learning_rate = {\n",
    "  \"init\": 1.0e-4,\n",
    "  \"decay_mult\": 0.7,\n",
    "  \"decay_step\": 50000,\n",
    "  \"clip\": 1.0e-6\n",
    "}\n",
    "weight_decay = 0.0\n",
    "grad_clip = 0.5\n",
    "\n",
    "c_dim = 32\n",
    "  \n",
    "# unet parameters\n",
    "in_channels = 32\n",
    "num_blocks = tree_depth\n",
    "f_maps = 32\n",
    "neck_dense_type = \"UNCHANGED\"\n",
    "neck_bound = [64, 64, 64] # useless but indicate here\n",
    "num_res_blocks = 1\n",
    "use_residual = False\n",
    "order = \"gcr\"\n",
    "is_add_dec = False\n",
    "use_attention = False\n",
    "use_checkpoint = False\n",
    "\n",
    "\n",
    "_custom_name =  \"objaverse\"\n",
    "_shapenet_path = \"\"\n",
    "_split_path = \"\"\n",
    "_text_emb_path = \"\"\n",
    "_null_embed_path = \"./assets/null_text_emb.pkl\"\n",
    "max_text_len: 77\n",
    "text_embed_drop_prob: 0.1\n",
    "\n",
    "train_dataset = \"ObjaverseDataset\"\n",
    "train_val_num_workers: 16\n",
    "train_kwargs = {\n",
    "  \"onet_base_path\": _shapenet_path,\n",
    "  \"resolution\": resolution,\n",
    "  \"custom_name\": _custom_name,\n",
    "  \"split_base_path\": _split_path,\n",
    "  \"split\": \"train\",\n",
    "  \"text_emb_path\": _text_emb_path,\n",
    "  \"null_embed_path\": _null_embed_path,\n",
    "  \"max_text_len\": max_text_len,\n",
    "  \"text_embed_drop_prob\": text_embed_drop_prob, # ! classifier-free training\n",
    "  \"random_seed\": 0\n",
    "}\n",
    "\n",
    "val_dataset = \"ObjaverseDataset\"\n",
    "val_kwargs = {\n",
    "  \"onet_base_path\": _shapenet_path,\n",
    "  \"resolution\": resolution,\n",
    "  \"custom_name\": _custom_name,\n",
    "  \"split_base_path\": _split_path,\n",
    "  \"split\": \"test\",\n",
    "  \"text_emb_path\": _text_emb_path,\n",
    "  \"null_embed_path\": _null_embed_path,\n",
    "  \"max_text_len\": max_text_len,\n",
    "  \"random_seed\": \"fixed\"\n",
    "}\n",
    "\n",
    "test_dataset = \"ObjaverseDataset\"\n",
    "test_num_workers =8\n",
    "test_kwargs = {\n",
    "  \"onet_base_path\": _shapenet_path,\n",
    "  \"resolution\": resolution,\n",
    "  \"custom_name\": _custom_name,\n",
    "  \"split_base_path\": _split_path,\n",
    "  \"split\": \"test\",\n",
    "  \"text_emb_path\": _text_emb_path,\n",
    "  \"null_embed_path\": _null_embed_path,\n",
    "  \"max_text_len\": max_text_len,\n",
    "  \"random_seed\": \"fixed\"\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_lr_wrapper(it, lr_config, batch_size, accumulate_grad_batches=1):\n",
    "    return max(\n",
    "        lr_config['decay_mult'] ** (int(it * batch_size * accumulate_grad_batches / lr_config['decay_step'])),\n",
    "        lr_config['clip'] / lr_config['init'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = StructPredictionNet(\n",
    "  in_channels=in_channels,\n",
    "  num_blocks=num_blocks,\n",
    "  f_maps=f_maps,\n",
    "  neck_dense_type=neck_dense_type,\n",
    "  neck_bound=neck_bound,\n",
    "  num_res_blocks=num_res_blocks,\n",
    "  use_residual=use_residual,\n",
    "  order=order,\n",
    "  is_add_dec=is_add_dec,\n",
    "  use_attention=use_attention,\n",
    "  use_checkpoint=use_checkpoint,\n",
    "  c_dim=c_dim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate[\"init\"],\n",
    "                                    weight_decay=weight_decay, amsgrad=True)\n",
    "\n",
    "scheduler = LambdaLR(optimizer,\n",
    "                lr_lambda=functools.partial(\n",
    "                    lambda_lr_wrapper, lr_config=learning_rate, batch_size=batch_size))\n",
    "\n",
    "exp.global_var_manager.register_variable('skip_backward', False)\n",
    "\n",
    "def list_collate(batch):\n",
    "    \"\"\"\n",
    "    This just do not stack batch dimension.\n",
    "    \"\"\"\n",
    "    \n",
    "    elem = None\n",
    "    for e in batch:\n",
    "        if e is not None:\n",
    "            elem = e\n",
    "            break\n",
    "    elem_type = type(elem)\n",
    "    if isinstance(elem, torch.Tensor):\n",
    "        return batch\n",
    "    elif elem_type.__module__ == 'numpy' and elem_type.__name__ != 'str_' \\\n",
    "            and elem_type.__name__ != 'string_':\n",
    "        if elem_type.__name__ == 'ndarray' or elem_type.__name__ == 'memmap':\n",
    "            return list_collate([torch.as_tensor(b) if b is not None else None for b in batch])\n",
    "        elif elem.shape == ():  # scalars\n",
    "            return torch.as_tensor(batch)\n",
    "    elif isinstance(elem, float):\n",
    "        return torch.tensor(batch, dtype=torch.float64)\n",
    "    elif isinstance(elem, int):\n",
    "        return torch.tensor(batch)\n",
    "    elif isinstance(elem, str):\n",
    "        return batch\n",
    "    elif isinstance(elem, DictConfig) or isinstance(elem, ListConfig):\n",
    "        return batch\n",
    "    elif isinstance(elem, collections.abc.Mapping):\n",
    "        return {key: list_collate([d[key] for d in batch]) for key in elem}\n",
    "    elif isinstance(elem, collections.abc.Sequence):\n",
    "        # check to make sure that the elements in batch have consistent size\n",
    "        it = iter(batch)\n",
    "        elem_size = len(next(it))\n",
    "        if not all(len(elem) == elem_size for elem in it):\n",
    "            raise RuntimeError('each element in list of batch should be of equal size')\n",
    "        transposed = zip(*batch)\n",
    "        return [list_collate(samples) for samples in transposed]\n",
    "    elif isinstance(elem, GridBatch):\n",
    "        return fvdb.cat(batch)\n",
    "    \n",
    "    return batch\n",
    "\n",
    "\n",
    "class DatasetSpec(Enum):\n",
    "    SHAPE_NAME = 100\n",
    "    INPUT_PC = 200\n",
    "    TARGET_NORMAL = 300\n",
    "    INPUT_COLOR = 350\n",
    "    INPUT_INTENSITY = 360\n",
    "    GT_DENSE_PC = 400\n",
    "    GT_DENSE_NORMAL = 500\n",
    "    GT_DENSE_COLOR = 550\n",
    "    GT_MESH = 600\n",
    "    GT_MESH_SOUP = 650\n",
    "    GT_ONET_SAMPLE = 700\n",
    "    GT_GEOMETRY = 800\n",
    "    DATASET_CFG = 1000\n",
    "    GT_DYN_FLAG = 1100\n",
    "    GT_SEMANTIC = 1200\n",
    "    LATENT_SEMANTIC = 1300\n",
    "    SINGLE_SCAN_CROP = 1400\n",
    "    SINGLE_SCAN_INTENSITY_CROP = 1410\n",
    "    SINGLE_SCAN = 1450\n",
    "    SINGLE_SCAN_INTENSITY = 1460\n",
    "    CLASS = 1500\n",
    "    TEXT_EMBEDDING = 1600\n",
    "    TEXT_EMBEDDING_MASK = 1610\n",
    "    TEXT = 1620\n",
    "    MICRO = 1630\n",
    "\n",
    "\n",
    "def get_dataset_spec(self):\n",
    "    DS = DatasetSpec\n",
    "    all_specs = [DS.SHAPE_NAME, DS.INPUT_PC,\n",
    "                    DS.GT_DENSE_PC, DS.GT_GEOMETRY]\n",
    "    if use_input_normal:\n",
    "        all_specs.append(DS.TARGET_NORMAL)\n",
    "        all_specs.append(DS.GT_DENSE_NORMAL)\n",
    "    if use_input_semantic or with_semantic_branch:\n",
    "        all_specs.append(DS.GT_SEMANTIC)\n",
    "    if use_input_intensity:\n",
    "        all_specs.append(DS.INPUT_INTENSITY)\n",
    "    return all_specs\n",
    "\n",
    "\n",
    "# # The data set init function:     def __init__(self, onet_base_path, spec, split, resolution, image_base_path=None,\n",
    "#                  random_seed=0, hparams=None, skip_on_error=False, custom_name=\"objaverse\",\n",
    "#                  text_emb_path=\"../data/objaverse/objaverse/text_emb\", null_embed_path=\"./assets/null_text_emb.pkl\", text_embed_drop_prob=0.0, max_text_len=77,\n",
    "#                  duplicate_num=1, split_base_path=None, **kwargs):\n",
    "\n",
    "\n",
    "def train_dataloader(self):\n",
    "    from layer_x_layer.data.objaverse import ObjaverseDataset\n",
    "    train_set =  ObjaverseDataset(onet_base_path=train_kwargs[\"onet_base_path\"], \n",
    "                                  spec=get_dataset_spec(), \n",
    "                                  split=train_kwargs[\"split\"], \n",
    "                                  resolution=train_kwargs[\"resolution\"], \n",
    "                                  image_base_path=None, \n",
    "                                  random_seed=0, \n",
    "                                  hparams=None, \n",
    "                                  skip_on_error=False, \n",
    "                                  custom_name=\"objaverse\", \n",
    "                                  text_emb_path=\"../data/objaverse/objaverse/text_emb\", \n",
    "                                  null_embed_path=\"./assets/null_text_emb.pkl\", \n",
    "                                  text_embed_drop_prob=0.0, \n",
    "                                  max_text_len=77, \n",
    "                                  duplicate_num=1, \n",
    "                                  split_base_path=None, \n",
    "                                  **train_kwargs)\n",
    "        \n",
    "    return DataLoader(train_set, batch_size=batch_size // world_size, shuffle=True,\n",
    "                        num_workers=train_val_num_workers, collate_fn=list_collate)\n",
    "\n",
    "def val_dataloader(self):\n",
    "    from layer_x_layer.data.objaverse import ObjaverseDataset\n",
    "    val_set = ObjaverseDataset(onet_base_path=val_kwargs[\"onet_base_path\"],\n",
    "                                spec=get_dataset_spec(), \n",
    "                                split=val_kwargs[\"split\"], \n",
    "                                resolution=val_kwargs[\"resolution\"], \n",
    "                                image_base_path=None, \n",
    "                                random_seed=0, \n",
    "                                hparams=None, \n",
    "                                skip_on_error=False, \n",
    "                                custom_name=\"objaverse\", \n",
    "                                text_emb_path=\"../data/objaverse/objaverse/text_emb\", \n",
    "                                null_embed_path=\"./assets/null_text_emb.pkl\", \n",
    "                                text_embed_drop_prob=0.0, \n",
    "                                max_text_len=77, \n",
    "                                duplicate_num=1, \n",
    "                                split_base_path=None,\n",
    "                                **val_kwargs)\n",
    "\n",
    "\n",
    "    return DataLoader(val_set, batch_size=batch_size // world_size, shuffle=False,\n",
    "                        num_workers=train_val_num_workers, collate_fn=list_collate)\n",
    "\n",
    "def test_dataloader(self, resolution=resolution, test_set_shuffle=False):\n",
    "    from layer_x_layer.data.objaverse import ObjaverseDataset\n",
    "    resolution = resolution # ! use for testing when training on X^3 but testing on Y^3\n",
    "\n",
    "    test_set =  ObjaverseDataset(onet_base_path=test_kwargs[\"onet_base_path\"],\n",
    "                                spec=get_dataset_spec(), \n",
    "                                split=test_kwargs[\"split\"], \n",
    "                                resolution=resolution, \n",
    "                                image_base_path=None, \n",
    "                                random_seed=0, \n",
    "                                hparams=None, \n",
    "                                skip_on_error=False, \n",
    "                                custom_name=\"objaverse\", \n",
    "                                text_emb_path=\"../data/objaverse/objaverse/text_emb\", \n",
    "                                null_embed_path=\"./assets/null_text_emb.pkl\", \n",
    "                                text_embed_drop_prob=0.0, \n",
    "                                max_text_len=77, \n",
    "                                duplicate_num=1, \n",
    "                                split_base_path=None,\n",
    "                                **test_kwargs)\n",
    "    \n",
    "    if test_set_shuffle:\n",
    "        torch.manual_seed(0)\n",
    "    return DataLoader(test_set, batch_size=1, shuffle=test_set_shuffle, \n",
    "                        num_workers=0, collate_fn=list_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If detect nan values, then this step is skipped\n",
    "has_nan_value_cnt = 0\n",
    "for p in filter(lambda p: p.grad is not None, model.parameters()):\n",
    "    if torch.any(p.grad.data != p.grad.data):\n",
    "        has_nan_value_cnt += 1\n",
    "if has_nan_value_cnt > 0:\n",
    "    exp.logger.warning(f\"{has_nan_value_cnt} parameters get nan-gradient -- this step will be skipped.\")\n",
    "    for p in filter(lambda p: p.grad is not None, model.parameters()):\n",
    "        p.grad.data.zero_()\n",
    "\n",
    "\n",
    "torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=grad_clip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training step\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        loss = model(batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{epochs}], Step [{i}/{len(train_dataloader)}], Loss: {loss.item()}\")\n",
    "            \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_dataloader):\n",
    "            loss = model(batch)\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch [{epoch}/{epochs}], Step [{i}/{len(val_dataloader)}], Loss: {loss.item()}\")\n",
    "            \n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), f\"model_{epoch}.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "layerx",
   "language": "python",
   "name": "layerx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
