{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import gc\n",
    "import importlib\n",
    "import inspect\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import shutil\n",
    "import traceback\n",
    "from collections import OrderedDict, defaultdict\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Mapping, Optional, Union\n",
    "\n",
    "# Third-party libraries - NumPy & Scientific\n",
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "\n",
    "# Third-party libraries - PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.tensorboard.summary import hparams\n",
    "\n",
    "# Third-party libraries - Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "# Third-party libraries - ML Tools\n",
    "import wandb\n",
    "from loguru import logger\n",
    "from omegaconf import DictConfig, ListConfig, OmegaConf\n",
    "import omegaconf.errors\n",
    "\n",
    "# Local imports\n",
    "from ext import common\n",
    "import fvdb\n",
    "import fvdb.nn as fvnn\n",
    "from fvdb import JaggedTensor, GridBatch\n",
    "\n",
    "# Local imports\n",
    "from xcube_refactored.modules.autoencoding.hparams import hparams_handler\n",
    "from xcube_refactored.utils.loss_util import AverageMeter\n",
    "from xcube_refactored.utils.loss_util import TorchLossMeter\n",
    "from xcube_refactored.utils import exp \n",
    "\n",
    "from xcube_refactored.modules.autoencoding.sunet import StructPredictionNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_pos_embed_world = True\n",
    "\n",
    "voxel_size = 0.0025\n",
    "resolution = 512\n",
    "\n",
    "# data info\n",
    "duplicate_num = 10 # repeat the dataset to save the time of building dataloader\n",
    "batch_size = 64\n",
    "accumulate_grad_batches = 4\n",
    "batch_size_val = 4\n",
    "train_val_num_workers = 16\n",
    "\n",
    "# diffusion - inference params\n",
    "use_ddim = True\n",
    "num_inference_steps = 100\n",
    "\n",
    "# diffusion - scheduler-related adjust params\n",
    "num_train_timesteps = 1000\n",
    "beta_start = 0.0001\n",
    "beta_end = 0.02\n",
    "beta_schedule = \"linear\"\n",
    "prediction_type = \"v_prediction\"\n",
    "\n",
    "# diffusion - scale by std\n",
    "scale_by_std = True\n",
    "scale_factor = 1.0\n",
    "\n",
    "ema = True\n",
    "ema_decay = 0.9999\n",
    "\n",
    "\n",
    "mse_weight = 1.0\n",
    "\n",
    "learning_rate = {\n",
    "  \"init\": 5.0e-5,\n",
    "  \"decay_mult\": 1.0,\n",
    "  \"decay_step\": 2000000000, # use a constant learning rate\n",
    "  \"clip\": 1.0e-6\n",
    "}\n",
    "weight_decay = 0.0\n",
    "grad_clip = 0.5\n",
    "\n",
    "dims_diffuser = 3 # 3D conv\n",
    "image_size = 128 # use during testing\n",
    "model_channels = 128 \n",
    "use_middle_attention: True\n",
    "channel_mult = [1, 2, 2, 4] # 128 -> 16\n",
    "attention_resolutions = [4, 8] # 32 | 16\n",
    "num_res_blocks = 2\n",
    "num_heads = 8\n",
    "variance_type = \"fixed_small\"\n",
    "clip_sample = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_dataloader(self):\n",
    "    import xcube.data as dataset\n",
    "    val_set = dataset.build_dataset(\n",
    "        self.hparams.val_dataset, self.get_dataset_spec(), self.hparams, self.hparams.val_kwargs)\n",
    "    return DataLoader(val_set, batch_size=self.hparams.batch_size // self.trainer.world_size, shuffle=False,\n",
    "                        num_workers=self.hparams.train_val_num_workers, collate_fn=self.get_collate_fn())\n",
    "\n",
    "\n",
    "def train_dataloader(self):\n",
    "    import xcube.data as dataset\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    shuffle = True\n",
    "    train_set = dataset.build_dataset(\n",
    "        self.hparams.train_dataset, self.get_dataset_spec(), self.hparams, self.hparams.train_kwargs, duplicate_num=self.hparams.duplicate_num) # !: A change here for adding duplicate num for trainset without lantet\n",
    "\n",
    "    batch_size = self.hparams.batch_size\n",
    "    \n",
    "    return DataLoader(train_set, batch_size=batch_size, shuffle=shuffle,\n",
    "                        num_workers=self.hparams.train_val_num_workers, collate_fn=self.get_collate_fn())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
