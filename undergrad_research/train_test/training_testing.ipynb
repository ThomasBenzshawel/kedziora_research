{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bdb\n",
    "import importlib\n",
    "import os\n",
    "import pdb\n",
    "import shutil\n",
    "import sys\n",
    "import tempfile\n",
    "import traceback\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "os.chdir('C:\\Repos\\kedziora_research\\kedziora_research\\summer_dev')\n",
    "from train_test.test import OverfitLoggerNull\n",
    "from typing import List, Optional\n",
    "import random\n",
    "\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import lightning as L\n",
    "import torch\n",
    "import yaml\n",
    "from loguru import logger as loguru_logger\n",
    "from omegaconf import OmegaConf\n",
    "from packaging import version\n",
    "from pycg import exp\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n",
    "from lightning.pytorch.strategies import DDPStrategy\n",
    "from pytorch_lightning.utilities import rank_zero_only\n",
    "from pytorch_lightning.utilities.exceptions import MisconfigurationException\n",
    "from torch.nn import DataParallel\n",
    "\n",
    "from utils import wandb_util\n",
    "\n",
    "if version.parse(pl.__version__) > version.parse('1.8.0'):\n",
    "    from pytorch_lightning.callbacks import Callback\n",
    "else:\n",
    "    from pytorch_lightning.callbacks.base import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CopyModelFileCallback(Callback):\n",
    "    def __init__(self):\n",
    "        self.source_path = None\n",
    "        self.target_path = None\n",
    "\n",
    "    def on_train_start(self, trainer, pl_module):\n",
    "        if self.source_path is not None and self.target_path is not None:\n",
    "            if self.target_path.parent.exists():\n",
    "                shutil.move(self.source_path, self.target_path)\n",
    "\n",
    "\n",
    "class CustomizedDataParallel(DataParallel):\n",
    "    def scatter(self, inputs, kwargs, device_ids):\n",
    "        inputs = self.module.module.dp_scatter(inputs, device_ids, self.dim) if inputs else []\n",
    "        kwargs = self.module.module.dp_scatter(kwargs, device_ids, self.dim) if kwargs else []\n",
    "        if len(inputs) < len(kwargs):\n",
    "            inputs.extend([() for _ in range(len(kwargs) - len(inputs))])\n",
    "        elif len(kwargs) < len(inputs):\n",
    "            kwargs.extend([{} for _ in range(len(inputs) - len(kwargs))])\n",
    "        inputs = tuple(inputs)\n",
    "        kwargs = tuple(kwargs)\n",
    "        return inputs, kwargs\n",
    "\n",
    "\n",
    "# class CustomizedDataParallelStrategy(DDPStrategy):\n",
    "#     def __init__(self, parallel_devices: Optional[List[torch.device]]):\n",
    "#         # Parallel devices will be later populated in accelerator. Well done!\n",
    "#         super().__init__(parallel_devices=parallel_devices)\n",
    "\n",
    "#     def setup(self, model):\n",
    "#         from pytorch_lightning.overrides.data_parallel import \\\n",
    "#             LightningParallelModule\n",
    "\n",
    "#         # model needs to be moved to the device before it is wrapped\n",
    "#         model.to(self.root_device)\n",
    "#         self._model = CustomizedDataParallel(LightningParallelModule(model), self.parallel_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_usable_gpus(gpus):\n",
    "    if gpus is None:\n",
    "        gpus = 1\n",
    "\n",
    "    if \"CUDA_VISIBLE_DEVICES\" in os.environ.keys():\n",
    "        original_cvd = [int(t) for t in os.environ['CUDA_VISIBLE_DEVICES'].split(',')]\n",
    "    else:\n",
    "        original_cvd = []\n",
    "\n",
    "    if len(original_cvd) == gpus:\n",
    "        # Everything is fine.\n",
    "        return\n",
    "\n",
    "    # Mismatched/missing CVD setting & #gpus, reset.\n",
    "    gpu_states = exp.get_gpu_status(\"localhost\")\n",
    "    # temporally remove this to run multiple experiments on the same machine\n",
    "    available_gpus = [t for t in gpu_states if t.gpu_mem_usage < 0.2 and t.gpu_compute_usage < 0.2]\n",
    "    # available_gpus = [t for t in gpu_states]\n",
    "\n",
    "    if len(available_gpus) == 0:\n",
    "        print(\"You cannot use GPU. Everything is full.\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    if len(available_gpus) < gpus:\n",
    "        print(f\"Warning: Available GPUs are {[t.gpu_id for t in available_gpus]}, \"\n",
    "              f\"but you want to use {gpus} GPUs.\")\n",
    "        gpus = len(available_gpus)\n",
    "\n",
    "    available_gpus = available_gpus[:gpus]\n",
    "    selection_str = ','.join([str(t.gpu_id) for t in available_gpus])\n",
    "    print(f\"Intelligent GPU selection: {selection_str}\")\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = selection_str\n",
    "\n",
    "def is_rank_zero():\n",
    "    # It will also set LOCAL_RANK env variable, so using that will be more consistent.\n",
    "    return os.environ.get('MASTER_PORT', None) is None\n",
    "\n",
    "def is_rank_node_zero():\n",
    "    return os.environ.get('NODE_RANK', '0') == '0'\n",
    "\n",
    "def remove_option(parser, option):\n",
    "    for action in parser._actions:\n",
    "        if vars(action)['option_strings'][0] == option:\n",
    "            parser._handle_conflict_resolve(None, [(option, action)])\n",
    "            break\n",
    "\n",
    "\n",
    "def readable_name_from_exec(exec_list: List[str]):\n",
    "    keys = {}\n",
    "    for exec_str in exec_list:\n",
    "        kvs = exec_str.split(\"=\")\n",
    "        k_name = kvs[0]\n",
    "        k_name_arr = [\"\".join([us[0] for us in t.split(\"_\") if len(us) > 0]) for t in k_name.split(\".\")]\n",
    "        # Collapse leading dots except for the last one.\n",
    "        k_name = ''.join(k_name_arr[:-2]) + '.'.join(k_name_arr[-2:])\n",
    "        k_value = kvs[1]\n",
    "        if k_value.lower() in [\"true\", \"false\"]:\n",
    "            k_value = str(int(k_value.lower() == \"true\"))\n",
    "        keys[k_name] = k_value\n",
    "    return '-'.join([k + keys[k] for k in sorted(list(keys.keys()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lightning.pytorch.cli import LightningCLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-10 21:43:37.161\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m42\u001b[0m - \u001b[1mThis is train_auto.py! Please note that you should use 300 instead of 300.0 for resuming.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "[1] Parse and initialize program arguments\n",
    "    these include: --debug, --profile, --gpus, --num_nodes, --resume, ...\n",
    "    they will NOT be saved for a checkpoints.\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "\n",
    "# program_parser = exp.argparse.ArgumentParser()\n",
    "# program_parser.add_argument('--debug', action='store_true', help='Use debug mode of pytorch')\n",
    "# program_parser.add_argument('--resume', action='store_true', help='Continue training. Use hparams.yaml file.')\n",
    "# program_parser.add_argument('--nolog', action='store_true', help='Do not create any logs.')\n",
    "# program_parser.add_argument('--nosync', action='store_true', help='Do not synchronize nas even if forced.')\n",
    "# program_parser.add_argument('--save_topk', default=2, type=int, help='How many top models to save. -1 to save all models.')\n",
    "# program_parser.add_argument('--validate_first', action='store_true', help='Do a full validation with logging before training starts.')\n",
    "# program_parser.add_argument('--logger_type', choices=['tb', 'wandb', 'none'], default='wandb')\n",
    "# program_parser.add_argument('--wname', default=None, type=str, help='Run name to be appended to wandb')\n",
    "# program_parser.add_argument('--wandb_base', type=str, default=\"../wandb/\", help=\"Path to wandb base directory.\")\n",
    "# program_parser.add_argument('--eval_interval', type=int, default=1, help='How often to evaluate the model.')\n",
    "# program_parser.add_argument('--save_every', default=50, type=int, help='How often to save the model.')\n",
    "# program_parser.add_argument('--resume_from_ckpt', default=None, type=str, help='checkpoint path we want to load')\n",
    "# program_parser.add_argument('--model_precision', default=32, help='Model precision to use.')\n",
    "# program_parser.add_argument('--seed', type=int, default=0, help='Set a random seed.')\n",
    "\n",
    "\n",
    "save_topk = 2\n",
    "wname = None\n",
    "wandb_base = \"../wandb/\"\n",
    "eval_interval = 1\n",
    "save_every = 50\n",
    "resume_from_ckpt = None\n",
    "model_precision = 32\n",
    "seed = 0\n",
    "nosync = False\n",
    "max_epochs = None\n",
    "debug = False\n",
    "logger_type = 'wandb'\n",
    "accelerator = 'ddp'\n",
    "gpus = 1\n",
    "\n",
    "\n",
    "\n",
    "loguru_logger.info(f\"This is train_auto.py! Please note that you should use 300 instead of 300.0 for resuming.\")\n",
    "\n",
    "# model_parser = exp.ArgumentParserX(base_config_path='configs/default/param.yaml')\n",
    "# model_args = model_parser.parse_args()\n",
    "# hyper_path = model_args.hyper\n",
    "# del model_args[\"hyper\"]\n",
    "\n",
    "# Default logger type\n",
    "# if program_args.nolog:\n",
    "#     program_args.logger_type = 'none'\n",
    "\n",
    "# AUTO resume with wandb logger\n",
    "## uncomment if you want to use it and fill in your own <WANDB_USER_NAME>!\n",
    "# if program_args.logger_type == 'wandb':\n",
    "#     wname = program_args.wname\n",
    "#     sep_pos = str(model_args.name).find('/')\n",
    "#     project_name = model_args.name[:sep_pos]\n",
    "#     run_name = model_args.name[sep_pos + 1:] + \"/\" + wname\n",
    "\n",
    "#     check_wandb_name = \"<WANDB_USER_NAME>/xcube-%s/%s:last\" % (project_name, run_name)\n",
    "#     try:\n",
    "#         print(\"Try to load from wandb:\", check_wandb_name)\n",
    "#         wdb_run, args_ckpt = wandb_util.get_wandb_run(check_wandb_name, wdb_base=program_args.wandb_base, default_ckpt=\"last\")\n",
    "#         assert args_ckpt is not None, \"Please specify checkpoint version!\"\n",
    "#         assert args_ckpt.exists(), \"Selected checkpoint does not exist!\"\n",
    "#         print(\"Load from wandb:\", check_wandb_name)\n",
    "#         program_args.resume = True\n",
    "#         other_args[0] = check_wandb_name\n",
    "#     except:\n",
    "#         print(\"No wandb checkpoint found, start training from scratch\")\n",
    "#         pass\n",
    "\n",
    "# Force not to sync to shorten bootstrap time.\n",
    "if nosync:\n",
    "    os.environ['NO_SYNC'] = '1'\n",
    "\n",
    "# Train forever\n",
    "if max_epochs is None:\n",
    "    max_epochs = -1\n",
    "\n",
    "if is_rank_zero():\n",
    "    # Detect usable GPUs.\n",
    "    \n",
    "    # determine_usable_gpus(gpus=gpus)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.set_device(0)\n",
    "        torch.cuda.init()\n",
    "        print(\"CUDA is available.\")\n",
    "        \n",
    "    # Wandb version check\n",
    "    if gpus > 1 and accelerator is None:\n",
    "            strategy = 'ddp'\n",
    "            accelerator = \"gpu\"\n",
    "else:\n",
    "    # Align parameters.\n",
    "        strategy = 'ddp'\n",
    "        accelerator = \"gpu\"\n",
    "        devices = gpus\n",
    "               \n",
    "\n",
    "# Profiling and debugging options\n",
    "torch.autograd.set_detect_anomaly(debug)\n",
    "\n",
    "# specify logdir if not use wandb\n",
    "dirpath = None\n",
    "# if logger_type == 'none':\n",
    "#     wname = os.path.basename(hyper_path).split(\".\")[0]\n",
    "#     sep_pos = str(model_args.name).find('/')\n",
    "#     project_name = model_args.name[:sep_pos]\n",
    "#     run_name = model_args.name[sep_pos + 1:] + \"_\" + wname\n",
    "#     logdir = os.path.join(\"./checkpoints\", project_name, run_name)\n",
    "#     os.makedirs(logdir, exist_ok=True)\n",
    "#     dirpath = logdir\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=dirpath,\n",
    "    filename='{epoch:06d}-{step:09d}',\n",
    "    save_last=True,\n",
    "    save_top_k=save_topk,\n",
    "    monitor=\"val_step\",\n",
    "    mode=\"max\",\n",
    "    every_n_train_steps=save_every,\n",
    ")\n",
    "\n",
    "lr_record_callback = LearningRateMonitor(logging_interval='step')\n",
    "copy_model_file_callback = CopyModelFileCallback()\n",
    "\n",
    "# # Determine parallel plugin:\n",
    "# if accelerator == 'ddp':\n",
    "#     if version.parse(pl.__version__) < version.parse('1.8.0'):\n",
    "#         from pytorch_lightning.plugins import DDPPlugin\n",
    "#         accelerator_plugins = [DDPPlugin(find_unused_parameters=False)]\n",
    "#     else:\n",
    "#         accelerator_plugins = []\n",
    "# elif accelerator == 'dp':\n",
    "#     accelerator_plugins = [CustomizedDataParallelPlugin(None)]\n",
    "# else:\n",
    "#     accelerator_plugins = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\"\\n[2] Determine model arguments\\n    MODEL args include: --lr, --num_layers, etc. (everything defined in YAML)\\n    These use AP-X module, which accepts CLI and YAML inputs.\\n    These args will be saved as hyper-params.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "resume = False\n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "[2] Determine model arguments\n",
    "    MODEL args include: --lr, --num_layers, etc. (everything defined in YAML)\n",
    "    These use AP-X module, which accepts CLI and YAML inputs.\n",
    "    These args will be saved as hyper-params.\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "# if resume:\n",
    "#     # raw_hyper = other_args[0]\n",
    "#     if raw_hyper.startswith(\"wdb:\"):\n",
    "#         # Load config and replace\n",
    "#         wdb_run, wdb_ckpt = wandb_util.get_wandb_run(raw_hyper, program_args.wandb_base, default_ckpt=\"last\")\n",
    "#         tmp_yaml_name = '/tmp/' + str(uuid.uuid4()) + '.yaml'\n",
    "#         with open(tmp_yaml_name, 'w') as outfile:\n",
    "#             yaml.dump(wandb_util.recover_from_wandb_config(wdb_run.config), outfile)\n",
    "#         other_args[0] = tmp_yaml_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"test\"\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "[3] Build / restore logger and checkpoints.\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "# Set checkpoint auto-save options.\n",
    "last_ckpt_path = None\n",
    "if resume_from_ckpt is not None:\n",
    "    last_ckpt_path = resume_from_ckpt\n",
    "\n",
    "if logger_type == 'tb':\n",
    "    if not resume:\n",
    "        logger_version_num = None\n",
    "        last_ckpt_path = None\n",
    "    else:\n",
    "        last_ckpt_path = Path(hyper_path).parent / \"checkpoints\" / \"last.ckpt\"\n",
    "        logger_version_num = Path(hyper_path).parent.name if program_args.resume else None\n",
    "    logger = TensorBoardLogger('../checkpoints/', name=model_args.name,\n",
    "                                version=logger_version_num, default_hp_metric=False)\n",
    "    # Call this property to assign the version early, so we don't have to wait for the model to be loaded\n",
    "    print(f\"Tensorboard logger, version number =\", logger.version)\n",
    "elif logger_type == 'wandb':\n",
    "    # Will create wandb folder automatically\n",
    "    from datetime import datetime, timedelta\n",
    "    import randomname\n",
    "\n",
    "    if not resume:\n",
    "        wname = wname\n",
    "        if 'WANDB_SWEEP_ID' in os.environ.keys():\n",
    "            # (Use exec to determine good names)\n",
    "            wname = os.environ['WANDB_SWEEP_ID'] + \"-\" + readable_name_from_exec(exec)\n",
    "        if wname is None:\n",
    "            # Example: 0105-clever-monkey\n",
    "            wname = (datetime.utcnow() + timedelta(hours=8)).strftime('%m%d') + \"-\" + randomname.get_name()\n",
    "        sep_pos = str(name).find('/')\n",
    "        if sep_pos == -1:\n",
    "            project_name = name\n",
    "            run_name = \"root/\" + wname\n",
    "        else:\n",
    "            project_name = name[:sep_pos]\n",
    "            run_name = name[sep_pos + 1:] + \"/\" + wname\n",
    "\n",
    "        if is_rank_node_zero():\n",
    "            logger = WandbLogger(name=run_name, save_dir=wandb_base, project='xcube-' + project_name)\n",
    "        else:\n",
    "            pass\n",
    "    else:\n",
    "        if is_rank_node_zero():\n",
    "            logger = WandbLogger(name=wdb_run.name, save_dir=wandb_base, project=wdb_run.project, id=wdb_run.id)\n",
    "        else:\n",
    "            pass\n",
    "        last_ckpt_path = wdb_ckpt\n",
    "        os.unlink(tmp_yaml_name)\n",
    "else:\n",
    "    logger = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fvdb'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mepoch_loop\u001b[38;5;241m.\u001b[39m_batches_that_stepped \u001b[38;5;241m=\u001b[39m global_step_offset\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m checkpoint    \n\u001b[1;32m---> 25\u001b[0m net_module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mModel\n\u001b[0;32m     26\u001b[0m net_model \u001b[38;5;241m=\u001b[39m net_module(model_args)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;129m@rank_zero_only\u001b[39m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_model_summary\u001b[39m():\n",
      "File \u001b[1;32mc:\\Users\\Thomas Benzshawel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1204\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:940\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mC:\\Repos\\kedziora_research\\kedziora_research\\summer_dev\\models\\autoencoder.py:11\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) 2024, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# NVIDIA CORPORATION & AFFILIATES and its licensors retain all intellectual property\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# distribution of this software and related documentation without an express\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# license agreement from NVIDIA CORPORATION & AFFILIATES is strictly prohibited.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgc\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfvdb\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfvdb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mfvnn\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fvdb'"
     ]
    }
   ],
   "source": [
    "model = \"autoencoder\"\n",
    "\n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "[4] Build trainer and determine final hparams. Set AMP if needed.\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "# Do it here because wandb name need some randomness.\n",
    "if seed == -1:\n",
    "    # random seed\n",
    "    seed = random.randint(0, 1000000)\n",
    "\n",
    "pl.seed_everything(seed)\n",
    "\n",
    "# Build trainer\n",
    "\n",
    "trainer = L.Trainer(precision=model_precision, logger=logger, log_every_n_steps=20)\n",
    "# fix wandb global_step resume bug\n",
    "if resume:\n",
    "    # get global step offset\n",
    "    checkpoint = torch.load(last_ckpt_path, map_location='cpu')\n",
    "    global_step_offset = checkpoint[\"global_step\"]\n",
    "    trainer.fit_loop.epoch_loop._batches_that_stepped = global_step_offset\n",
    "    del checkpoint    \n",
    "\n",
    "net_module = importlib.import_module(\"models.\" + model).Model\n",
    "net_model = net_module(model_args)\n",
    "\n",
    "@rank_zero_only\n",
    "def print_model_summary():\n",
    "    print(\" >>>> ======= MODEL HYPER-PARAMETERS ======= <<<< \")\n",
    "    print(OmegaConf.to_yaml(net_model.hparams, resolve=True))\n",
    "    print(\" >>>> ====================================== <<<< \")\n",
    "\n",
    "print_model_summary()\n",
    "\n",
    "# No longer use this..\n",
    "del is_rank_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "[5] Main training iteration.\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "# Note: In debug mode, trainer.fit will automatically end if NaN occurs in backward.\n",
    "e = None\n",
    "try:\n",
    "    net_model.overfit_logger = OverfitLoggerNull()\n",
    "    if program_args.validate_first:\n",
    "        trainer.validate(net_model, ckpt_path=last_ckpt_path)\n",
    "    with exp.pt_profile_named(\"training\", \"1.json\"):\n",
    "        trainer.fit(net_model, ckpt_path=last_ckpt_path)\n",
    "except Exception as ex:\n",
    "    e = ex\n",
    "    # https://stackoverflow.com/questions/52081929/pdb-go-to-a-frame-in-exception-within-exception\n",
    "    if isinstance(e, MisconfigurationException):\n",
    "        if e.__context__ is not None:\n",
    "            traceback.print_exc()\n",
    "            if program_args.accelerator is None:\n",
    "                pdb.post_mortem(e.__context__.__traceback__)\n",
    "    elif isinstance(e, bdb.BdbQuit):\n",
    "        print(\"Post mortem is skipped because the exception is from Pdb.\")\n",
    "    else:\n",
    "        traceback.print_exc()\n",
    "        if program_args.accelerator is None:\n",
    "            pdb.post_mortem(e.__traceback__)\n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "[6] If ended premature, add to delete list.\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "if trainer.current_epoch < 1 and last_ckpt_path is None and trainer.local_rank == 0:\n",
    "    if program_args.logger_type == 'tb':\n",
    "        if Path(trainer.log_dir).exists():\n",
    "            with open(\".premature_checkpoints\", \"a\") as f:\n",
    "                f.write(f\"{trainer.log_dir}\\n\")\n",
    "            print(f\"\\n\\nTB Checkpoint at {trainer.log_dir} marked to be cleared.\\n\\n\")\n",
    "        sys.exit(-1)\n",
    "    elif program_args.logger_type == 'wandb':\n",
    "        with open(\".premature_checkpoints\", \"a\") as f:\n",
    "            f.write(f\"wdb:{trainer.logger.experiment.path}:{trainer.logger.experiment.name}\\n\")\n",
    "        print(f\"\\n\\nWandb Run of {trainer.logger.experiment.path} \"\n",
    "                f\"(with name {trainer.logger.experiment.name}) marked to be cleared.\\n\\n\")\n",
    "        sys.exit(-1)\n",
    "\n",
    "if trainer.local_rank == 0:\n",
    "    print(f\"Training Finished. Best path = {checkpoint_callback.best_model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
